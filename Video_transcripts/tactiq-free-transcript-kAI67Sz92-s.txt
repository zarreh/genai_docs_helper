# tactiq.io free youtube transcript
# TS-1: Curve fitting is (almost) all you need
# https://www.youtube.com/watch/kAI67Sz92-s

00:00:00.430 [Music]
00:00:25.439 hello everyone and welcome to my youtube
00:00:27.359 channel today is a very exciting day
00:00:29.039 because we are starting a new series for
00:00:32.079 time series uh with the help of my very
00:00:35.040 good friend conrad who volunteered to
00:00:37.600 help actually he didn't volunteer i
00:00:39.040 bugged him a lot so thank you
00:00:42.320 thank you conrad for uh this special
00:00:45.120 time series episodes that we will be
00:00:47.440 doing
00:00:48.559 over the next few weeks or maybe even
00:00:50.960 months right so uh
00:00:53.680 if you don't know conrad you can
00:00:56.559 go to kaggle and go to his linkedin
00:00:58.320 profile he's a lead data scientist
00:01:00.559 currently based in netherlands and he
00:01:02.800 has been on this youtube channel before
00:01:05.199 a few months ago and that was a very
00:01:08.560 fun episode i should say
00:01:10.640 uh it was a lot of learning and it was a
00:01:12.799 lot of fun and hopefully it's going to
00:01:14.799 be the same today i'm looking forward to
00:01:16.720 that and i'm also looking forward to
00:01:18.640 learn more about time series
00:01:20.720 so over to you conrad
00:01:22.560 thank you well first of all i'm shaq
00:01:24.080 thank you well thank you for the kind
00:01:25.600 into kind words introduction and the
00:01:27.520 repeated invite uh in terms of you
00:01:30.240 mobbing into this one you know what they
00:01:31.920 say there are no victims only volunteers
00:01:34.320 so i wouldn't worry an awful lot about
00:01:36.240 that one
00:01:37.280 uh i'm very happy to be here obviously
00:01:40.960 and
00:01:42.240 well what can i say probably i think i
00:01:44.000 mentioned something to you the last time
00:01:46.240 it's a bit of a surprise to me how much
00:01:48.479 interest there actually is in in time
00:01:50.960 series because i thought for a long time
00:01:53.200 that this was a field that was you know
00:01:54.640 kind of dying out
00:01:56.719 goes to show how much i know
00:01:58.560 uh
00:01:59.600 so
00:02:00.479 i am happy to be
00:02:02.960 showing being they're all showing guys
00:02:04.399 the gateway dragging to the interesting
00:02:06.479 sections of the universe where
00:02:08.639 what can you do without deep learning
00:02:10.800 everything although don't worry for
00:02:12.640 those of you strongly attached deep
00:02:14.480 learning this will this will appear as
00:02:16.239 well
00:02:17.840 uh so
00:02:19.440 without further ado uh
00:02:22.720 can you guys see what i'm sharing i hope
00:02:25.120 you can yeah we can
00:02:27.360 okay
00:02:29.280 uh
00:02:30.239 the usual thing when you're trying to
00:02:32.160 share something with people
00:02:34.319 in the in in terms of coding is what's
00:02:37.280 the environment you do it in
00:02:39.440 uh
00:02:41.360 well the simplest common denominator
00:02:44.319 given
00:02:45.200 abc's in my lineage just do it as a
00:02:47.760 kernel which is exactly what we are
00:02:50.000 going to do today
00:02:51.920 uh by the way the
00:02:53.760 sort of life coding exercise that i'm
00:02:55.519 doing right now i will be sharing this
00:02:57.599 kernel later so you can or actually can
00:03:01.280 we still still say kernel or is it only
00:03:03.440 notebooks officially
00:03:04.959 i'm not sure
00:03:06.080 i think it's cool
00:03:07.210 [Laughter]
00:03:09.920 it's not good now
00:03:11.680 yeah
00:03:13.040 okay
00:03:14.000 whatever
00:03:15.840 let's start let's start with a new
00:03:17.920 notebook
00:03:20.319 let's call it
00:03:21.519 ts one
00:03:23.760 ah i can't use it as a name that is
00:03:26.159 deeply unfortunate
00:03:28.239 uh curves
00:03:30.400 slack did why the hell do you call it a
00:03:32.159 slug
00:03:33.200 uh let's clean up this bit a little bit
00:03:37.920 the life of me i don't understand why do
00:03:40.239 people insist on having this one
00:03:41.840 everywhere but yeah
00:03:43.519 it is what it is
00:03:45.120 uh we'll start with some basic imports
00:03:47.760 that we will
00:03:48.959 need along the way uh
00:03:51.200 obviously
00:03:52.560 if if you don't if you don't need the
00:03:54.720 the sides sidebar or whatever it is
00:03:57.200 called with input output and everything
00:03:59.040 do you mind hiding it
00:04:00.879 uh i'm trying
00:04:03.280 you just you can just click on the top
00:04:05.120 right arrow and that button yeah
00:04:08.159 this one yeah well i will actually need
00:04:10.640 it in a moment but not not right now
00:04:12.959 sure better now yeah this is more
00:04:15.760 spacious
00:04:18.560 well i live in a country that if
00:04:20.320 anything could use some more space
00:04:23.759 i i teach you not if you spend some time
00:04:25.840 in amsterdam and then you go
00:04:29.199 for instance to a place like paris
00:04:31.520 first thing you notice it's actually
00:04:33.360 streets that have normal size
00:04:36.240 i mean amsterdam has a bunch of
00:04:38.160 advantages but man is
00:04:40.840 it and you have a lot of bikes
00:04:45.120 bikes are actually what bikes is
00:04:46.880 actually what i count on the on the
00:04:48.479 advantage side
00:04:50.240 uh okay
00:04:52.240 i think it's start oh it's still
00:04:54.840 starting so just just for the audience
00:04:58.000 if you have any questions feel free to
00:05:00.160 ask them along the way i'm doing my
00:05:02.160 other house best follow if i can't
00:05:04.479 follow that abhishek
00:05:06.560 will alert me to
00:05:08.840 it session started
00:05:11.919 because i accidentally changed something
00:05:13.520 to markdown
00:05:15.280 this is strictly speaking
00:05:17.840 not needed for the pen series itself but
00:05:20.800 it might come in handy if you actually
00:05:22.800 want to have
00:05:25.039 well something readable
00:05:27.840 so let's start with the basics
00:05:30.160 what's a time series time series is
00:05:32.960 pretty much anything you measure over
00:05:34.479 time
00:05:35.440 period
00:05:36.479 that's the end of the definition uh you
00:05:38.960 can get a lot more formal uh around that
00:05:42.400 uh claim that this is a time series
00:05:44.960 process uh the stochastic process
00:05:47.600 underneath of which you observe a single
00:05:49.199 realization and it's all very useful but
00:05:52.560 it's not the core that's not the most
00:05:54.320 important bit in practice in practice
00:05:56.000 what you need
00:05:57.840 is the best way to think about it like
00:05:59.840 this we have everything you measure over
00:06:02.000 time and the question is what can you
00:06:04.720 what can you actually do with a time
00:06:06.720 series
00:06:07.840 uh
00:06:12.000 if you want to figure out what's
00:06:13.600 actually going on
00:06:17.199 uh upside down
00:06:27.520 ah
00:06:28.280 begin
00:06:30.240 equation
00:06:36.000 don't remember the syntax that well
00:06:41.440 yeah
00:06:42.319 that's what i wanted
00:06:43.759 uh
00:06:44.960 have a good look at this equation
00:06:46.639 because this is something that will be
00:06:48.240 our friend
00:06:49.440 uh over and over again in different
00:06:51.599 parts of this will be called a longer
00:06:53.360 number
00:06:54.400 the notebook
00:06:55.680 answering the question from kashish
00:06:57.919 the notebook will be available after the
00:06:59.599 stream
00:07:00.880 it most certainly will be right now it's
00:07:02.960 set as a private kernel but uh
00:07:06.160 i will i will share it
00:07:07.759 of course
00:07:09.360 uh
00:07:10.479 so okay so xt is our time series
00:07:13.039 something that we just measure over time
00:07:16.160 well that's a nice simple setup but it's
00:07:18.880 not all there that's not super useful if
00:07:21.520 we want to answer well
00:07:23.360 fundamental question in time series and
00:07:25.360 in philosophy alike namely what the hell
00:07:27.360 is going on
00:07:28.800 so how can we approach this question
00:07:31.199 well we can try to break the time series
00:07:34.560 into components that are somewhat
00:07:36.560 meaningful to what we are trying to do
00:07:39.120 and uh what you see here is a so-called
00:07:41.840 structural decomposition structural
00:07:45.280 structural decomposition canonical
00:07:47.039 equation
00:07:48.240 variations around the same thing the
00:07:50.160 point is you have a component
00:07:52.879 that's
00:07:54.400 deterministic a trend
00:07:57.199 as an example you look at
00:08:00.960 sales of beer
00:08:02.560 over time
00:08:03.919 and then the trend component which
00:08:05.599 undoubtedly has something to say about
00:08:07.440 is temperature
00:08:08.879 i mean with the with few exceptions
00:08:10.479 people don't lit don't you don't drink a
00:08:12.720 lot of beer if it's minus 20 outside or
00:08:15.440 whatever the minimum is
00:08:17.199 in the place from which you guys are
00:08:18.639 listening to it right now
00:08:20.160 then we have a seasonal component
00:08:22.960 as pretty much you measure the
00:08:24.879 temperature well one obvious thing
00:08:27.199 you're going to have is a daily cycle
00:08:30.160 typically
00:08:31.840 when when the sun is shining it's a bit
00:08:33.760 warmer than otherwise
00:08:36.320 uh so that's something deterministic so
00:08:39.599 that's what that's important that's not
00:08:41.279 a trend so something that just grows or
00:08:43.440 changes monotonically over time
00:08:45.279 something that changes is determinism
00:08:47.200 and is deterministic and we know when
00:08:49.200 that happens for instance we know
00:08:52.000 well we know when the sun rises we know
00:08:54.640 quite well in advance
00:08:56.240 then sometimes in certain encounters
00:08:58.240 that's mostly useful in things like
00:09:00.240 economics
00:09:01.360 uh you have the cycle so think like
00:09:03.839 economic cycles we know that usually
00:09:06.320 after a recession
00:09:10.080 uh that i can answer quite simply
00:09:12.240 because that's the middle ground the
00:09:14.480 question from pranav why are the
00:09:16.320 galaxies because it's the first choice
00:09:18.959 for a
00:09:20.399 wallpaper for me because it's not
00:09:22.800 offending to anybody
00:09:24.640 you choose just about anything else
00:09:26.399 someone will be unhappy
00:09:29.519 uh there is a level components well it's
00:09:30.880 confusing it's taken together
00:09:33.040 yes it is slightly confusing for the
00:09:35.120 moment i am assuming there is this is
00:09:37.360 this is uh
00:09:39.839 that the deterministic component is
00:09:41.600 buried inside the trend yes
00:09:44.640 that's that's the idea that's the idea
00:09:46.560 here
00:09:47.680 uh so rolling forward
00:09:51.680 by the way what i'm doing right now
00:09:54.160 is based
00:09:56.880 uh
00:10:00.320 on the notebook where all of those
00:10:02.399 things are
00:10:03.600 elaborated upon in a slight more
00:10:06.240 statement reference dedicated would be
00:10:08.079 better maybe
00:10:09.920 educated notebook
00:10:14.160 uh feel free to consult to consult this
00:10:16.880 one later
00:10:18.399 uh
00:10:19.200 so
00:10:21.279 let's look at an example time series
00:10:25.200 which we can and now i need to get back
00:10:27.839 here
00:10:28.880 add data
00:10:31.040 your data says this one by the way is
00:10:33.040 with this one by the way is public
00:10:34.800 already i'm pretty sure if at any point
00:10:37.600 later
00:10:38.640 you guys cannot use something because i
00:10:40.959 left it on private
00:10:42.560 do do let
00:10:44.480 myself rubbish no
00:10:47.440 yeah i should be fine
00:10:49.680 uh
00:10:50.560 we have a question so what is the
00:10:52.399 difference between seasonal and cyclic
00:10:54.640 component
00:10:55.760 seasonal we know the period
00:10:58.160 so
00:10:59.680 well we know when the day ends and when
00:11:02.160 it begins
00:11:03.200 we know when the days of the week occur
00:11:05.519 we know when the months of the year
00:11:07.360 occur those kinds of things so any
00:11:10.800 component of the time series that's
00:11:12.320 related to things that we know when they
00:11:14.640 happen
00:11:15.680 that's that that that you will try to
00:11:17.519 model through seasonal
00:11:19.120 cyclical like i said the best example
00:11:21.519 and probably the most frequent one is um
00:11:25.360 economic cycles
00:11:27.200 like you know a bubble cannot go on go
00:11:29.920 on forever in a bookmark boom in the
00:11:31.760 stock market at some point there will be
00:11:33.760 a correction well trouble is we don't
00:11:36.079 know
00:11:38.240 i mean
00:11:39.279 if we knew when corrections happened in
00:11:41.120 the storm stock market we will we would
00:11:43.120 all be filthy rich by now
00:11:45.360 uh
00:11:47.600 i hope that answers the question
00:11:50.240 the important bit
00:11:52.320 unless you are dealing with financial
00:11:55.120 data specifically related to economy or
00:11:57.200 the stock market
00:11:58.639 uh you can you're probably fine ignoring
00:12:00.720 it at this stage
00:12:02.560 would be to be quite honest
00:12:05.760 uh let's have a look at the
00:12:08.240 u.s energy data this is a
00:12:10.880 monthly consumption
00:12:13.360 along
00:12:14.320 well going way back to the 70s so quite
00:12:17.440 a bit that has happened since then
00:12:19.680 what do we do once we get our hands on a
00:12:21.440 time series well
00:12:24.079 first thing we do
00:12:26.079 try to make sure that we can actually
00:12:28.000 represent it as a time series
00:12:31.680 if at any point i am doing something
00:12:34.000 that seems non-obvious
00:12:37.120 please shout the thing is i have i mean
00:12:40.160 i'm trying to live up to their
00:12:42.639 reputation under which abhishek
00:12:44.560 introduced meaningless time series
00:12:46.720 specialist by the way thank you for the
00:12:48.079 compliment
00:12:49.360 uh
00:12:50.880 the tiny trouble with this is certain
00:12:53.200 things are almost muscle memory so i do
00:12:56.720 it on borderline autopilot kind of like
00:12:58.639 you know you go on kaggle and you start
00:13:00.399 with import note by snp input pandas spd
00:13:02.720 on autopilot without thinking much about
00:13:04.399 it that's more or less what might happen
00:13:06.880 with me so feel free to stop me if it
00:13:09.279 does
00:13:11.279 and then we
00:13:12.639 plot
00:13:17.200 okay i will
00:13:20.720 help myself to a setup so that this
00:13:23.760 thing is actually a bit more readable
00:13:27.200 because the default settings
00:13:31.200 are kind of horrible in terms of images
00:13:35.600 so there is there are a couple of
00:13:37.680 questions one is about uh profit i think
00:13:40.320 it's better if we take it a little bit
00:13:42.160 later today
00:13:43.920 yeah and another question will be later
00:13:47.040 yeah and the question is if the circle
00:13:49.519 is not deterministic if this cycle is
00:13:52.240 not deterministic
00:13:53.760 how can you how can you model it
00:13:55.600 mathematically with ct
00:13:59.199 uh
00:14:00.320 probably through a combination of
00:14:02.399 intervention variables that would be the
00:14:05.040 that would be the general idea
00:14:07.120 uh namely
00:14:09.040 okay you can't really capture it well in
00:14:12.000 the exponential smoothing framework
00:14:14.720 you can capture it in things like uh
00:14:18.079 state space models which are
00:14:21.360 uh i hate to say this is coming later in
00:14:23.440 the series but that's literally what
00:14:25.839 what the plan is
00:14:27.760 exponential smoothing is pretty much
00:14:30.720 the simplest
00:14:33.120 what would you call it
00:14:34.519 non-degenerate model or non-trivial
00:14:36.800 model that's probably better non-trivial
00:14:38.800 like
00:14:39.760 the most trivial prediction is it's
00:14:41.839 constant okay that's not very
00:14:43.760 interesting uh teensy weensy bit above
00:14:46.480 that is saying okay what's my best
00:14:49.440 prediction for tomorrow well same to the
00:14:51.440 same as today
00:14:53.519 uh it's a little better
00:14:55.920 but
00:14:56.800 you know like
00:14:58.480 it's a bit like with the fallacy of the
00:15:00.160 chicken
00:15:01.120 if you interpolate too much chicken is
00:15:02.720 sitting or a turkey is sitting in a farm
00:15:05.199 saying okay every day the farmer comes
00:15:06.959 along and he feeds me and that's my
00:15:09.279 model for the future
00:15:10.959 and it's all good until the dates the
00:15:12.800 day before thanksgiving
00:15:14.480 and then the model fails and it fails
00:15:16.240 spectacularly so that's the risk with
00:15:18.560 using just the most recent past as a
00:15:21.279 prediction going forward
00:15:23.279 how do we know that cyclicality is
00:15:24.639 present in the data i'm getting towards
00:15:26.240 it i'm getting towards that one
00:15:28.959 so where were we
00:15:31.519 plotting the data
00:15:36.480 then
00:15:37.199 the next thing
00:15:38.560 by the way do not don't worry if you
00:15:41.040 don't remember
00:15:42.959 it's far from obvious
00:15:44.959 for you where to find things like the
00:15:47.680 season of the composition i've been
00:15:49.600 using stats models for a while which is
00:15:54.560 the library if you want a lot of time
00:15:56.399 today's functionality under one umbrella
00:15:58.800 and i don't remember it like i had this
00:16:01.199 one written on the side and i'm typing
00:16:03.600 from what i wrote down on the side
00:16:04.800 because no normal person can remember
00:16:06.560 this psa seasonal and then something
00:16:09.040 else
00:16:10.079 and that harkens back to the base when i
00:16:11.920 had to type right i had to call them in
00:16:13.920 c sharp that's how important just about
00:16:16.320 anything felt
00:16:18.959 uh what are what am i doing here i am
00:16:21.519 importing seasonal decompose which is a
00:16:24.560 wrapper around the functionality
00:16:26.800 that allows us to calculate
00:16:29.199 the decompose the series into trend
00:16:31.600 seasonality
00:16:32.959 and the reminder we're ignoring the
00:16:35.759 cycle thing at the moment
00:16:37.839 uh decomposed df
00:16:40.160 we are dealing with monthly data
00:16:43.440 approximately anyway
00:16:45.199 i mean not exactly because it wasn't
00:16:47.040 sampled perfectly for it should be the
00:16:48.880 first day of the month and not the last
00:16:50.639 of the previous one i'm guessing they
00:16:52.800 were just dealing with uh
00:16:56.000 the first working day or something like
00:16:57.759 this
00:17:00.800 okay
00:17:02.000 so
00:17:04.400 df
00:17:06.480 meaning period equals 12. why is it
00:17:08.480 relevant we uh what do we specify as
00:17:10.959 arguments well what's our data set our
00:17:13.039 series that we feed into it
00:17:14.959 and what's the seasonality that around
00:17:16.799 which we expect it to work
00:17:20.959 okay
00:17:23.359 ah i forgot to plot it
00:17:28.079 so our decomposition object has a plot
00:17:31.280 method
00:17:37.679 i hate it so much
00:17:43.280 how do i increase it
00:17:45.679 how do i increase the image size that is
00:17:47.679 the big question here
00:17:51.360 for some reason the setup i try the bolt
00:17:53.760 doesn't work
00:17:55.520 give me one moment to figure it out
00:18:05.440 that's better
00:18:07.600 that's our original data
00:18:11.200 normalized to the same scale
00:18:13.760 then we have the trend
00:18:15.840 which is just on the basis of looking at
00:18:18.400 well at the raw data what's the
00:18:19.760 deterministic component that we can fit
00:18:22.640 then we can see the seasonal pattern
00:18:24.880 nice regular
00:18:26.559 a pretty solid variation but okay that's
00:18:28.960 okay as you can see that's pretty
00:18:30.320 repetitive
00:18:31.600 however
00:18:32.720 then we look at the residuals
00:18:34.880 and if you recall residuals in the
00:18:37.200 time-honored tradition of statistics
00:18:39.919 what's a residual it's the stuff that
00:18:42.000 our model couldn't account for
00:18:44.400 so we throw it into the resi the
00:18:46.080 component code as it called residual and
00:18:48.000 pretend that it's white noise and if on
00:18:50.320 the basis of data characteristics nobody
00:18:52.400 can call us out then we're
00:18:54.840 good mean sounds flippant but that's
00:18:57.520 what it comes down to really well the
00:18:59.679 tiny problem here is if you look
00:19:03.360 there's a lot of down well negative
00:19:05.840 observations
00:19:07.520 then it's kind of symmetric around zero
00:19:09.840 in this period and then around post 2000
00:19:12.799 way more
00:19:13.919 upward observations uh positive ones
00:19:16.559 what does this mean that the behavior
00:19:19.360 of the residual series here and here in
00:19:22.559 the two parts of the sample by the way
00:19:24.720 when i'm scrolling the cursor is it
00:19:26.960 visible that i'm covering over here
00:19:29.679 i think it is
00:19:31.600 yeah
00:19:32.640 uh
00:19:35.600 it's different
00:19:37.039 which means we are not getting the same
00:19:39.600 thing
00:19:40.559 uh what can if you if we scroll back to
00:19:43.360 the original data
00:19:45.440 you can see here the magnitude okay
00:19:47.679 actually even better here
00:19:49.280 here there's an overall trend and the
00:19:51.039 mag the
00:19:52.400 fluctuations around the strand
00:19:54.480 are kinda modest in this part
00:19:57.840 and then they get much larger here
00:20:00.400 one of the things this can indicate
00:20:02.559 that our
00:20:05.520 series
00:20:06.400 is in its multiplicative dynamics
00:20:08.720 translated to plain english that in this
00:20:10.880 equation
00:20:12.159 trend is actually multiplied by
00:20:14.000 seasonality or and multiplied by by the
00:20:17.120 residuals
00:20:18.480 leading to the to the fact that the
00:20:20.960 level
00:20:21.919 the the level of the fluctuations of the
00:20:23.760 seasonality is bigger here than it would
00:20:27.360 have been here
00:20:29.440 uh
00:20:30.400 can we check that yes we can
00:20:34.159 guys sound like a bomb the builder
00:20:41.679 df
00:20:46.240 period this 12 model i mean by default
00:20:50.000 smallest the model is added
00:20:54.460 [Music]
00:20:57.840 and then we go the same way as before
00:21:10.440 [Music]
00:21:13.520 sorry uh sorry to interrupt you before
00:21:15.600 we move to the next part there are a few
00:21:17.120 questions
00:21:18.640 you want to take so one question is
00:21:21.120 period is not always known do we always
00:21:23.360 try to deduce it manually by looking at
00:21:25.600 trends or is there a mathematical way to
00:21:27.679 find it
00:21:29.440 uh
00:21:31.919 kinda both
00:21:33.919 uh in several instances like for
00:21:36.320 instance here if we have monthly sample
00:21:39.440 data
00:21:40.799 then just about the only period that
00:21:42.720 makes sense is monthly
00:21:45.440 the with the with the duration of the
00:21:47.039 time series if we have something that's
00:21:49.120 for instance daily
00:21:51.280 uh
00:21:52.480 then it's a bit of a judgment call
00:21:54.720 namely
00:21:55.679 if we have say uh daily sample time
00:21:59.120 series
00:22:00.000 if this time series happens and it's
00:22:02.240 over a span of multiple years
00:22:04.960 then in a sales time series we are
00:22:07.280 likely to have um
00:22:10.000 excuse me a weekly seasonality
00:22:12.400 so dependence on day of the week as well
00:22:14.960 as annual one dependence on the month of
00:22:17.120 the year
00:22:18.480 just makes sense people make more
00:22:20.000 shopping on weekends or something yeah
00:22:22.400 on the other hand if the same if we have
00:22:24.240 the c a series over a similar period
00:22:26.480 also sample daily
00:22:28.080 but it's only it's for instance
00:22:30.480 representing temperature
00:22:32.640 then annual seasonality very much makes
00:22:34.960 sense because seasons of the year change
00:22:37.600 weekly doesn't
00:22:39.039 there's no reason to expect weekly
00:22:41.360 pattern within weather
00:22:43.760 so it kind of depends
00:22:45.600 in terms of is there a mathematical way
00:22:47.360 to find it honestly
00:22:50.799 if you can get away with it
00:22:52.240 computationally just assume multiple
00:22:54.960 seasonalities fit a model and then see
00:22:57.200 which ones pop up as significant
00:23:00.320 that's that's about it uh leading to the
00:23:03.039 question the point made by alexandra yes
00:23:05.360 yes year is 12 months and that's why i'm
00:23:08.080 specifying periodical 12. yes correct
00:23:14.880 ali
00:23:15.840 another question is what if we change
00:23:18.000 the period to 11 and find regular
00:23:20.840 periodicity i don't know let's find out
00:23:25.600 there's one way to find out
00:23:31.840 it kind of fitted something
00:23:35.039 so you can well
00:23:37.120 11 times 12
00:23:39.120 you will have the
00:23:40.799 the same behavior the the multiple of
00:23:42.960 the same period
00:23:44.400 uh
00:23:45.360 quite well question is what would you
00:23:46.799 call regular periodicity if you observe
00:23:49.520 this has a variation of a very tiny
00:23:52.559 variation so the com in terms of
00:23:54.080 absolute value between 0.99 and 101
00:23:57.360 and this is the variation the residuals
00:23:59.279 and if we try to do the same thing
00:24:02.159 with
00:24:03.679 periodicity of 12.
00:24:08.640 you have a slightly bigger variation in
00:24:10.480 seasonality
00:24:11.919 and less variation if you compare this
00:24:14.799 one the residuals
00:24:16.880 to this one the one with seasonality 12
00:24:20.640 is more concentrated along the around
00:24:22.400 the level of one
00:24:24.159 it's beijing and inference used in the
00:24:25.760 real world oh absolutely
00:24:27.760 absolutely
00:24:29.200 and
00:24:31.039 best example will be again
00:24:34.000 deferring to the future
00:24:35.840 uh the state space models
00:24:38.159 state space models at the price of
00:24:39.840 setting up uh something
00:24:42.320 relatively simple you're getting
00:24:43.840 bayesian influence for free
00:24:45.600 so proper
00:24:47.440 kosher or
00:24:49.200 kosher proper or
00:24:50.720 confidence intervals
00:24:52.480 uh
00:24:54.080 posterior distribution you name it yes
00:24:57.279 uh did the police recommend the book or
00:24:59.200 the textbook for time series data
00:25:02.400 have a look
00:25:03.919 in the notebooks i'm pretty sure this
00:25:05.600 one is public
00:25:07.039 the
00:25:07.840 one i mentioned here
00:25:12.159 where did i do it this one
00:25:15.360 time c is the basics i am giving some
00:25:17.600 pointers to the literature there with
00:25:19.600 comments
00:25:22.559 seasonality versus period same
00:25:25.279 in practice same people use it
00:25:27.039 interchangeably
00:25:28.400 uh kind of like prediction and
00:25:30.320 forecasting
00:25:31.840 or in the course is whether someone
00:25:34.240 calls uh
00:25:35.919 says stochastic process or random
00:25:38.159 process they mean exact same thing says
00:25:40.880 that they say stochastic they've been
00:25:42.559 reading books by english authors and if
00:25:44.320 they say random
00:25:45.919 process they've been reading books by
00:25:47.120 russian authors that but but it's the
00:25:49.039 same thing
00:25:50.559 uh useful to map to frequency domain 48
00:25:52.880 to find cycles in the ts data yes
00:25:54.880 spoiler alert that's what profit is
00:25:56.640 about
00:25:57.600 that's how profit is handling
00:25:58.799 seasonality under the hood
00:26:01.039 uh speeding up because it's 25 past
00:26:04.480 i if that's okay i'm i'm wondering if
00:26:07.600 it's okay
00:26:09.440 that i handle the questions
00:26:11.840 totally because
00:26:13.679 we we had we had some question i think
00:26:16.640 we missed it
00:26:18.159 so
00:26:19.760 maybe this one is it mandatory for
00:26:21.520 residual to be symmetrical around zero
00:26:23.440 can it be symmetrical around some other
00:26:24.960 number
00:26:26.799 uh what does it intuitively mean that
00:26:28.880 you didn't bother to do the linear
00:26:31.440 series
00:26:32.559 uh actual level around which it is
00:26:35.360 symmetrical it's it's irrelevant because
00:26:38.080 you can always subtract that constant
00:26:39.679 and make it zero what's important is
00:26:42.080 indeed that it's symmetrical so it's not
00:26:44.080 skewed and then it's serially
00:26:45.840 uncorrelated
00:26:47.679 that's that those those are the relevant
00:26:49.440 bits
00:26:50.799 because at the end of the day what you
00:26:52.240 want is you want residuals from your
00:26:54.480 model to be as close to uh white noise
00:26:58.320 so serially uncorrelated same variants
00:27:00.880 uh yada yada
00:27:02.720 that's that that's the 11 bit uh
00:27:04.640 swimming along
00:27:09.600 i'm gonna copy from my little helper
00:27:13.360 everything on the idea that you are that
00:27:16.000 was asked about
00:27:18.080 about autocorrelation or serial
00:27:20.880 dependence overall
00:27:22.799 uh again i refer you to the first
00:27:25.200 notebook that i linked above
00:27:28.159 uh
00:27:29.520 we have two primary devices to see
00:27:31.200 whether things depend are serially
00:27:32.720 dependent on each other autocorrelation
00:27:34.799 partial autocorrelation best way to
00:27:36.880 think about it autocorrelation
00:27:39.039 well it's a correlation of the series
00:27:41.200 with itself so it has various values
00:27:43.200 between minus one and one
00:27:45.279 and what it has on the horizontal axis
00:27:47.679 is lag
00:27:48.960 so
00:27:49.919 this value which is like point 90
00:27:51.760 something probably
00:27:53.200 tells me this is the average correlation
00:27:55.600 between each observation and its direct
00:27:57.520 predecessor
00:27:59.120 uh and so on for feather locks
00:28:02.799 partial autocorrelation
00:28:05.200 is the same thing but corrected for
00:28:07.600 things that are in between
00:28:09.440 meaning
00:28:10.320 if each observation depends on the
00:28:12.399 previous one
00:28:14.159 then
00:28:15.120 obviously there will be dependence
00:28:17.520 between now and two time periods ago
00:28:21.760 now the question wanna answer in
00:28:24.159 practice
00:28:25.919 how
00:28:26.720 the relationship between now and two
00:28:28.640 moments ago is it directly depending on
00:28:31.840 what was two moments ago or through the
00:28:34.080 intermediate step because right now
00:28:36.000 depends on the previous one and previous
00:28:38.000 depends on its own previous
00:28:40.000 uh
00:28:40.960 that's what partial autocorrelation and
00:28:42.880 autocorrelation can help us
00:28:44.720 differentiate between
00:28:48.159 trying to
00:28:50.559 show something
00:28:53.200 around what we are doing here
00:28:55.600 uh did i load the data somewhere
00:28:59.520 i think i did yes
00:29:01.840 uh
00:29:03.200 let's take the data set that we used for
00:29:05.039 our decomposition here
00:29:07.200 and use it to show what can be done with
00:29:10.159 exponential smoothing
00:29:12.399 so we take the energy consumption data
00:29:15.360 visualized here we take
00:29:18.000 2005 as a cut-off so there's a little
00:29:20.320 bit of data going forward but not an
00:29:22.080 awful lot
00:29:23.279 i mean pretty much standard stuff the
00:29:25.360 reason i normalized by divided by a
00:29:27.760 hundred here
00:29:29.120 is that without doing this i was do i
00:29:31.279 was getting some sort of horrible uh
00:29:34.240 overflow method the overflow error which
00:29:36.720 for the life of me i couldn't track so
00:29:38.960 that's why i sold it that's why i sold
00:29:41.039 it like this uh what we can do next we
00:29:44.080 import
00:29:45.120 uh
00:29:51.120 for exponentials moving in the interest
00:29:53.440 of speeding up a little bit i will speed
00:29:57.200 up so yes abhishek if you could handle
00:29:59.679 the questions i'd be very very grateful
00:30:01.600 about that yeah definitely so
00:30:04.159 before we move to the next part there
00:30:06.320 are actually a couple of questions
00:30:08.640 okay three minutes
00:30:10.799 i know we do need to talk about the
00:30:12.960 relation and test when we find the best
00:30:15.840 part
00:30:16.720 sorry
00:30:18.240 oh i'm sorry uh i apologize you go on
00:30:21.919 okay uh no i chose another question so
00:30:24.320 when to choose multiplicative instead of
00:30:26.480 additive for decomposition
00:30:29.120 uh if you have reason to believe that
00:30:31.600 something changes for instance variation
00:30:34.799 proportionally to the level of the
00:30:36.480 process
00:30:37.919 like
00:30:39.120 a
00:30:40.480 bit much maybe to call it domain
00:30:42.080 knowledge but it's a bit like
00:30:44.320 what is the question that you are
00:30:45.919 expecting for instance in the context of
00:30:47.360 your sales
00:30:48.480 if it's a warm day the sales of uh well
00:30:52.480 cold drinks
00:30:54.080 is uh increased by ten thousand
00:30:56.799 so you are expecting a change as an
00:30:58.799 absolute value
00:31:00.320 then you go for additive but if you say
00:31:02.720 i it to jump by upward by five percent
00:31:05.760 then go for multiplicative that's a sort
00:31:08.159 of rule of number on this one
00:31:12.480 let's take another one and then move to
00:31:14.399 the next part
00:31:15.440 the eighties what if happening in this
00:31:16.799 with the increase which is because
00:31:19.919 uh
00:31:21.039 that's the price we are paying for the
00:31:23.519 fact that exponential smoothing is very
00:31:26.240 very simple
00:31:27.760 there are actually potentially important
00:31:30.559 things that we are that we are missing
00:31:33.039 that's sort of what uh
00:31:35.120 well profit and more advanced models can
00:31:37.760 help address
00:31:39.120 uh essentially
00:31:41.600 anything that allows you to handle
00:31:43.279 change points
00:31:45.760 should be able to capture such a
00:31:46.960 phenomenon because i agree i agree
00:31:48.799 that's that's that's a that's a
00:31:50.640 qualitative change in the nature of the
00:31:52.480 process we will touch upon it briefly if
00:31:54.880 there's time today but there will
00:31:56.720 definitely be a separate module which
00:31:59.440 has a working title of something we
00:32:01.039 could this way comes
00:32:02.559 um and that will be dealing with change
00:32:04.720 points and that will be dealing with
00:32:06.159 anomaly detection
00:32:08.399 but it long story short is there a risk
00:32:10.720 will miss it in a simple model like this
00:32:12.399 the answer is yes yes probably
00:32:16.720 uh
00:32:18.480 where were we
00:32:20.240 here import exponential smoothing
00:32:24.080 ah yes
00:32:25.519 if you want to catch up a little bit
00:32:27.600 more on exponential's
00:32:29.840 moving
00:32:31.600 fabricated notebook
00:32:36.880 exponentials moving where were we
00:32:40.840 okay let's
00:32:43.519 let's feed the model
00:32:44.960 uh
00:32:46.159 for those of you
00:32:48.720 just like me
00:32:51.679 shall english-language english language
00:32:54.000 hard language we used to
00:32:58.159 turn back syntax maybe the identical fit
00:33:00.720 on everything and
00:33:02.399 be done with that
00:33:03.760 the syntax of stats models is not
00:33:06.720 exactly the most friendly on occasions
00:33:09.840 and there's a reason for the uh black go
00:33:12.159 away
00:33:14.320 too soon there's a reason for that
00:33:16.080 because
00:33:17.679 stats models has been around for a long
00:33:20.000 time in particular
00:33:22.320 before
00:33:23.519 things were
00:33:25.200 well think that more or less
00:33:26.399 standardized
00:33:27.760 without psychic learning styles and so
00:33:29.679 it does kind of stand out it does feel
00:33:31.279 at times like
00:33:33.039 the 1990s called and said they wanted
00:33:35.120 their centers back
00:33:37.840 uh that's how we specify the model
00:33:41.200 uh values
00:33:43.039 not necessary per se but occasionally
00:33:45.360 again this was written before pandas
00:33:47.519 became a thing
00:33:48.880 so most of the time it works fine if you
00:33:51.519 feed the data frame but sometimes it's
00:33:53.279 throw a tantrum so to be on the safe
00:33:54.960 side i'm just typing it as values
00:33:56.640 numbers values seasonal period same as
00:33:58.799 before because we are working with
00:34:00.159 monthly data and assuming annual pattern
00:34:03.679 as you can see
00:34:05.120 if you want it to be fancy that way we
00:34:07.039 could specify trend basically specified
00:34:09.359 trend and seasonal independently in
00:34:11.359 terms of the nature of the relationship
00:34:13.520 so you might have for instance additive
00:34:15.040 seasonality but multiplicative trend
00:34:16.800 around this or well four combinations in
00:34:19.440 total
00:34:20.560 uh i'm a simple man so i'm going with
00:34:22.639 both multiplicative
00:34:25.119 and then
00:34:26.719 we have
00:34:28.719 fit
00:34:29.599 method
00:34:32.480 that's also there's still the partial
00:34:34.000 encounter thing uh
00:34:36.399 what do we what do we do about it next
00:34:39.440 one we forecast important bit here
00:34:42.960 uh and that's the reason why i gave the
00:34:46.159 well data science buzzfeed title of
00:34:49.280 curve fitting is almost all you need
00:34:52.480 it's important to remember
00:34:54.399 exponential smoothing is not a
00:34:56.399 probabilistic model
00:34:58.160 it doesn't allow for a proper inference
00:35:00.720 it doesn't say anything about the
00:35:02.400 probability distribution it just fits
00:35:05.119 curves that replicate the pattern as
00:35:07.119 well as possible
00:35:08.560 which means in particular that if we are
00:35:11.119 going to predict into the future the
00:35:12.800 only at the only parameter we really
00:35:14.640 need is the la is the
00:35:17.040 time horizon
00:35:21.040 uh that also means that because it's a
00:35:23.440 big pin let's call it politely vintage
00:35:25.839 uh
00:35:28.320 a lot of things are not done
00:35:29.760 automatically
00:35:31.680 uh
00:35:32.480 we calculate the forecast and steps
00:35:34.560 ahead into the future
00:35:36.320 and we calculate the residuals
00:35:38.640 if you are curious
00:35:40.960 what actually is going on
00:35:43.680 in terms of the parameters
00:35:45.760 you can have a look at
00:35:48.560 look at them alpha is a constant for
00:35:51.200 smoothing the level i am glossing over
00:35:53.040 things a little bit because this is
00:35:54.800 discussed in more detail in the notebook
00:35:56.640 i linked above this is first moving the
00:35:58.960 level
00:35:59.839 this is first moving the trend component
00:36:02.560 and this is for smoothing the
00:36:03.599 seasonality all of those are
00:36:06.480 optimized meaning they are fitted to the
00:36:08.640 data
00:36:10.480 to minimize i believe mean squared error
00:36:13.119 but i would have to double check on this
00:36:14.560 one
00:36:15.440 and then you have also the seasonal
00:36:18.079 components
00:36:20.400 uh
00:36:22.560 that are automated that are
00:36:23.839 automatically fitted within the sample
00:36:28.160 so where do we go from here
00:36:30.079 uh we can have a look at the residuals
00:36:33.440 and we get something like this
00:36:40.079 i did it i did that one i did that one
00:36:41.920 above sorry
00:36:43.280 ready
00:36:44.240 uh let's have a look at the residuals
00:36:46.079 because that tells us
00:36:48.000 how much can we actually get away with
00:36:50.880 this one print
00:36:53.359 is a sort of well-known trick to solve
00:36:56.400 the problem that without it mud plot
00:36:58.480 leap in notebooks very frequently
00:37:01.359 produces the plot twice with certain
00:37:02.960 combos of settings
00:37:04.880 i have no idea why but adding print
00:37:08.320 after plotting statements has solved the
00:37:10.240 problem
00:37:11.200 what do we see here
00:37:12.640 uh we see an autocorrelation that's
00:37:14.880 decreasing over time
00:37:16.560 not terribly strongly the blue bar the
00:37:19.920 bluish pale blue light blue poetic blue
00:37:23.839 that you see around it this is the
00:37:25.440 confidence interval uh
00:37:28.480 and i am stepping over the entire
00:37:30.320 bayesian versus frequencies discussion
00:37:32.560 what's a proper confidence interval okay
00:37:34.560 this is my uncertainty in the past maybe
00:37:36.640 better
00:37:37.599 at 95 level meaning
00:37:40.800 we only have to worry about coefficients
00:37:43.680 at lags that stick out
00:37:45.599 so we can pretty much ignore everything
00:37:47.280 from luck what is it
00:37:49.040 7 onward
00:37:51.839 what's
00:37:52.880 what about
00:37:54.800 partial autocorrelation of the residuals
00:38:03.119 relaxed
00:38:08.480 that's a little better
00:38:10.320 we still haven't solved the problem of
00:38:13.040 the dependence occurring here at lag one
00:38:16.320 just about everything else is gone which
00:38:18.160 means the trend component is still
00:38:20.320 making our lives difficult or at least a
00:38:23.440 step one relationship one step behind
00:38:26.800 but most of the others are are taken
00:38:29.440 care of
00:38:31.440 uh i'll spare you the plotting because
00:38:35.040 let's see if there are any more
00:38:36.400 questions that might be useful for this
00:38:39.200 yeah there are a couple of questions
00:38:42.880 and then we jump to profit because i
00:38:44.640 kind of want to show profit a bit more
00:38:46.320 than this one
00:38:48.400 okay so then then let's take the
00:38:50.000 questions that we have
00:38:51.680 uh really quickly so can you give some
00:38:54.079 intuition to understand autocorrelation
00:38:56.240 partial autocorrelation as to what
00:38:59.280 we get from it and how is it helpful so
00:39:01.599 that's this one is actually this last
00:39:03.680 one is actually a pretty good example um
00:39:08.359 autocorrelation
00:39:10.000 tells us okay
00:39:11.760 each observation depends on the previous
00:39:13.760 one
00:39:14.480 and the one before it and the one before
00:39:16.560 it etc etc and keeps dying down so
00:39:19.520 without looking at partial
00:39:20.640 autocorrelation we might be tempted to
00:39:22.720 say okay i need
00:39:25.119 to specify coefficients of my officer
00:39:28.079 each observation depending on
00:39:30.720 each observation xt depending on
00:39:32.400 depending on xt minus one xt minus two
00:39:34.720 xt minus three yada yada
00:39:37.200 however if we look at partial
00:39:38.800 autocorrelation
00:39:40.560 you see that there's significant
00:39:42.320 dependence only on the previous step
00:39:44.400 which means the dependence between x t
00:39:46.560 and x t minus 2 is only through the
00:39:48.480 intermediate one
00:39:49.920 which means it's fine
00:39:52.480 uh it's fine to just
00:39:54.160 start by modeling with first order auto
00:39:56.079 regression first order aggression fancy
00:39:58.640 way of saying each observation depends
00:40:00.880 on it's all only on its own pre on the
00:40:03.520 previous value of itself
00:40:08.480 okay
00:40:11.119 yeah i think
00:40:12.319 there is one more new question about
00:40:14.240 autocorrelation and partial
00:40:15.680 autocorrelation i i hope those questions
00:40:18.480 are also answered
00:40:20.480 one more question
00:40:22.079 is
00:40:23.040 what is auto what is partial auto
00:40:25.200 correlation as opposed to full auto
00:40:27.520 correlation full auto correlation is uh
00:40:31.040 is calculating all that
00:40:32.960 um
00:40:33.839 is calculating the correlation between
00:40:36.880 each observation and the one well one
00:40:39.520 lug
00:40:40.560 uh two lags three lugs before three
00:40:43.040 steps before
00:40:44.800 this one
00:40:45.920 partial autocorrelation
00:40:48.079 well it's the same for lag one
00:40:50.480 but partial autocorrelation between one
00:40:52.720 and two
00:40:53.920 is sorry zero and two i'd like to it's
00:40:56.480 the dependence between x t and x t minus
00:40:59.680 two
00:41:00.640 correct excuse me corrected
00:41:03.520 for the intermediate effects the formal
00:41:06.640 way of uh
00:41:08.880 this correction
00:41:10.240 is by calculating is that this is a
00:41:11.839 conditional distribution
00:41:13.920 let's see actually if wikipedia is any
00:41:17.040 good on that front because if i remember
00:41:20.160 correctly it is
00:41:22.560 partial autocorrelation
00:41:24.580 [Music]
00:41:29.920 yeah
00:41:30.720 that should do
00:41:31.920 that should do this one in particular
00:41:36.480 spend some time looking at this equation
00:41:39.680 and its interpretation and then crawl
00:41:41.599 for the references that should that
00:41:43.520 should that should help you develop an
00:41:45.040 intuition
00:41:46.560 uh
00:41:47.359 overall yeah
00:41:49.200 well i can repeat wikipedia for your
00:41:51.119 benefit but i don't think it will be
00:41:52.800 very useful
00:41:55.440 so another question is in the current
00:41:57.200 data set we have two features what if we
00:41:59.359 have multiple features which are also
00:42:01.040 important how to be that's what i'm
00:42:02.319 switching towards
00:42:03.599 yeah that's that's what i'm swimming
00:42:05.119 towards in proper profit of using
00:42:07.920 we're waiting for profit now
00:42:11.839 i hope you guys have a little more time
00:42:14.160 above what's allocated i i took the
00:42:16.480 liberty to answer this question uh it's
00:42:18.880 about your practical time series
00:42:20.480 notebooks are you going to finish them
00:42:24.000 uh yes
00:42:26.240 yeah
00:42:27.359 i'd rather hope so although i i might
00:42:30.400 end up having to do things in parallel
00:42:32.720 uh juggling between
00:42:34.720 uh
00:42:35.680 well preparing them and presenting in
00:42:37.839 the channel
00:42:38.960 although i don't know maybe i can do two
00:42:40.560 things two things in one go let's let's
00:42:42.960 see but yes
00:42:44.480 yes
00:42:46.160 i hope i'm pronouncing it right uh i've
00:42:48.960 it is very much my intention yes
00:42:52.560 okay um i i guess we can move to the
00:42:56.880 next part now
00:42:58.480 because we are low on time
00:43:00.720 dedicated to a notebook on profit this
00:43:02.960 one
00:43:03.760 the data set i'm using is this
00:43:06.720 uh it's called autonomous greenhouse
00:43:08.319 challenge
00:43:09.599 except i did a sort of adjusted version
00:43:13.359 of this one because for some ridiculous
00:43:15.680 reason
00:43:16.560 when i was reading the original data
00:43:19.359 i was getting some errors
00:43:21.520 related to percentage signs and column
00:43:23.839 names
00:43:24.800 the moment i downloaded it locally saved
00:43:27.440 it and then uploaded as my own data set
00:43:29.359 everything was fine
00:43:31.280 for the life of me i don't know why
00:43:35.040 but this one this one will be made
00:43:36.560 public as well so no worry
00:43:39.760 what have we got
00:43:42.960 uh
00:43:44.400 from prophet let's see
00:43:47.359 it's working no
00:43:49.599 this is good this is a fun part
00:43:52.000 because i tested it on my local
00:43:54.400 installation and the trouble is uh
00:43:57.920 prophet has had a new release since
00:44:00.960 uh and
00:44:02.240 except cable image is not updated so
00:44:05.200 what i'm doing
00:44:06.720 is importing from fb profit
00:44:09.440 and because in the cuddle image the
00:44:11.440 version of the package is 0.7 or
00:44:13.359 something
00:44:14.400 and since then profit has had a 1.0
00:44:17.040 release
00:44:18.160 so
00:44:19.040 pay attention to this one and if you
00:44:21.200 start getting errors check check this
00:44:23.760 one first
00:44:25.760 [Music]
00:44:31.680 this is what we have here
00:44:33.359 uh the data that we are using is from
00:44:37.200 a sort of competition i mean
00:44:39.680 uh i guess you could call it local
00:44:41.119 patriotism from my adopted homeland
00:44:43.520 netherlands is big on agriculture
00:44:46.800 and i mean like if there's a you know a
00:44:49.119 couple hundred square meters there's a
00:44:50.800 chance someone will build a greenhouse
00:44:52.240 here but you can kind of see in the
00:44:53.920 economic statistics of how much food
00:44:55.839 this country exports it's actually
00:44:57.839 double impressive given how shitty most
00:44:59.440 of the soil is
00:45:00.640 it's it's double impressive that way uh
00:45:02.800 and that means also that a lot of the
00:45:04.800 producers in greenhouses are interested
00:45:06.880 in using new technologies
00:45:08.640 such as such as ai
00:45:10.880 in general
00:45:12.880 uh or machine learning at least generate
00:45:15.920 so
00:45:17.119 there was a competition a while back
00:45:19.040 where people were supposed to optimize
00:45:20.800 placements and different controls in a
00:45:23.119 in a greenhouse in a manner that would
00:45:24.960 allow them to optimize the temperature
00:45:27.440 for
00:45:28.640 i believe it was tomatoes they were
00:45:30.480 trying to grow and this is the data that
00:45:33.119 we get that we that they make
00:45:34.640 subsequently made public
00:45:36.480 now what's the first thing that we see
00:45:38.560 a really really weird time stamp
00:45:41.119 that's something that literally falls
00:45:42.960 under the hashtag today i learned
00:45:45.280 because it took me like half an hour to
00:45:47.920 figure out that excel
00:45:50.319 has something called five minute time
00:45:52.400 stance format which is apparently known
00:45:54.400 to everyone who use excel except i don't
00:45:56.880 and the beautiful part is
00:45:59.280 this is the this is their origin of the
00:46:02.000 uh epoch
00:46:04.319 now i grew up with linux as far as i'm
00:46:06.240 concerned epochs start in 1970 but no
00:46:10.079 so if you find yourself in a situation
00:46:12.960 where there's weird stuff like this
00:46:14.640 check for this option
00:46:16.560 kids do try this at home
00:46:18.640 uh we format it
00:46:20.560 daily frequency and with this origin and
00:46:23.920 now we are getting something a little
00:46:25.599 more sane
00:46:26.880 which is because it's blue
00:46:29.359 approximately every five minutes
00:46:33.280 uh
00:46:34.800 i am cutting the corners a little bit
00:46:37.200 here but i will explain to you why
00:46:40.000 uh
00:46:40.800 obviously we need time
00:46:42.640 keep the time variable we need the tr
00:46:45.599 the air temperature that was the one
00:46:47.280 that they used and there were there were
00:46:49.280 some controls that were also used in the
00:46:52.160 model
00:46:53.200 but
00:46:54.160 i mean here
00:46:55.760 we have what 50 columns
00:46:58.560 uh discounting those two that's 48
00:47:01.839 but only this this subset which i as
00:47:05.040 part of my homework prepared earlier
00:47:07.200 were available going forward
00:47:09.760 which means we can safely ignore
00:47:11.440 everything else
00:47:12.800 because if we want to use covariates
00:47:15.520 to predict things along the way going
00:47:17.599 forward
00:47:18.640 then only those are useful
00:47:20.880 there's no point in using variables that
00:47:22.800 we know are not available beyond the
00:47:25.599 time horizon
00:47:27.440 so we subset the data set
00:47:31.200 uh
00:47:34.240 five minutes is a bit of an overkill
00:47:36.800 so let's resample it to our way
00:47:40.800 you can see
00:47:42.640 and then see if we can
00:47:45.040 use we can have anything useful there
00:47:48.640 uh
00:47:50.079 if the data set that
00:47:53.680 this relates to which i think i linked
00:47:56.400 here yes that's the original data set it
00:47:58.800 has a nice read me
00:48:00.720 so if you don't believe me feel free to
00:48:02.559 verify that all the variables that are
00:48:05.280 sub-selected here are numerical and it
00:48:08.400 does actually make logical sense to take
00:48:10.480 their averages
00:48:12.079 which means we are fine with just
00:48:13.760 aggregating everything to its hourly
00:48:15.839 means
00:48:18.319 reset index
00:48:26.079 okay
00:48:27.800 [Music]
00:48:32.800 we can drop some missing values nice
00:48:36.079 thing about profit
00:48:38.480 is that
00:48:39.920 in most cases we are fine with missing
00:48:42.319 values so if we're only focusing
00:48:44.240 exclusively on the target we could leave
00:48:46.800 missing values in the data as you can
00:48:48.880 see there are
00:48:50.160 there's a little bit of nodes
00:48:53.040 uh
00:48:54.000 uni two unique values once we convert it
00:48:56.160 to logical it's mostly the same rows
00:48:59.200 but uh
00:49:02.160 i also want to demonstrate later
00:49:04.480 how this how it works if we uh
00:49:07.839 wants to use external covariance and the
00:49:09.920 weird thing about profit it's f well
00:49:12.880 just it is what it is
00:49:14.480 it's fine to use covariate it's fine to
00:49:16.559 have missing values in the target
00:49:18.240 because those just get interpolated but
00:49:20.400 not in the covariates we are trying to
00:49:22.000 you to explain them
00:49:23.760 so being a simple man i'm just killing
00:49:25.599 all of them
00:49:28.559 uh what does this actually look like
00:49:34.960 make sure it's actually visible
00:49:37.760 we get something like this
00:49:40.079 a little bit of a trend going forward
00:49:43.280 some drop at the start but our data if
00:49:45.280 you check the readme in detail i
00:49:47.040 apologize by the way that i'm
00:49:48.240 accelerating a bit but it took went
00:49:50.559 went a little slower than i thought it
00:49:52.000 would do the exponential smoothing i
00:49:53.280 apologize about that
00:49:55.119 uh
00:49:56.000 this data is spanning a period for half
00:49:58.640 a year so this is january and also
00:50:00.960 netherlands well what passes for winter
00:50:03.440 in this country usually happens in the
00:50:05.040 netherlands in in january
00:50:07.119 so
00:50:08.160 that's not much of a shocker that
00:50:09.760 atmospheric conditions had some
00:50:11.119 spillover effect
00:50:12.559 on what we are doing here
00:50:16.079 now
00:50:16.880 the relevant bit around profit
00:50:20.960 actually for those of you who've never
00:50:22.720 dealt with profit
00:50:24.319 profit
00:50:25.760 is fantastic profit is flexible profit
00:50:29.359 works out of the box with a lot of
00:50:30.880 places in a different frequencies but it
00:50:32.960 doesn't mean it also means unfortunately
00:50:35.359 it comes with a few quirks and one of
00:50:37.200 those is
00:50:38.400 hard coded names for columns
00:50:41.359 i mean that for me the first time i saw
00:50:43.680 it was like what
00:50:46.480 i'm like dude it's two thousand well
00:50:48.400 what was it 16 or so
00:50:50.480 but that remains effect that's what we
00:50:52.240 are dealing with here
00:50:55.359 so
00:50:56.240 we have to rename columns the timestamp
00:50:58.160 column always needs to be called yes
00:51:00.960 and the
00:51:02.800 target value column
00:51:04.559 needs to be called
00:51:06.960 uh
00:51:10.240 it needs to be called why
00:51:14.800 oh
00:51:15.680 okay
00:51:17.839 i'm sure can you say something because i
00:51:19.680 had some glitch i'm not sure i'm still
00:51:21.280 online
00:51:22.480 yeah you are online yeah you're
00:51:23.920 listening
00:51:24.800 you are alive and i am alive that's good
00:51:26.880 it's like talking to a wall sometimes
00:51:29.040 right
00:51:30.800 i i don't mind it's just that it's a bit
00:51:33.440 of a waste of time for everybody so they
00:51:35.839 just
00:51:36.640 face on the screen
00:51:38.160 everything is fine yeah that was a
00:51:39.760 sanity check
00:51:41.119 uh this is profit so this is the quirk
00:51:43.440 of prophet
00:51:44.559 uh
00:51:46.800 oh what the hell now
00:51:54.880 you know what i'm gonna ignore the
00:51:56.400 plotting for this moment because i think
00:51:58.319 i over did i overdid something along the
00:52:00.400 way
00:52:02.640 too bad let's see
00:52:04.720 uh
00:52:05.599 what can we do in terms of seasonality
00:52:08.400 or modeling itself once we have the data
00:52:11.839 data frame that we'll be using
00:52:13.839 instantiated
00:52:15.599 um
00:52:17.119 well we have set it up in the right
00:52:18.400 format now we can set up a model
00:52:22.240 if you recall what i did earlier is i uh
00:52:26.870 [Music]
00:52:28.240 specify i mentioned that this data spans
00:52:31.760 a period of six months
00:52:34.079 what does this mean and it's sampled
00:52:35.920 hourly
00:52:37.200 uh by default
00:52:39.440 profit will try to fit any seasonality
00:52:42.960 when it makes sense
00:52:44.480 we have six months so it doesn't make
00:52:46.960 sense to fit well you shouldn't period
00:52:50.160 fit a annual seasonality
00:52:52.880 like general rule of thumb unless you
00:52:55.599 have
00:52:56.720 twice the size of your cycle do not feed
00:53:00.240 the seasonality so if you have less than
00:53:02.079 two years of data don't fit annual
00:53:04.000 seasonality if you have less than two
00:53:06.079 weeks don't fit weekly seasonality etc
00:53:09.599 uh
00:53:10.480 we do however have a lot of weeks so by
00:53:13.359 default the model would be attempting to
00:53:15.599 fit weekly seasonality doesn't make
00:53:17.920 sense for a physical phenomenon
00:53:20.160 we skip it
00:53:22.160 uh
00:53:24.319 profit in terms of the underlying
00:53:26.480 structure is effectively a smarter
00:53:28.319 version of
00:53:30.079 exponential smoothing namely all it does
00:53:32.640 it's it's fitting curves it's just
00:53:34.720 fitting curves in
00:53:36.160 slightly sophisticated manner
00:53:40.000 if you are interested in at least crash
00:53:42.079 intro to this one i refer you to the
00:53:44.240 linked notebook from the practical time
00:53:46.559 series sequence
00:53:48.480 uh by default it does
00:53:50.720 that does that that being said the
00:53:52.800 creators of prophet acknowledged very
00:53:55.119 correctly in my view
00:53:56.800 that
00:53:57.599 point forecasts which is for instance
00:53:59.680 what um
00:54:01.760 well exponential smoothing gives us
00:54:03.839 are not good enough we would like to be
00:54:05.599 no we would like to have some idea
00:54:08.319 uh how much off we might be some measure
00:54:11.200 of uncertainty around it
00:54:13.040 and this is what uh
00:54:16.960 this this is achieved
00:54:19.040 by generating monte carlo samples
00:54:21.680 from the from the data and then
00:54:24.000 averaging those let's see if this works
00:54:26.640 yes it seems to single
00:54:29.520 uh we set it up okay
00:54:32.319 now we call fit so that's so weekly
00:54:34.800 seasonality we disable
00:54:36.960 because it doesn't make sense here
00:54:39.440 we wanna inter we want uh uncertainty
00:54:42.319 measures certainly bend uh at 95 level
00:54:45.920 meaning on average we expect
00:54:48.880 to cover what's going on in 95 percent
00:54:51.520 of cases
00:54:52.480 but important point this also means we
00:54:55.920 expect our prediction to stick out of
00:54:58.400 the interval in five percent of cases
00:55:00.880 because if you construct 95 if you
00:55:03.839 specify 95 interval and it always
00:55:06.799 captures your data
00:55:08.400 well then it's not 95
00:55:10.160 something went wrong with your
00:55:11.200 calculation mcmc samples uh well how
00:55:14.240 many samples are we using for that
00:55:17.119 and this is initiated at the level of
00:55:19.839 model specifications so the next thing
00:55:21.760 we do after that
00:55:24.319 sorry
00:55:26.630 [Music]
00:55:30.240 what is it what is it now
00:55:34.240 ah i didn't rename it
00:55:36.160 i have something to do with that
00:55:41.839 yes
00:55:43.119 that's fine
00:55:46.880 sometimes i hate the interface so much
00:55:50.480 show me this stuff and i can't see what
00:55:52.240 i'm typing
00:55:55.119 there you go
00:55:58.640 [Music]
00:55:59.839 metric ton of warnings
00:56:02.480 so that's something i probably need to
00:56:04.720 investigate myself a bit more
00:56:09.760 but if you notice
00:56:11.599 i mean for some reason normally when you
00:56:13.520 launch it there's a widget that's
00:56:15.599 showing the process uh kind of like tqdm
00:56:18.880 showing the progress of your fitting
00:56:20.640 process for some reason it doesn't work
00:56:22.799 in the catanos right now
00:56:24.400 i don't know why
00:56:25.760 uh
00:56:26.720 those are the messages related to
00:56:29.599 fitting
00:56:30.400 and if you look here current metropolis
00:56:32.720 proposal metropolis sampling that's an
00:56:35.680 that's part of the underlying algo for
00:56:37.359 the sampling so all the warnings here
00:56:39.760 that's some weird stuff happening in the
00:56:41.760 simulation part
00:56:43.599 uh
00:56:44.480 i'm gonna do what's commonly commonly
00:56:46.720 done in business uh in such situations
00:56:49.680 namely applause employ an ostrich
00:56:51.760 strategy i'll stick my head in the sand
00:56:53.599 and pretend it's not there
00:56:55.680 uh
00:56:57.040 how do we build a forecast once we have
00:56:58.960 fitted uh successfully fitted the profit
00:57:00.880 model
00:57:02.079 it's a
00:57:03.119 bit of a mouthful because we have
00:57:05.119 something called make future data frame
00:57:07.280 on the upside it's pretty easy to
00:57:09.680 understand what it does the downside
00:57:12.000 it's well takes it takes a second to
00:57:14.319 type
00:57:15.760 uh then
00:57:17.359 we
00:57:20.319 all
00:57:21.760 prediction on this model
00:57:24.079 and we would also like to
00:57:28.480 put the components of our model
00:57:31.599 to see what actually happened
00:57:42.799 yes
00:57:43.760 it's working sing hallelujah
00:57:46.160 uh
00:57:48.000 the bluish
00:57:50.000 band around the dark blue line
00:57:53.359 this is our j this is our generated
00:57:55.359 confidence interval
00:57:57.119 shows us that the uh well
00:58:00.000 trend over this period
00:58:02.319 say in this instance it's whatever 21.6
00:58:06.640 but there's quite a bit of uncertainty
00:58:08.240 around it and even more towards the end
00:58:10.880 of the sample more importantly in terms
00:58:13.119 of daily seasonality we do have a
00:58:15.359 pattern that's fitted
00:58:20.480 but there is uncertainty around it
00:58:22.960 in particular as a general rule you have
00:58:25.520 something like
00:58:26.640 like at this point
00:58:29.200 this value itself say around whatever
00:58:31.440 10ish
00:58:32.720 this is non-zero but the confidence
00:58:35.280 interval around it includes zero
00:58:38.240 which means what does it mean in
00:58:39.520 practice that we cannot meaningfully say
00:58:42.960 that uh this is significantly different
00:58:45.119 than zero in general if your confidence
00:58:47.680 interval includes zero don't bother
00:58:49.839 because that means your difference from
00:58:51.520 baseline is is not relevant
00:58:54.960 any questions around this one so far
00:58:59.920 there are a few questions
00:59:02.240 hit me
00:59:03.359 is metropolis algorith okay sorry you
00:59:05.200 choose i apologize
00:59:07.040 no problem
00:59:08.559 yeah i i will take it from the from the
00:59:10.400 beginning so
00:59:12.720 the question is i've heard of something
00:59:14.000 about data stationarity which we need to
00:59:16.640 check before solving time series problem
00:59:18.400 can you please explain a little bit
00:59:20.400 about that
00:59:21.520 the beauty of profit is we don't care
00:59:25.280 that would be the short answer long
00:59:27.200 answer work long answer would be the
00:59:29.119 following
00:59:30.079 the reason people have okay
00:59:32.480 stationarity is a very nice concept
00:59:34.960 uh i think i talked a bit about this in
00:59:37.680 the first notebook
00:59:39.200 stationery is a very nice concept what
00:59:40.720 does it mean practice
00:59:42.880 to tomorrow
00:59:45.520 or today the variation how things change
00:59:48.559 around me
00:59:50.000 is quite similar to the way it's going
00:59:51.599 to happen tomorrow and the way it
00:59:52.880 happened yesterday
00:59:54.640 so there will might be for instance if
00:59:56.880 say a temperature is stationary that
00:59:59.200 means there will be variation
01:00:03.119 uh
01:00:04.079 but it will be between 10 and 20 degrees
01:00:06.960 it was between 10 and 20 yesterday
01:00:08.880 between 10 and 20 today it's gonna be 10
01:00:10.960 and 20 tomorrow then is stationary it's
01:00:13.760 useful it makes life very easy when you
01:00:16.400 want to
01:00:18.319 develop things
01:00:19.839 however there's there's too big however
01:00:22.640 however number one it's a horror show to
01:00:25.520 verifying practice
01:00:27.200 like the full statistical definition you
01:00:28.880 cannot verify it in practice because you
01:00:30.319 have to check everything that's not
01:00:32.000 possible problem number two most things
01:00:34.640 in the real world are not stationary
01:00:37.119 so how did decisions historically
01:00:39.200 approach it
01:00:40.480 uh
01:00:41.520 well i always like the joke with the
01:00:43.920 piece of wood how does a mathematician
01:00:46.000 is giving a piece of wood with two nails
01:00:47.599 in it one hammered all the way
01:00:50.240 the other partly
01:00:52.799 uh i'm gonna get to that one uh how does
01:00:56.480 he solve it well he first removes the
01:00:58.079 one that's hammered all the way in
01:00:59.760 because it's more interesting
01:01:01.520 and then he moves to the one that's
01:01:02.720 halfway in and what's the first thing he
01:01:04.799 does he beats it all the way in to
01:01:06.640 reduce it to a known case and that's how
01:01:09.119 stationarity has historically been
01:01:10.720 approved
01:01:11.839 and history approached people have been
01:01:14.079 solving models for stationary case
01:01:16.240 because it's more fun
01:01:17.760 and then they were devising ways of
01:01:19.839 reducing any problem to something
01:01:22.079 stationary
01:01:23.440 trouble is you reduce the problem
01:01:25.119 something stationary frequently you you
01:01:26.880 lose a lot of information that's why
01:01:28.880 people started developing methods like
01:01:31.040 for instance state space or integrated
01:01:32.799 lima where you explicitly model the fact
01:01:35.440 that it's non-stationary
01:01:37.599 so
01:01:38.640 do you have to solve it to summarize uh
01:01:42.240 do you need to check for stationarity
01:01:44.240 not always
01:01:45.359 not always in some instances like you
01:01:47.040 want to use arma models yes but but not
01:01:49.760 always
01:01:52.480 okay uh there are a few questions a
01:01:54.640 couple of questions around uh profit
01:01:56.799 model and com comparison with
01:01:58.799 exponential models so this is one of
01:02:00.400 them
01:02:01.599 a little bit more elaboration yeah
01:02:05.680 before you answer i will also
01:02:08.799 ask the next one which is about some
01:02:11.280 advantages and disadvantages of tree
01:02:13.200 based models over
01:02:15.200 statistical models like arima arima i
01:02:17.520 guess we are doing
01:02:18.839 another advantages and disadvantages for
01:02:21.760 what just for forecasting time series
01:02:23.839 depending uh
01:02:26.799 arima works out of the box
01:02:29.119 without preparing anything for the
01:02:30.720 future
01:02:32.079 if you can create features that
01:02:35.039 represent lags in a manner that's not
01:02:37.920 horrible
01:02:39.200 then yes three models is almost
01:02:41.119 guaranteed to be tariman
01:02:43.599 if you can create features that
01:02:45.039 represent likes arima by definition
01:02:47.119 works with lugs and locked values
01:02:49.119 automatically if your three does not
01:02:52.480 three three based model and symbol of
01:02:54.400 three whatever is is pretty much a
01:02:57.039 regression it doesn't take into account
01:02:59.039 time
01:02:59.920 it's snapshot by snapshot by snapshot by
01:03:02.079 snapshot
01:03:04.400 if you can capture the sequential nature
01:03:07.119 of the process by having lag features
01:03:10.480 um
01:03:12.799 from the top of my head i'd say i'm
01:03:14.640 almost sure uh
01:03:18.319 a tree-based model will beat arima
01:03:21.119 i don't well i in fact i'm trying to
01:03:23.520 think of an example where it wouldn't
01:03:25.280 work
01:03:26.400 uh maybe three were overfeeding
01:03:30.000 ah that's a good point
01:03:31.839 that that might actually work uh arima
01:03:34.160 might work
01:03:35.839 if there if you have very little data
01:03:38.559 because in order to converge uh things
01:03:41.200 like well xg boos random forest you name
01:03:44.720 it like gbm cardboard
01:03:47.839 they need data they are not as data
01:03:50.240 intensive as well as deep learning
01:03:52.400 models
01:03:53.839 who suffer from the same problem by the
01:03:55.280 way if you don't have enough data arima
01:03:57.599 will work arima will work on very little
01:03:59.680 data and so for the matter so for so for
01:04:02.000 that matter will exponential smoothing
01:04:04.799 which is kind of what this
01:04:07.039 models were built for and that's a funny
01:04:09.280 thing that you know they were built in
01:04:10.799 the 50s or like a nasa computers had
01:04:14.480 less memory than my phone had phoned us
01:04:16.559 and my phone is not last year flagship
01:04:19.280 uh
01:04:20.880 and then the amount of data we needed
01:04:22.799 started using exploded so much our
01:04:25.280 computing power just can't keep up
01:04:27.280 which means we need to avoid latency and
01:04:29.280 whatnot
01:04:30.319 uh we need models that work real time
01:04:32.319 almost so use at most a few last
01:04:34.640 observations you know
01:04:37.599 if you need to serve a prediction to a
01:04:40.160 user
01:04:41.359 who
01:04:42.400 is impatient won't wait for what you
01:04:44.000 have to say
01:04:45.200 um you need to do it at runtime you
01:04:48.000 can't call an expensive model back well
01:04:50.480 unless you're google nasa
01:04:53.039 but normal people can't which means your
01:04:55.520 model you need to be able to wrap up in
01:04:57.280 i don't know for like her elastic search
01:04:59.200 query
01:05:00.240 well good luck wrapping up a deep
01:05:01.839 learning model with that
01:05:03.440 arima easy
01:05:07.119 okay so peasy this question we skipped
01:05:09.680 in the in the beginning so could we
01:05:11.039 mention covet period and if we profit
01:05:12.960 and expect it to be treated by it
01:05:16.799 could you mention
01:05:18.240 period
01:05:19.599 so like
01:05:20.799 i guess the question is this is the
01:05:22.640 start of the pandemic
01:05:27.839 that's a pet peeve of mine and by pet
01:05:30.559 peeve i mean psychotic hatred
01:05:33.599 because the last few months have been a
01:05:35.920 journey of discovery for a lot of us i
01:05:38.319 think for me for me for sure
01:05:40.240 in terms of
01:05:41.520 how horrible the raw data uncovered was
01:05:45.680 so until
01:05:47.119 i get data uncovered that i would
01:05:49.359 actually consider reliable i wouldn't
01:05:52.000 treat it with profit
01:05:54.400 that's that's that's simple
01:05:58.079 that's simple i just don't trust the
01:05:59.599 data because if we post factum find out
01:06:02.160 from uh well those that i noticed dutch
01:06:05.599 health authorities british health
01:06:07.119 authorities american ones
01:06:09.119 that they admit
01:06:10.720 something between 30 and 70 percent of
01:06:13.680 people for instance count who counted as
01:06:15.520 dead from covet
01:06:17.760 actually die off covet
01:06:19.440 they just happened to have copied by
01:06:20.799 that of something else
01:06:22.400 that means
01:06:23.440 the definitions in our data are unclear
01:06:26.960 and messy
01:06:28.400 and until we fix that there's no point
01:06:31.119 in touching any forecasting if we can't
01:06:33.280 trust the raw data and right now we
01:06:35.599 cannot because definitions are changing
01:06:38.400 and that's that's a massive problem so
01:06:40.960 before this is fixed i wouldn't touch
01:06:42.640 copy i wouldn't touch copy data with
01:06:45.119 profit or any other statistical tool
01:06:48.640 okay
01:06:50.240 and the question is can you talk a bit
01:06:52.480 about time series forecasting
01:06:54.799 more than one variable predicting
01:06:56.799 temperature of multiple cities
01:07:00.160 uh
01:07:01.039 it will later
01:07:02.480 we will later there are two points where
01:07:04.319 this definitely will figure uh one of
01:07:06.960 the nice things about state space models
01:07:09.920 which are definitely scheduled is that
01:07:12.559 switching from univariate to multi-value
01:07:15.599 forecasting it's pretty simple because
01:07:17.920 all you need to do is just well you
01:07:20.000 replace colors with matrices but the
01:07:22.799 equations look exactly the same and work
01:07:24.880 exactly the same manner
01:07:26.720 um
01:07:28.559 where else will we touch it in general
01:07:30.559 in multivariate models
01:07:32.720 uh well we'll touch upon it in lstm bit
01:07:35.839 if there's continued interest then we
01:07:38.000 get to this
01:07:40.480 uh can we use copied period this
01:07:42.240 holidays for us external aggressors in
01:07:43.920 prophet yeah jesus that's a long holiday
01:07:47.359 that's a long holiday
01:07:50.319 i mean in theory we could
01:07:53.920 but like i said
01:07:57.200 my single biggest problem with anything
01:07:59.920 any relation anything about predicting
01:08:02.480 covet cases and what not
01:08:06.160 data is the the raw data that goes as
01:08:09.039 input is a mess
01:08:11.200 because if you look at the last two
01:08:12.640 years of data and then you find you'll
01:08:14.720 feed it into your model that will work
01:08:16.238 that will work just fine
01:08:18.960 say you want to look at kovic deaths
01:08:22.000 and then post you find out
01:08:23.759 actually the methodology of counting
01:08:26.000 what constitutes dying off covet as
01:08:28.560 opposed to wake of it which is not the
01:08:30.960 same thing
01:08:32.640 uh it was changed midway through
01:08:35.520 uh
01:08:36.880 yeah like what good is it
01:08:41.679 it's it's a huge problem it's a huge
01:08:43.600 problem so data data data data
01:08:47.040 before we get into statistical modeling
01:08:50.719 okay uh let's take one final question
01:08:53.920 before moving to the next part
01:08:56.130 [Music]
01:08:58.238 we're close to an end so we can go along
01:09:00.238 with this okay does the profit work well
01:09:02.319 with large data where there is spike in
01:09:04.560 activity at some intervals
01:09:07.040 like intermittent forecasting things
01:09:08.960 like demand
01:09:10.880 uh if that's what you mean then yes
01:09:13.279 yes and in there there's a related one
01:09:16.000 below it how to handle gaps in the time
01:09:17.920 series in profit ignore them
01:09:21.040 in prophet literally ignore them because
01:09:23.359 prophet
01:09:24.479 if you scroll back to the
01:09:28.319 where did i have it the original
01:09:30.319 equation
01:09:32.399 here
01:09:33.679 the subscription
01:09:36.000 you can think of uh
01:09:37.649 [Music]
01:09:41.198 is it hazardous
01:09:42.799 dp i didn't i didn't know that
01:09:44.880 i didn't know that i grabbed the first
01:09:46.640 background that was that was like decent
01:09:49.279 quality and open source uh
01:09:52.319 you can think of this as being a
01:09:53.839 function of time
01:09:56.159 so you are fitting a function
01:09:58.800 at no point when you're trying to fit a
01:10:01.199 function does it specify that the points
01:10:03.520 in which you know the data values of the
01:10:05.360 function have to be equally spaced so if
01:10:07.760 you have longer gaps
01:10:10.000 whatever
01:10:11.360 you just ignore them highly volatile
01:10:13.679 data
01:10:15.360 uh
01:10:17.360 i will pick a starting value from
01:10:20.000 honestly start with default values and
01:10:22.960 then see what's wrong
01:10:25.199 like it's varying too much start
01:10:26.960 increasing the regularization
01:10:29.040 because you're at mcmc parameter that's
01:10:31.199 that's actually plural there's multiple
01:10:33.199 parameters to mess with
01:10:34.880 uh you tinker with it
01:10:37.920 if you have enough time
01:10:40.640 you optimize them as hyper parameters
01:10:43.280 whatever you have multiple times is the
01:10:44.880 consumer production
01:10:46.800 as they are coming from different data
01:10:48.320 sets
01:10:52.239 uh
01:10:53.280 well if they come from different data
01:10:54.880 says the question is why would we treat
01:10:56.960 them jointly and not as free separate
01:10:59.360 univariate problems
01:11:04.880 is there to fit
01:11:06.320 yeah
01:11:07.120 yeah that would be my question to the
01:11:09.040 answer the question by normander
01:11:12.400 we have no reason to assume they are
01:11:13.679 from the same data set
01:11:15.520 uh
01:11:16.640 just treat them independently
01:11:18.800 like unless you have information about
01:11:20.719 dependence between them
01:11:22.719 why would you make your life difficult
01:11:24.800 by trying to have a
01:11:26.880 three-dimensional problem or ten
01:11:28.400 dimensional instead of ten univariate
01:11:30.320 ones there's there's no real reason
01:11:33.280 so yeah i see that there are more
01:11:35.679 follow-up questions on the covet stuff
01:11:38.239 uh i think youtube is going to put a
01:11:40.719 label on this video
01:11:44.000 well we'll be in good company together
01:11:45.760 with juror
01:11:48.000 okay so let's let's take this one so can
01:11:50.560 i use a naive forecast so this is
01:11:52.320 related to the question that you
01:11:53.920 answered earlier
01:11:55.840 so
01:11:57.440 probably
01:11:59.520 yes
01:12:00.320 i mean at least with naive forecast you
01:12:02.080 know where you're wrong
01:12:03.520 and you know what's blowing up and you
01:12:05.440 are not making unrealistic assumptions
01:12:08.480 based on fl deeply deeply flood data
01:12:14.080 there's a question from kiran are we
01:12:15.679 covering neural profit in so far as we
01:12:18.080 squeeze in the time we'll show you the
01:12:19.920 profit but
01:12:22.159 well no i didn't i
01:12:26.159 as it used to be said in academic
01:12:28.400 textbooks comparing the performance is
01:12:30.800 left as an exercise to the reader
01:12:33.040 and the reason for that is that
01:12:36.960 neural profit
01:12:39.120 improves on profit
01:12:40.960 by adding the ar net
01:12:43.520 by the way since we're talking about ar
01:12:45.360 net for those of you who haven't
01:12:47.840 i wholeheartedly recommend
01:12:51.120 reading the paper
01:12:53.199 neural profit and then component
01:12:58.800 this one
01:13:03.679 let's see what can we ah
01:13:06.960 somewhat relevant
01:13:08.880 uh i'm wrapping up with the prophet if
01:13:11.199 that's if that's okay with everyone
01:13:13.199 because there's one more
01:13:14.960 yes so let's wrap
01:13:16.840 up on this part and after that uh do you
01:13:20.640 do you want to like continue or take
01:13:22.960 more questions
01:13:24.320 i have time so i'm good okay sure
01:13:27.040 i i have some time
01:13:29.280 well within reason i assume at some
01:13:31.120 point people this will decide they have
01:13:33.280 something better to do on their saturday
01:13:36.320 but let's hope not immediately
01:13:38.400 uh we're gonna do the same thing but
01:13:42.080 with without the mcmc samples
01:13:45.120 because then calculating the volatility
01:13:46.960 the performance matrix is going to take
01:13:49.440 absolutely forever
01:13:51.199 this is getting the same thing but
01:13:52.560 without without the confidence intervals
01:13:54.880 and the reason i want to use this one
01:13:57.280 is to show what i personally consider
01:13:59.600 one of the coolest things about profits
01:14:02.239 because anybody who's ever worked with
01:14:06.239 time series data
01:14:07.840 knows what an absolute joy it is to
01:14:10.320 write your own wrappers around metric
01:14:12.400 calculation especially if you want to
01:14:13.840 evaluate performance of the close across
01:14:16.400 different horizons
01:14:18.000 you know like the walk forward
01:14:19.520 validation that you say over multiple
01:14:21.679 years you have first two years as
01:14:23.360 training predicted next month then shift
01:14:25.840 your window by one month and repeat it
01:14:28.719 that's what i mean by walk forward
01:14:31.199 well the beauty of the beauty of the
01:14:33.440 thing about prophet
01:14:35.520 you don't care the good people wrote it
01:14:37.520 for you
01:14:39.840 so how does it work
01:14:41.760 well you import from diagnostics
01:14:44.640 uh a function called cross validation
01:14:46.880 sorry yeah functional cross-validation
01:14:49.280 performance matrix i'm getting to what
01:14:51.520 does each of them do we have a fitted
01:14:53.360 model
01:14:54.480 which is the reason i refitted it here
01:14:56.640 without them cmc samples to have
01:14:58.400 something that works faster
01:15:00.320 in in evaluation
01:15:02.400 uh
01:15:06.000 specify the initial period
01:15:08.640 which is how long
01:15:10.640 is the training data we start with
01:15:13.760 we say we specified argument period
01:15:17.040 namely how long is the shift how big is
01:15:19.120 the shift by which we keep moving our
01:15:21.120 training data
01:15:23.440 there's a bunch of other ways to specify
01:15:25.120 the syntax i'm just i'm just being lazy
01:15:27.280 and specific
01:15:28.400 going with expression in hours horizon
01:15:31.360 pretty much self-explanatory
01:15:33.760 going 24 hours ahead
01:15:37.040 what would it look like
01:15:40.400 all right now that we did work no it
01:15:41.920 doesn't
01:15:44.320 okay i don't get it
01:15:46.800 i hope you guys can survive with the the
01:15:48.800 bunch of diagnostics that's being
01:15:50.560 printed along the way
01:15:53.880 [Music]
01:15:58.480 we can we can answer a question
01:16:00.800 while i'm waiting for the widget to
01:16:02.159 finish
01:16:06.480 i have seen forecasting
01:16:08.320 uh
01:16:11.040 yes
01:16:12.239 yes
01:16:13.760 yes outliers outline especially
01:16:17.120 um
01:16:18.840 outliers
01:16:20.640 uh
01:16:21.760 special days that we know have impact or
01:16:25.360 thing on things but they are not fixed
01:16:29.199 and the missing values
01:16:31.040 can be pretty much handled in a similar
01:16:32.640 manner uh the thing about the special
01:16:35.679 days that are not fixed like christmas
01:16:38.880 is fixed it's always 24 25th i'm sorry
01:16:42.159 the actual christmas
01:16:43.760 uh 25th of december
01:16:46.880 but uh you forgot when christmases
01:16:51.199 sorry
01:16:52.239 you forgot when christmases
01:16:55.360 well i'm po
01:16:56.560 no it's eastern versus western europe oh
01:16:58.960 yeah okay okay
01:17:00.800 in poland the biggest deal is actually
01:17:02.560 christmas eve so the dinner on christmas
01:17:04.960 eve where is in western europe it's the
01:17:07.280 the day of christmas itself it's about
01:17:09.920 that no come on i'm not that old
01:17:12.239 so
01:17:14.880 i'm just making a joke
01:17:17.760 i'm playing along uh building with a
01:17:19.840 topic but special days are things that
01:17:22.159 you know will happen but you don't know
01:17:23.679 exactly where so for instance to use
01:17:25.840 well depending wherever you guys are
01:17:27.600 listening to this stream from or
01:17:29.679 watching uh in europe uh easter
01:17:33.440 it is there but it's moving every year
01:17:36.000 uh
01:17:37.199 ramadan
01:17:38.560 those kinds of things we know they
01:17:40.719 happen but they fluctuate during the
01:17:42.960 year so some those kinds of special uh
01:17:46.719 public holidays usually deriving from a
01:17:48.960 religious context that's why they became
01:17:50.480 private public holidays
01:17:52.320 um
01:17:55.040 that's how you capture them same for
01:17:57.280 same for outliers
01:17:58.719 okay this has finally finished hooray
01:18:01.520 after already 30 seconds
01:18:03.440 what does this horror show demonstrate
01:18:06.080 ah that's what i wanted
01:18:09.679 we have a fitted model we specify the
01:18:11.840 parameters what do we get
01:18:14.000 for each timestamp
01:18:16.960 this is a weird ordering of columns
01:18:19.040 which for the life of me i don't
01:18:20.480 understand but i hope you can bear with
01:18:22.239 me
01:18:23.199 this is the act y is the actual value on
01:18:26.000 this day
01:18:27.280 that the real value because we are doing
01:18:29.199 validation inside the original sample so
01:18:31.280 we know the ground truth
01:18:33.040 uh y hat is the prediction
01:18:36.000 uh y had lower y hat upper lower and
01:18:39.040 upper ends of the confidence interval we
01:18:41.360 can do it for the point for the for this
01:18:43.920 one because in order to get
01:18:46.719 confidence bands around when certain
01:18:48.719 events around seasonality that's where
01:18:50.640 we need proper mcmc sampling if we only
01:18:53.679 want it around the trend or the basic
01:18:55.280 poise point forecast we can get away
01:18:57.280 with doing maximum posteriori it's not
01:18:59.760 that good so there was a question
01:19:01.360 earlier about patient statistics it's
01:19:03.199 not that good but it's fit for purpose
01:19:06.640 why had the upper white head lower
01:19:08.800 cut off what was the end of the training
01:19:11.520 period that we used in this instance
01:19:14.560 this is nice
01:19:15.760 this is useful
01:19:17.120 this is also bloody overwhelming at
01:19:19.920 least for me so what can we do about
01:19:22.320 this
01:19:23.760 we can very elegantly
01:19:26.080 aggregate
01:19:28.000 by doing the following performance
01:19:30.400 metrics
01:19:31.679 when we feed the object fitted in the
01:19:33.840 previous step
01:19:35.360 let's argument
01:19:38.880 this is what we get
01:19:40.960 a very elegant summary showing for
01:19:44.080 forecasts three days ahead this is mean
01:19:46.960 squared error between square zero mean
01:19:48.719 average zero mean average percentage
01:19:50.560 mean
01:19:51.679 md i believe is median average
01:19:53.600 percentage error and coverage of your
01:19:55.760 confidence band
01:19:58.000 for different horizons
01:20:00.000 uh
01:20:01.040 pretty much clear i think
01:20:06.400 rolling forward
01:20:08.480 to demonstrate one last bit that i want
01:20:10.560 to show if we can squeeze it i hope you
01:20:12.560 guys can bear with me for a little
01:20:14.480 longer
01:20:16.480 if you recall
01:20:17.760 in the early part
01:20:20.239 what we had was a bunch of
01:20:22.480 uh explanatory variables
01:20:28.320 yes
01:20:29.600 right we had we only used prime and the
01:20:31.679 target itself
01:20:33.120 but as was specified in the rig me from
01:20:35.440 the data of the competition
01:20:37.600 uh
01:20:38.560 there are others that can also be used
01:20:40.080 as covariates there's a lot of them
01:20:41.920 there's a lot of them so
01:20:43.679 let's make a small sub selection uh
01:20:47.679 in order to be properly
01:20:50.080 properly proper here what we should be
01:20:52.560 doing is doing this inside the
01:20:53.840 validation loop
01:20:55.199 but
01:20:56.239 in the interest of time i'm speeding up
01:20:57.840 a little bit so original aggression will
01:21:00.000 be our selector
01:21:01.679 we wrap around it uh recursive feature
01:21:04.560 elimination we want free
01:21:07.280 in steps of one
01:21:12.000 those are the ones that were selected as
01:21:13.600 the three most promising ones
01:21:19.120 what do we do about this as the next
01:21:21.520 step
01:21:22.880 uh
01:21:25.840 we reformat the data frame the same way
01:21:27.679 as before
01:21:28.880 specify weekly seasonality we ignore
01:21:31.360 again the
01:21:34.080 uh
01:21:35.679 mcmc sampling because we just the only
01:21:37.760 thing we want to do is we want to see if
01:21:40.239 this helps
01:21:41.760 does adding the regressors help with
01:21:43.920 fitting the model
01:21:45.199 how do we do it we
01:21:47.440 add
01:21:48.320 the regressors to the model we should we
01:21:50.639 instantiated the model
01:21:52.480 add them to the data frame and add the
01:21:54.239 regressors boom fit model
01:22:01.600 oh that went reasonably quickly
01:22:04.000 okay
01:22:05.600 uh
01:22:08.400 let's look at the tactic let's look at
01:22:10.719 the performance essentially same thing
01:22:12.639 as before
01:22:31.520 well let's take a question in the
01:22:32.719 meantime exactly what i was going to
01:22:34.800 suggest
01:22:36.400 i see time series as univariate
01:22:38.000 basically yes we have business more
01:22:40.159 features
01:22:41.760 uh
01:22:43.120 in a way
01:22:44.480 in a way if you have variables if you
01:22:47.920 include lagged that's that's the
01:22:50.159 question answer the question asked by
01:22:51.520 moi
01:22:52.560 uh if we have
01:22:54.639 if you use the
01:22:56.639 lagged values of the target as an
01:22:58.800 explanatory variable then yes very much
01:23:01.120 very much you are reducing the time
01:23:02.480 series problem to a regression one yes
01:23:05.440 actually if you look at things like auto
01:23:07.679 regression outer aggressive models which
01:23:09.760 we'll be touching upon next time
01:23:12.000 um
01:23:13.679 that's literally how you fit ar models
01:23:16.880 you you take your target shift it shift
01:23:19.280 it shift it by as many likes as you want
01:23:21.199 use those shifted values as covariates
01:23:23.440 and then you fit a linear regression
01:23:25.280 model so yes that's exactly what you are
01:23:27.600 doing remember the comparison i gave
01:23:29.760 with
01:23:31.679 the wood and the nail
01:23:33.679 we have the problem of linear regression
01:23:35.600 solved and nailed rather well
01:23:38.880 bum-bum reduce it to a known case i mean
01:23:41.520 why bother we have we have we know it's
01:23:43.679 working
01:23:45.040 so yes
01:23:46.239 maybe next time uh
01:23:49.199 if all goes well in two weeks
01:23:52.239 but
01:23:52.960 that's a tentative assessment
01:23:55.840 and you know
01:23:57.679 i mean you want a quick way to have a
01:23:59.520 laughter
01:24:00.639 take your new year's resolution from two
01:24:02.239 years ago right before covet started
01:24:04.560 since we're on the topic of kovit anyway
01:24:06.320 and then let's see
01:24:07.679 but
01:24:08.400 that's what i would hope to be able to
01:24:10.480 achieve
01:24:11.600 uh okay this one converged
01:24:14.639 we produced the
01:24:17.679 performance metrics
01:24:19.440 and now we can compare
01:24:22.159 the performance metrics of this one
01:24:25.120 with and without covariates so this is
01:24:27.199 mean squared error on raw very on the
01:24:29.600 raw model using only the dynamics we can
01:24:31.520 capture in the target
01:24:32.960 and this is what happens when we include
01:24:34.239 covariates here
01:24:36.080 root mean squared error detail
01:24:38.719 pretty much every metric you look at
01:24:40.880 it's improving
01:24:42.320 some more some worse but there is always
01:24:44.400 an improvement
01:24:45.520 uh yes yes i will i want some done
01:24:48.320 coding i will literally
01:24:50.719 run then share and then we'll and while
01:24:53.440 i'm taking questions this will run and
01:24:54.880 can be made public yes to answers and
01:24:57.120 kills
01:24:58.560 uh ah to this comment oh that's even
01:25:00.880 better
01:25:02.080 uh
01:25:03.360 so there's this one super quicky so we
01:25:06.639 can have time for a question or two more
01:25:10.480 neural profit the first thing about
01:25:12.800 neural profit if you're running it in a
01:25:14.480 cargo environment you need to install it
01:25:17.120 because for some inexplicable reason
01:25:19.280 despite its apparently solid popularity
01:25:22.159 it is not included in the cargo image
01:25:25.199 i expect it to change sooner rather than
01:25:28.560 later but so far it hasn't
01:25:32.040 [Music]
01:25:34.080 well that went all right
01:25:35.679 well abhishek you should be happy this
01:25:37.120 is running on python underneath
01:25:40.400 yeah i'm looking forward to learning
01:25:42.080 this
01:25:42.880 never use it
01:25:44.320 i know although personally i think an
01:25:46.719 even cooler thing is
01:25:48.960 what i want to use to explain state
01:25:51.040 space models namely pyro because pyro is
01:25:54.320 built on top of python as well
01:25:56.320 and there it really shines
01:25:59.120 it really really shines
01:26:01.199 uh what do we do from neural prophet
01:26:04.000 important neural prophet
01:26:06.400 it's very very created
01:26:16.880 ah i added the new data my ipad
01:26:21.199 let's redo the
01:26:24.360 minimalistic data frame
01:26:32.639 in all fairness my acquaintance with
01:26:37.120 profit
01:26:38.960 specially experimental
01:26:41.840 so i haven't taken it for that as much
01:26:44.719 of a test drive as i have the proper
01:26:47.520 profit
01:26:49.440 so this is quite literally work this
01:26:51.840 part is quite literally working progress
01:26:56.320 but as a starting point or something
01:26:58.080 that can get you guys you know
01:27:00.880 started this should this should work
01:27:09.360 and it produces some form of estimate
01:27:11.840 during the during the sample period
01:27:15.199 uh
01:27:16.239 well the important bit
01:27:19.199 it has
01:27:20.719 very simple syntax and the same
01:27:22.239 requirements on input
01:27:24.239 as the
01:27:25.679 as the ordinary profit and not so good
01:27:27.920 news
01:27:28.960 most of the
01:27:31.199 most of the
01:27:32.560 uh nice things around profit
01:27:36.320 they don't seem to work here
01:27:38.880 so that's that's a little bit of an
01:27:41.199 issue
01:27:42.400 but you know your mileage might vary the
01:27:44.560 question is how much do you care about
01:27:47.199 actually things being interpretable
01:27:50.560 which i would argue with the ordinary
01:27:52.639 prophet they are more interpretable than
01:27:54.880 they are with uh
01:27:56.960 with the neural one
01:27:58.880 but
01:28:00.800 it's it's it's to a degree to a degree a
01:28:02.960 matter of personal preference
01:28:05.760 and in let's see can i save the version
01:28:09.280 and run this thing
01:28:12.480 ts1 curves let's see what happens
01:28:15.679 uh
01:28:18.400 in terms of content
01:28:20.560 uh in terms of content i guess this is
01:28:22.880 it
01:28:24.080 i'm happy to take a few more questions
01:28:25.760 if people want to stay a little longer
01:28:29.280 sure uh i mean the one one question was
01:28:31.600 about the notebooks which you have
01:28:33.040 already answered yes this is this will
01:28:35.440 be certain
01:28:37.040 i see a question about exponential
01:28:39.920 no not
01:28:41.600 it cannot that's sort of one of the main
01:28:43.679 reasons i combined it in a single one so
01:28:46.320 people sort of get an intuition uh
01:28:48.480 exponential smoothing shows you is based
01:28:50.800 on one season i mean
01:28:54.480 i guess if you are really really and i
01:28:57.440 mean really persistent you can probably
01:29:00.239 derive an extension of exponentials
01:29:02.080 moving
01:29:03.120 to to account for multiple seasonal
01:29:05.199 patterns
01:29:06.320 but at some point it's going to be just
01:29:08.000 so
01:29:08.840 much uh that
01:29:11.280 that will just that will just not be
01:29:13.360 worth the effort
01:29:15.679 some books for recommend for time series
01:29:17.440 using python uh yes the ones in the
01:29:19.760 first notebook
01:29:23.440 we will also share the links later so
01:29:26.639 you can you can those are those are
01:29:28.400 there not sure if they are python
01:29:29.840 specific no no they're not uh actually
01:29:32.639 python books on time series or sorry
01:29:35.199 time series books popping up in python
01:29:38.159 uh this is a thing of the last couple
01:29:40.239 years
01:29:41.440 before that everybody was either writing
01:29:43.840 a purely academic theory
01:29:46.400 or writing in r
01:29:48.159 or writing bless his heart of simeon
01:29:50.960 cokeman and
01:29:54.480 james durbin the awesome time series
01:29:56.480 book
01:29:57.440 in a weird language that they developed
01:30:00.080 sort of scripting thingy called ox
01:30:03.199 it kinda looked like c and matlab had a
01:30:06.000 child and the child came out really ugly
01:30:08.159 that's more or less what the skeptic
01:30:09.360 language looked like so
01:30:11.520 one of one of the best books is yet to
01:30:13.600 come from you
01:30:16.320 we're hoping for that
01:30:17.920 let's let's work in that direction let's
01:30:20.320 work in that direction
01:30:21.840 let's let's take one final question for
01:30:24.000 today and then let's call it a day
01:30:26.400 the metrics we saw rmse mscmap etc if we
01:30:30.239 need to select any one metric which one
01:30:31.840 is the best for times reason why
01:30:34.159 depending on a time series
01:30:36.719 depending on a time series
01:30:38.719 the best
01:30:39.760 or more specifically in the business
01:30:41.600 context
01:30:42.800 rolling back to
01:30:44.639 uh
01:30:48.080 here it is
01:30:49.679 do go away
01:30:52.080 matrix
01:30:54.320 i am really i am really really sorry by
01:30:56.080 the way about the amount of
01:30:58.719 luck logs and output along the way i'm
01:31:00.880 not sure why that's ah
01:31:04.159 let's do it let's do this thing before i
01:31:06.320 forget in the meantime
01:31:08.480 where's the where are the sharing
01:31:10.239 settings
01:31:11.360 here i believe yes
01:31:14.080 public
01:31:16.960 safe changes
01:31:20.320 yes
01:31:22.000 can i park anything in the comments i
01:31:24.239 don't think i can because i'm not logged
01:31:26.000 in
01:31:27.040 huh
01:31:29.120 i i don't think you can but we will
01:31:31.199 share it immediately after
01:31:33.040 well i sent it to your chat so if you
01:31:35.199 can throw it along to comments that
01:31:36.800 should that should work
01:31:41.120 okay oh sure yeah sounds great where
01:31:44.159 were we on the questions sorry what was
01:31:46.159 the one i was answering that specific
01:31:50.159 yes specific metrics
01:31:54.320 uh
01:31:56.159 depends on what you want my philos my
01:31:58.719 personal philosophy is when i when i
01:32:01.360 have an error metric i want the metric
01:32:03.920 to explode if things are really wrong
01:32:06.480 so that's sort of the reason i like mean
01:32:08.080 squared error
01:32:10.320 uh that being said if you want something
01:32:13.840 directly interpretable
01:32:15.679 then the root mean squared error is
01:32:17.520 better because by definition if you look
01:32:19.760 at the formula this is expressed in the
01:32:21.600 same units as the original like mean
01:32:23.600 squared error will give me an error in
01:32:27.040 centigrade square
01:32:28.880 which is from a physical point
01:32:30.480 of view root means squared error it's
01:32:32.880 centigrade it's comparable
01:32:34.800 uh mean absolute error medium absolute
01:32:37.040 error i'm sorry mae
01:32:38.880 that's more average uh
01:32:40.880 across and less less sensitive to
01:32:43.120 outliers so the question is what do you
01:32:44.800 want do you want to be sensitive to
01:32:46.800 attires or do you want to ignore some of
01:32:49.199 them
01:32:50.480 your mileage might vary
01:32:53.199 the percentage error metrics they make
01:32:55.920 sense those are variations around the my
01:32:58.159 map and main
01:32:59.840 voila
01:33:01.840 m
01:33:02.560 how the hell am i supposed to say it and
01:33:04.400 escape
01:33:06.080 the cement this symmetrized version
01:33:09.679 those are variations and on the same
01:33:11.360 theme
01:33:12.159 namely you want to see how badly you are
01:33:15.199 off in percentage terms
01:33:18.000 uh
01:33:18.960 that means well on average i'm fine with
01:33:21.120 being all by ten percent or by five
01:33:23.199 percent or something if that's the
01:33:24.800 question you answer so
01:33:27.360 um proportional error and not absolute
01:33:30.239 values then you go from one for one of
01:33:32.239 the percentage metrics and here the same
01:33:34.400 mean average mean average percentage
01:33:36.400 error versus median average percentage
01:33:38.320 error do you want to be alerted when
01:33:40.960 things are really really wrong in terms
01:33:42.480 of absolute value or do you want it
01:33:45.360 smoothed out
01:33:47.920 the
01:33:48.840 smap
01:33:50.639 that's pretty much a symmetric
01:33:53.199 version for symmetry look up the formula
01:33:56.400 i don't remember it from top of my head
01:33:59.040 of the mape
01:34:00.480 namely accounting for weird behavior
01:34:02.880 around extreme values
01:34:04.800 dart versus prophet oh come on come on
01:34:08.080 that's like restarting the editor wars
01:34:13.280 the exercises from the upcoming book use
01:34:15.440 the data yeah
01:34:17.440 that's to be considered if they allow us
01:34:19.600 to use that
01:34:21.520 uh
01:34:23.600 thank you very much well my pleasure i i
01:34:26.000 hope you guys enjoyed it and we can see
01:34:27.840 each other next time
01:34:29.679 uh abhishek and i will touch base on
01:34:31.440 this one and will of course keep
01:34:32.800 everyone
01:34:33.920 you know up to date
01:34:36.560 if you have time
01:34:38.239 there's space i believe i'm checked the
01:34:40.239 discord server doesn't come with a limit
01:34:42.159 on the number of users or at least
01:34:43.920 limited we are realistically likely to
01:34:45.679 hit right now is it
01:34:47.280 no
01:34:48.320 no no it doesn't so that's that's why we
01:34:51.119 move from slack to discord
01:34:54.000 really slack actually does have a limit
01:34:57.280 it has limit on number of messages oh
01:35:00.239 sorry ah right
01:35:02.000 you can't see the whole messages
01:35:04.080 but but anyways it was it was a very
01:35:06.719 interesting and fun session that's what
01:35:08.719 i expected and
01:35:10.639 thanks a lot conrad i think we are way
01:35:12.960 over time there will still be questions
01:35:15.119 if you have any questions
01:35:16.800 conrad has agreed or let's say i forced
01:35:19.920 him to join this course
01:35:26.480 there is
01:35:28.400 some questions there
01:35:29.840 so if you have some questions you can
01:35:31.360 you can definitely go there and we i i
01:35:33.760 don't see i'm i'm not able to comment on
01:35:36.560 this video right now but i will do that
01:35:38.800 after the video ends and i will add the
01:35:41.280 notebook as a pinned comment and we are
01:35:43.280 also going to share it in the
01:35:45.040 discord channel time series so join the
01:35:47.840 time series channel
01:35:49.440 so
01:35:50.159 i think uh that's all for today and
01:35:53.119 thank you
01:35:53.920 thank you
01:35:56.000 for being such a well engaged audience
01:35:58.719 i wish i managed to
01:36:00.480 switch between this and questions a bit
01:36:02.159 more but alas
01:36:04.400 uh i see a bunch of interesting
01:36:05.840 questions guys if you are in discord do
01:36:08.159 take them up there i'm happy to i'm
01:36:10.639 happy to address them okay
01:36:13.199 that's very kind of you're gonna thank
01:36:14.800 you very much thanks once again and have
01:36:17.520 a
01:36:18.560 nice sunday
01:36:20.080 well have a nice evening or nice day
01:36:22.159 whichever time zone you guys are in for
01:36:23.920 me it's evening
01:36:26.000 okay then see you bye thanks bye
