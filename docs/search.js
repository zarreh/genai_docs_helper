window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "genai_docs_helper", "modulename": "genai_docs_helper", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache", "modulename": "genai_docs_helper.cache", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache", "kind": "class", "doc": "<p>High-performance caching layer with Redis primary and in-memory fallback.</p>\n\n<p>This cache implementation provides transparent failover between Redis and\nin-memory storage, ensuring system reliability even when Redis is unavailable.\nIt includes automatic cleanup, performance monitoring, and configurable TTL.</p>\n\n<p>Features:\n    - Dual-layer caching (Redis + in-memory)\n    - Automatic failover and recovery\n    - TTL-based expiration\n    - Memory usage monitoring and cleanup\n    - Comprehensive error handling\n    - Cache hit/miss statistics</p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache.__init__", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.__init__", "kind": "function", "doc": "<p>Initialize the caching system with Redis and memory backends.</p>\n\n<p>Args:\n    redis_url: Redis connection string (optional)\n    ttl: Time-to-live for cache entries in seconds\n    max_memory_entries: Maximum entries in memory cache before cleanup\n    enable_redis: Whether to attempt Redis connection (default: False for development)</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">redis_url</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">ttl</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3600</span>,</span><span class=\"param\">\t<span class=\"n\">max_memory_entries</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>,</span><span class=\"param\">\t<span class=\"n\">enable_redis</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">False</span></span>)</span>"}, {"fullname": "genai_docs_helper.cache.QueryCache.ttl", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.ttl", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache.max_memory_entries", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.max_memory_entries", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache.memory_cache", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.memory_cache", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Dict[str, Any]]"}, {"fullname": "genai_docs_helper.cache.QueryCache.redis_client", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.redis_client", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache.stats", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.stats", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.cache.QueryCache.get", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.get", "kind": "function", "doc": "<p>Retrieve cached result with fallback from Redis to memory.</p>\n\n<p>Args:\n    question: User's question\n    context: Additional context for cache lookup</p>\n\n<p>Returns:\n    Cached result dictionary or None if not found</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.cache.QueryCache.set", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.set", "kind": "function", "doc": "<p>Store data in cache with Redis primary and memory fallback.</p>\n\n<p>Args:\n    question: User's question (for key generation)\n    context: Additional context\n    data: Data to cache</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">data</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.cache.QueryCache.get_stats", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.get_stats", "kind": "function", "doc": "<p>Get cache performance statistics.</p>\n\n<p>Returns:\n    Dictionary with hit rates, error counts, and other metrics</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Union</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.cache.QueryCache.clear", "modulename": "genai_docs_helper.cache", "qualname": "QueryCache.clear", "kind": "function", "doc": "<p>Clear all cached data from both Redis and memory.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">) -> <span class=\"kc\">None</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains", "modulename": "genai_docs_helper.chains", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.answer_grader", "modulename": "genai_docs_helper.chains.answer_grader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.answer_grader.GradeAnswer", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "GradeAnswer", "kind": "class", "doc": "<p>!!! abstract \"Usage Documentation\"\n    <a href=\"../concepts/models.md\">Models</a></p>\n\n<p>A base class for creating Pydantic models.</p>\n\n<p>Attributes:\n    __class_vars__: The names of the class variables defined on the model.\n    __private_attributes__: Metadata about the private attributes of the model.\n    __signature__: The synthesized <code>__init__</code> [<code>Signature</code>][inspect.Signature] of the model.</p>\n\n<pre><code>__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.\n</code></pre>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.answer_grader.GradeAnswer.binary_score", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "GradeAnswer.binary_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "genai_docs_helper.chains.answer_grader.GradeAnswer.model_config", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "GradeAnswer.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.answer_grader.structured_llm_grader", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "structured_llm_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &quot;Answer addresses the question, &#x27;yes&#x27; or &#x27;no&#x27;&quot;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeAnswer&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.answer_grader.GradeAnswer&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.answer_grader.GradeAnswer&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.answer_grader.system", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&quot;You are a grader assessing whether an answer addresses / resolves a question \\n\\n     Give a binary score &#x27;yes&#x27; or &#x27;no&#x27;. Yes&#x27; means that the answer resolves the question.&quot;"}, {"fullname": "genai_docs_helper.chains.answer_grader.answer_prompt", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "answer_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;generation&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a grader assessing whether an answer addresses / resolves a question \\n\\n     Give a binary score &#x27;yes&#x27; or &#x27;no&#x27;. Yes&#x27; means that the answer resolves the question.&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;generation&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;User question: \\n\\n {question} \\n\\n LLM generation: {generation}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.answer_grader.answer_grader", "modulename": "genai_docs_helper.chains.answer_grader", "qualname": "answer_grader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": langchain_core.runnables.base.RunnableSequence", "default_value": "ChatPromptTemplate(input_variables=[&#x27;generation&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a grader assessing whether an answer addresses / resolves a question \\n\\n     Give a binary score &#x27;yes&#x27; or &#x27;no&#x27;. Yes&#x27; means that the answer resolves the question.&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;generation&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;User question: \\n\\n {question} \\n\\n LLM generation: {generation}&#x27;), additional_kwargs={})])\n| RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &quot;Answer addresses the question, &#x27;yes&#x27; or &#x27;no&#x27;&quot;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeAnswer&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.answer_grader.GradeAnswer&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.answer_grader.GradeAnswer&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.batch_grader", "modulename": "genai_docs_helper.chains.batch_grader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.batch_grader.DocumentRelevance", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "DocumentRelevance", "kind": "class", "doc": "<p>Relevance score for a single document</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.batch_grader.DocumentRelevance.document_index", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "DocumentRelevance.document_index", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "genai_docs_helper.chains.batch_grader.DocumentRelevance.is_relevant", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "DocumentRelevance.is_relevant", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "genai_docs_helper.chains.batch_grader.DocumentRelevance.confidence", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "DocumentRelevance.confidence", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "genai_docs_helper.chains.batch_grader.DocumentRelevance.model_config", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "DocumentRelevance.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.batch_grader.BatchGradeResult", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "BatchGradeResult", "kind": "class", "doc": "<p>Batch grading results for multiple documents</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.batch_grader.BatchGradeResult.scores", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "BatchGradeResult.scores", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[genai_docs_helper.chains.batch_grader.DocumentRelevance]"}, {"fullname": "genai_docs_helper.chains.batch_grader.BatchGradeResult.model_config", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "BatchGradeResult.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.batch_grader.structured_batch_grader", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "structured_batch_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;$defs&#x27;: {&#x27;DocumentRelevance&#x27;: {&#x27;description&#x27;: &#x27;Relevance score for a single document&#x27;, &#x27;properties&#x27;: {&#x27;document_index&#x27;: {&#x27;description&#x27;: &#x27;Index of the document (0-based)&#x27;, &#x27;title&#x27;: &#x27;Document Index&#x27;, &#x27;type&#x27;: &#x27;integer&#x27;}, &#x27;is_relevant&#x27;: {&#x27;description&#x27;: &#x27;Whether the document is relevant to the question&#x27;, &#x27;title&#x27;: &#x27;Is Relevant&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}, &#x27;confidence&#x27;: {&#x27;description&#x27;: &#x27;Confidence score from 0.0 to 1.0&#x27;, &#x27;title&#x27;: &#x27;Confidence&#x27;, &#x27;type&#x27;: &#x27;number&#x27;}}, &#x27;required&#x27;: [&#x27;document_index&#x27;, &#x27;is_relevant&#x27;, &#x27;confidence&#x27;], &#x27;title&#x27;: &#x27;DocumentRelevance&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}}, &#x27;description&#x27;: &#x27;Batch grading results for multiple documents&#x27;, &#x27;properties&#x27;: {&#x27;scores&#x27;: {&#x27;description&#x27;: &#x27;Relevance scores for each document&#x27;, &#x27;items&#x27;: {&#x27;$ref&#x27;: &#x27;#/$defs/DocumentRelevance&#x27;}, &#x27;title&#x27;: &#x27;Scores&#x27;, &#x27;type&#x27;: &#x27;array&#x27;}}, &#x27;required&#x27;: [&#x27;scores&#x27;], &#x27;title&#x27;: &#x27;BatchGradeResult&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.batch_grader.BatchGradeResult&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.batch_grader.BatchGradeResult&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.batch_grader.system", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;You are a grader assessing relevance of multiple documents to a user question.\\nFor each document, determine if it contains information relevant to answering the question.\\n\\nGrade each document as:\\n- is_relevant: true if the document contains relevant information\\n- confidence: your confidence in this assessment (0.0 to 1.0)\\n\\nBe efficient but accurate. Look for keywords, concepts, and semantic relevance.&#x27;"}, {"fullname": "genai_docs_helper.chains.batch_grader.batch_grade_prompt", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "batch_grade_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a grader assessing relevance of multiple documents to a user question.\\nFor each document, determine if it contains information relevant to answering the question.\\n\\nGrade each document as:\\n- is_relevant: true if the document contains relevant information\\n- confidence: your confidence in this assessment (0.0 to 1.0)\\n\\nBe efficient but accurate. Look for keywords, concepts, and semantic relevance.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Question: {question}\\n\\nDocuments to grade:\\n{documents_text}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.batch_grader.format_documents_for_batch_grading", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "format_documents_for_batch_grading", "kind": "function", "doc": "<p>Format documents for batch grading</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains.batch_grader.batch_document_grader", "modulename": "genai_docs_helper.chains.batch_grader", "qualname": "batch_document_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableAssign(mapper={\n  documents_text: RunnableLambda(lambda x: format_documents_for_batch_grading(x[&#x27;documents&#x27;]))\n})\n| ChatPromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a grader assessing relevance of multiple documents to a user question.\\nFor each document, determine if it contains information relevant to answering the question.\\n\\nGrade each document as:\\n- is_relevant: true if the document contains relevant information\\n- confidence: your confidence in this assessment (0.0 to 1.0)\\n\\nBe efficient but accurate. Look for keywords, concepts, and semantic relevance.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Question: {question}\\n\\nDocuments to grade:\\n{documents_text}&#x27;), additional_kwargs={})])\n| RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;$defs&#x27;: {&#x27;DocumentRelevance&#x27;: {&#x27;description&#x27;: &#x27;Relevance score for a single document&#x27;, &#x27;properties&#x27;: {&#x27;document_index&#x27;: {&#x27;description&#x27;: &#x27;Index of the document (0-based)&#x27;, &#x27;title&#x27;: &#x27;Document Index&#x27;, &#x27;type&#x27;: &#x27;integer&#x27;}, &#x27;is_relevant&#x27;: {&#x27;description&#x27;: &#x27;Whether the document is relevant to the question&#x27;, &#x27;title&#x27;: &#x27;Is Relevant&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}, &#x27;confidence&#x27;: {&#x27;description&#x27;: &#x27;Confidence score from 0.0 to 1.0&#x27;, &#x27;title&#x27;: &#x27;Confidence&#x27;, &#x27;type&#x27;: &#x27;number&#x27;}}, &#x27;required&#x27;: [&#x27;document_index&#x27;, &#x27;is_relevant&#x27;, &#x27;confidence&#x27;], &#x27;title&#x27;: &#x27;DocumentRelevance&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}}, &#x27;description&#x27;: &#x27;Batch grading results for multiple documents&#x27;, &#x27;properties&#x27;: {&#x27;scores&#x27;: {&#x27;description&#x27;: &#x27;Relevance scores for each document&#x27;, &#x27;items&#x27;: {&#x27;$ref&#x27;: &#x27;#/$defs/DocumentRelevance&#x27;}, &#x27;title&#x27;: &#x27;Scores&#x27;, &#x27;type&#x27;: &#x27;array&#x27;}}, &#x27;required&#x27;: [&#x27;scores&#x27;], &#x27;title&#x27;: &#x27;BatchGradeResult&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.batch_grader.BatchGradeResult&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.batch_grader.BatchGradeResult&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.confidence_scorer", "modulename": "genai_docs_helper.chains.confidence_scorer", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.ConfidenceScore", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "ConfidenceScore", "kind": "class", "doc": "<p>Confidence score for document relevance.</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.ConfidenceScore.confidence_score", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "ConfidenceScore.confidence_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.ConfidenceScore.reasoning", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "ConfidenceScore.reasoning", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.ConfidenceScore.model_config", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "ConfidenceScore.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.structured_llm_scorer", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "structured_llm_scorer", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Confidence score for document relevance.&#x27;, &#x27;properties&#x27;: {&#x27;confidence_score&#x27;: {&#x27;description&#x27;: &#x27;Confidence score from 0.0 to 1.0 indicating how relevant the document is to the question&#x27;, &#x27;title&#x27;: &#x27;Confidence Score&#x27;, &#x27;type&#x27;: &#x27;number&#x27;}, &#x27;reasoning&#x27;: {&#x27;description&#x27;: &#x27;Brief explanation of the confidence score&#x27;, &#x27;title&#x27;: &#x27;Reasoning&#x27;, &#x27;type&#x27;: &#x27;string&#x27;}}, &#x27;required&#x27;: [&#x27;confidence_score&#x27;, &#x27;reasoning&#x27;], &#x27;title&#x27;: &#x27;ConfidenceScore&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.confidence_scorer.ConfidenceScore&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.confidence_scorer.ConfidenceScore&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.system", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&quot;You are a confidence scorer assessing how well a document answers or relates to a user question.\\nProvide a confidence score from 0.0 to 1.0 where:\\n- 1.0: Document directly and completely answers the question\\n- 0.8-0.9: Document provides most of the answer with good detail\\n- 0.6-0.7: Document provides partial answer or relevant context\\n- 0.4-0.5: Document has some relevance but limited usefulness\\n- 0.2-0.3: Document has minimal relevance\\n- 0.0-0.1: Document is not relevant\\n\\nConsider:\\n- Direct relevance to the question\\n- Completeness of information\\n- Quality and specificity of content\\n- How well it addresses the user&#x27;s intent&quot;"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.confidence_prompt", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "confidence_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a confidence scorer assessing how well a document answers or relates to a user question.\\nProvide a confidence score from 0.0 to 1.0 where:\\n- 1.0: Document directly and completely answers the question\\n- 0.8-0.9: Document provides most of the answer with good detail\\n- 0.6-0.7: Document provides partial answer or relevant context\\n- 0.4-0.5: Document has some relevance but limited usefulness\\n- 0.2-0.3: Document has minimal relevance\\n- 0.0-0.1: Document is not relevant\\n\\nConsider:\\n- Direct relevance to the question\\n- Completeness of information\\n- Quality and specificity of content\\n- How well it addresses the user&#x27;s intent&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Question: {question}\\n\\nDocument: {document}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.confidence_scorer.confidence_scorer", "modulename": "genai_docs_helper.chains.confidence_scorer", "qualname": "confidence_scorer", "kind": "variable", "doc": "<p></p>\n", "annotation": ": langchain_core.runnables.base.RunnableSequence", "default_value": "ChatPromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a confidence scorer assessing how well a document answers or relates to a user question.\\nProvide a confidence score from 0.0 to 1.0 where:\\n- 1.0: Document directly and completely answers the question\\n- 0.8-0.9: Document provides most of the answer with good detail\\n- 0.6-0.7: Document provides partial answer or relevant context\\n- 0.4-0.5: Document has some relevance but limited usefulness\\n- 0.2-0.3: Document has minimal relevance\\n- 0.0-0.1: Document is not relevant\\n\\nConsider:\\n- Direct relevance to the question\\n- Completeness of information\\n- Quality and specificity of content\\n- How well it addresses the user&#x27;s intent&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Question: {question}\\n\\nDocument: {document}&#x27;), additional_kwargs={})])\n| RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Confidence score for document relevance.&#x27;, &#x27;properties&#x27;: {&#x27;confidence_score&#x27;: {&#x27;description&#x27;: &#x27;Confidence score from 0.0 to 1.0 indicating how relevant the document is to the question&#x27;, &#x27;title&#x27;: &#x27;Confidence Score&#x27;, &#x27;type&#x27;: &#x27;number&#x27;}, &#x27;reasoning&#x27;: {&#x27;description&#x27;: &#x27;Brief explanation of the confidence score&#x27;, &#x27;title&#x27;: &#x27;Reasoning&#x27;, &#x27;type&#x27;: &#x27;string&#x27;}}, &#x27;required&#x27;: [&#x27;confidence_score&#x27;, &#x27;reasoning&#x27;], &#x27;title&#x27;: &#x27;ConfidenceScore&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.confidence_scorer.ConfidenceScore&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.confidence_scorer.ConfidenceScore&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.document_reranker", "modulename": "genai_docs_helper.chains.document_reranker", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentScore", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentScore", "kind": "class", "doc": "<p>Score for document relevance</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentScore.document_index", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentScore.document_index", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentScore.relevance_score", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentScore.relevance_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": float"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentScore.reasoning", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentScore.reasoning", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentScore.model_config", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentScore.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentRanking", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentRanking", "kind": "class", "doc": "<p>Ranking of documents by relevance</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentRanking.rankings", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentRanking.rankings", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[genai_docs_helper.chains.document_reranker.DocumentScore]"}, {"fullname": "genai_docs_helper.chains.document_reranker.DocumentRanking.model_config", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "DocumentRanking.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.document_reranker.structured_llm_ranker", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "structured_llm_ranker", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;$defs&#x27;: {&#x27;DocumentScore&#x27;: {&#x27;description&#x27;: &#x27;Score for document relevance&#x27;, &#x27;properties&#x27;: {&#x27;document_index&#x27;: {&#x27;description&#x27;: &#x27;Index of the document (0-based)&#x27;, &#x27;title&#x27;: &#x27;Document Index&#x27;, &#x27;type&#x27;: &#x27;integer&#x27;}, &#x27;relevance_score&#x27;: {&#x27;description&#x27;: &#x27;Relevance score from 0.0 to 1.0&#x27;, &#x27;title&#x27;: &#x27;Relevance Score&#x27;, &#x27;type&#x27;: &#x27;number&#x27;}, &#x27;reasoning&#x27;: {&#x27;description&#x27;: &#x27;Brief explanation of the score&#x27;, &#x27;title&#x27;: &#x27;Reasoning&#x27;, &#x27;type&#x27;: &#x27;string&#x27;}}, &#x27;required&#x27;: [&#x27;document_index&#x27;, &#x27;relevance_score&#x27;, &#x27;reasoning&#x27;], &#x27;title&#x27;: &#x27;DocumentScore&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}}, &#x27;description&#x27;: &#x27;Ranking of documents by relevance&#x27;, &#x27;properties&#x27;: {&#x27;rankings&#x27;: {&#x27;description&#x27;: &#x27;List of document scores, ordered by relevance&#x27;, &#x27;items&#x27;: {&#x27;$ref&#x27;: &#x27;#/$defs/DocumentScore&#x27;}, &#x27;title&#x27;: &#x27;Rankings&#x27;, &#x27;type&#x27;: &#x27;array&#x27;}}, &#x27;required&#x27;: [&#x27;rankings&#x27;], &#x27;title&#x27;: &#x27;DocumentRanking&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.document_reranker.DocumentRanking&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.document_reranker.DocumentRanking&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.document_reranker.system", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;You are an expert document ranker. Given a question and a list of documents, rank them by relevance to the question.\\n\\nFor each document, provide:\\n1. document_index: The index of the document (starting from 0)\\n2. relevance_score: A score from 0.0 to 1.0 (1.0 being most relevant)\\n3. reasoning: Brief explanation of why this score was given\\n\\nRank documents based on:\\n- Direct relevance to the question\\n- Quality and depth of information\\n- Specificity to the topic\\n- Completeness of the answer\\n\\nReturn the rankings ordered from most relevant to least relevant.&#x27;"}, {"fullname": "genai_docs_helper.chains.document_reranker.ranking_prompt", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "ranking_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are an expert document ranker. Given a question and a list of documents, rank them by relevance to the question.\\n\\nFor each document, provide:\\n1. document_index: The index of the document (starting from 0)\\n2. relevance_score: A score from 0.0 to 1.0 (1.0 being most relevant)\\n3. reasoning: Brief explanation of why this score was given\\n\\nRank documents based on:\\n- Direct relevance to the question\\n- Quality and depth of information\\n- Specificity to the topic\\n- Completeness of the answer\\n\\nReturn the rankings ordered from most relevant to least relevant.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;documents_text&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Question: {question}\\n\\nDocuments to rank:\\n{documents_text}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.document_reranker.format_documents_for_ranking", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "format_documents_for_ranking", "kind": "function", "doc": "<p>Format documents for the ranking prompt</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains.document_reranker.rerank_documents", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "rerank_documents", "kind": "function", "doc": "<p>Rerank documents based on relevance to the question</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains.document_reranker.document_reranker", "modulename": "genai_docs_helper.chains.document_reranker", "qualname": "document_reranker", "kind": "function", "doc": "<p>Rerank documents based on relevance to the question</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains.generation", "modulename": "genai_docs_helper.chains.generation", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.generation.prompt", "modulename": "genai_docs_helper.chains.generation", "qualname": "prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;context&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, metadata={&#x27;lc_hub_owner&#x27;: &#x27;rlm&#x27;, &#x27;lc_hub_repo&#x27;: &#x27;rag-prompt&#x27;, &#x27;lc_hub_commit_hash&#x27;: &#x27;50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e&#x27;}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;context&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&quot;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&#x27;t know the answer, just say that you don&#x27;t know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:&quot;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.generation.generation_chain", "modulename": "genai_docs_helper.chains.generation", "qualname": "generation_chain", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;context&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, metadata={&#x27;lc_hub_owner&#x27;: &#x27;rlm&#x27;, &#x27;lc_hub_repo&#x27;: &#x27;rag-prompt&#x27;, &#x27;lc_hub_commit_hash&#x27;: &#x27;50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e&#x27;}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;context&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&quot;You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don&#x27;t know the answer, just say that you don&#x27;t know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:&quot;), additional_kwargs={})])\n| ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;)\n| StrOutputParser()"}, {"fullname": "genai_docs_helper.chains.hallucination_grader", "modulename": "genai_docs_helper.chains.hallucination_grader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.GradeHallucinations", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "GradeHallucinations", "kind": "class", "doc": "<p>Binary score for hallucination present in generation answer.</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.GradeHallucinations.binary_score", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "GradeHallucinations.binary_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": bool"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.GradeHallucinations.model_config", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "GradeHallucinations.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.structured_llm_grader", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "structured_llm_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Binary score for hallucination present in generation answer.&#x27;, &#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &#x27;Answer is grounded in the facts, True or False&#x27;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeHallucinations&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.hallucination_grader.GradeHallucinations&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.hallucination_grader.GradeHallucinations&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.system", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\\n\\nGive a binary score True or False. True means that the answer is grounded in / supported by the set of facts.\\n\\nBe lenient in your assessment:\\n- If the answer correctly summarizes or paraphrases information from the documents, score it as True\\n- If the answer uses slightly different wording but conveys the same meaning, score it as True\\n- If the answer combines information from multiple documents logically, score it as True\\n- Only score as False if the answer contains information that directly contradicts the documents or makes claims not supported by any document\\n\\nConsider semantic meaning, not just exact word matches.&#x27;"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.hallucination_prompt", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "hallucination_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;documents&#x27;, &#x27;generation&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\\n\\nGive a binary score True or False. True means that the answer is grounded in / supported by the set of facts.\\n\\nBe lenient in your assessment:\\n- If the answer correctly summarizes or paraphrases information from the documents, score it as True\\n- If the answer uses slightly different wording but conveys the same meaning, score it as True\\n- If the answer combines information from multiple documents logically, score it as True\\n- Only score as False if the answer contains information that directly contradicts the documents or makes claims not supported by any document\\n\\nConsider semantic meaning, not just exact word matches.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;documents&#x27;, &#x27;generation&#x27;], input_types={}, partial_variables={}, template=&#x27;Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.hallucination_grader.hallucination_grader", "modulename": "genai_docs_helper.chains.hallucination_grader", "qualname": "hallucination_grader", "kind": "variable", "doc": "<p></p>\n", "annotation": ": langchain_core.runnables.base.RunnableSequence", "default_value": "ChatPromptTemplate(input_variables=[&#x27;documents&#x27;, &#x27;generation&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\\n\\nGive a binary score True or False. True means that the answer is grounded in / supported by the set of facts.\\n\\nBe lenient in your assessment:\\n- If the answer correctly summarizes or paraphrases information from the documents, score it as True\\n- If the answer uses slightly different wording but conveys the same meaning, score it as True\\n- If the answer combines information from multiple documents logically, score it as True\\n- Only score as False if the answer contains information that directly contradicts the documents or makes claims not supported by any document\\n\\nConsider semantic meaning, not just exact word matches.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;documents&#x27;, &#x27;generation&#x27;], input_types={}, partial_variables={}, template=&#x27;Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}&#x27;), additional_kwargs={})])\n| RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Binary score for hallucination present in generation answer.&#x27;, &#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &#x27;Answer is grounded in the facts, True or False&#x27;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;boolean&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeHallucinations&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.hallucination_grader.GradeHallucinations&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.hallucination_grader.GradeHallucinations&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.paraphraser", "modulename": "genai_docs_helper.chains.paraphraser", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.paraphraser.system", "modulename": "genai_docs_helper.chains.paraphraser", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;You are a helpful assistant that rephrases user questions to improve clarity and retrieval performance.\\nRephrase the question while preserving its original intent.\\n\\nReturn only the rephrased question. Do not include any explanations, prefixes, or additional commentary.&#x27;"}, {"fullname": "genai_docs_helper.chains.paraphraser.paraphrase_prompt", "modulename": "genai_docs_helper.chains.paraphraser", "qualname": "paraphrase_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a helpful assistant that rephrases user questions to improve clarity and retrieval performance.\\nRephrase the question while preserving its original intent.\\n\\nReturn only the rephrased question. Do not include any explanations, prefixes, or additional commentary.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Original question: {question}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.paraphraser.paraphraser_chain", "modulename": "genai_docs_helper.chains.paraphraser", "qualname": "paraphraser_chain", "kind": "variable", "doc": "<p></p>\n", "annotation": ": langchain_core.runnables.base.RunnableSequence", "default_value": "ChatPromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are a helpful assistant that rephrases user questions to improve clarity and retrieval performance.\\nRephrase the question while preserving its original intent.\\n\\nReturn only the rephrased question. Do not include any explanations, prefixes, or additional commentary.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Original question: {question}&#x27;), additional_kwargs={})])\n| ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;)\n| StrOutputParser()"}, {"fullname": "genai_docs_helper.chains.query_expander", "modulename": "genai_docs_helper.chains.query_expander", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.query_expander.system", "modulename": "genai_docs_helper.chains.query_expander", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;You are an expert at generating multiple variations of a search query to improve retrieval performance.\\nGenerate 3-5 different variations of the given question that would help retrieve relevant documents.\\n\\nGuidelines:\\n1. Maintain the core intent and meaning\\n2. Use different phrasings and terminology\\n3. Include both specific and general variations\\n4. Consider synonyms and related concepts\\n5. Keep variations concise and focused\\n\\nReturn ONLY the variations, one per line, without numbering or additional text.&#x27;"}, {"fullname": "genai_docs_helper.chains.query_expander.query_expansion_prompt", "modulename": "genai_docs_helper.chains.query_expander", "qualname": "query_expansion_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are an expert at generating multiple variations of a search query to improve retrieval performance.\\nGenerate 3-5 different variations of the given question that would help retrieve relevant documents.\\n\\nGuidelines:\\n1. Maintain the core intent and meaning\\n2. Use different phrasings and terminology\\n3. Include both specific and general variations\\n4. Consider synonyms and related concepts\\n5. Keep variations concise and focused\\n\\nReturn ONLY the variations, one per line, without numbering or additional text.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Original question: {question}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.query_expander.parse_query_variations", "modulename": "genai_docs_helper.chains.query_expander", "qualname": "parse_query_variations", "kind": "function", "doc": "<p>Parse the response into a list of query variations</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">response</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.chains.query_expander.query_expander_chain", "modulename": "genai_docs_helper.chains.query_expander", "qualname": "query_expander_chain", "kind": "variable", "doc": "<p></p>\n", "annotation": ": langchain_core.runnables.base.RunnableSequence", "default_value": "ChatPromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&#x27;You are an expert at generating multiple variations of a search query to improve retrieval performance.\\nGenerate 3-5 different variations of the given question that would help retrieve relevant documents.\\n\\nGuidelines:\\n1. Maintain the core intent and meaning\\n2. Use different phrasings and terminology\\n3. Include both specific and general variations\\n4. Consider synonyms and related concepts\\n5. Keep variations concise and focused\\n\\nReturn ONLY the variations, one per line, without numbering or additional text.&#x27;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Original question: {question}&#x27;), additional_kwargs={})])\n| ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;)\n| StrOutputParser()\n| RunnableLambda(parse_query_variations)"}, {"fullname": "genai_docs_helper.chains.retrieval_grader", "modulename": "genai_docs_helper.chains.retrieval_grader", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.GradeDocuments", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "GradeDocuments", "kind": "class", "doc": "<p>Binary score for relevance check on retrieved documents.</p>\n", "bases": "pydantic.main.BaseModel"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.GradeDocuments.binary_score", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "GradeDocuments.binary_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.GradeDocuments.model_config", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "GradeDocuments.model_config", "kind": "variable", "doc": "<p>Configuration for the model, should be a dictionary conforming to [<code>ConfigDict</code>][pydantic.config.ConfigDict].</p>\n", "annotation": ": ClassVar[pydantic.config.ConfigDict]", "default_value": "{}"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.structured_llm_grader", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "structured_llm_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Binary score for relevance check on retrieved documents.&#x27;, &#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &quot;Documents are relevant to the question, &#x27;yes&#x27; or &#x27;no&#x27;&quot;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;string&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeDocuments&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.retrieval_grader.GradeDocuments&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.retrieval_grader.GradeDocuments&#x27;&gt;)"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.system", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "system", "kind": "variable", "doc": "<p></p>\n", "default_value": "&quot;You are a grader assessing relevance of a retrieved document to a user question. \\n\\n    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\\n    Give a binary score &#x27;yes&#x27; or &#x27;no&#x27; score to indicate whether the document is relevant to the question.&quot;"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.grade_prompt", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "grade_prompt", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a grader assessing relevance of a retrieved document to a user question. \\n\\n    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\\n    Give a binary score &#x27;yes&#x27; or &#x27;no&#x27; score to indicate whether the document is relevant to the question.&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Retrieved document: \\n\\n {document} \\n\\n User question: {question}&#x27;), additional_kwargs={})])"}, {"fullname": "genai_docs_helper.chains.retrieval_grader.retrieval_grader", "modulename": "genai_docs_helper.chains.retrieval_grader", "qualname": "retrieval_grader", "kind": "variable", "doc": "<p></p>\n", "default_value": "ChatPromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=&quot;You are a grader assessing relevance of a retrieved document to a user question. \\n\\n    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\\n    Give a binary score &#x27;yes&#x27; or &#x27;no&#x27; score to indicate whether the document is relevant to the question.&quot;), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[&#x27;document&#x27;, &#x27;question&#x27;], input_types={}, partial_variables={}, template=&#x27;Retrieved document: \\n\\n {document} \\n\\n User question: {question}&#x27;), additional_kwargs={})])\n| RunnableBinding(bound=ChatOllama(model=&#x27;llama3.2&#x27;, num_ctx=2048, num_gpu=1, num_thread=8, temperature=0.1, base_url=&#x27;http://localhost:11434&#x27;), kwargs={&#x27;format&#x27;: {&#x27;description&#x27;: &#x27;Binary score for relevance check on retrieved documents.&#x27;, &#x27;properties&#x27;: {&#x27;binary_score&#x27;: {&#x27;description&#x27;: &quot;Documents are relevant to the question, &#x27;yes&#x27; or &#x27;no&#x27;&quot;, &#x27;title&#x27;: &#x27;Binary Score&#x27;, &#x27;type&#x27;: &#x27;string&#x27;}}, &#x27;required&#x27;: [&#x27;binary_score&#x27;], &#x27;title&#x27;: &#x27;GradeDocuments&#x27;, &#x27;type&#x27;: &#x27;object&#x27;}, &#x27;ls_structured_output_format&#x27;: {&#x27;kwargs&#x27;: {&#x27;method&#x27;: &#x27;json_schema&#x27;}, &#x27;schema&#x27;: &lt;class &#x27;genai_docs_helper.chains.retrieval_grader.GradeDocuments&#x27;&gt;}}, config={}, config_factories=[])\n| PydanticOutputParser(pydantic_object=&lt;class &#x27;genai_docs_helper.chains.retrieval_grader.GradeDocuments&#x27;&gt;)"}, {"fullname": "genai_docs_helper.config", "modulename": "genai_docs_helper.config", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.config.LLM_TYPE", "modulename": "genai_docs_helper.config", "qualname": "LLM_TYPE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;ollama&#x27;"}, {"fullname": "genai_docs_helper.config.EMBEDDING_TYPE", "modulename": "genai_docs_helper.config", "qualname": "EMBEDDING_TYPE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;ollama&#x27;"}, {"fullname": "genai_docs_helper.config.ORIGINAL_DOCS_PATH", "modulename": "genai_docs_helper.config", "qualname": "ORIGINAL_DOCS_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;./data/warehouse_docs/&#x27;"}, {"fullname": "genai_docs_helper.config.VECTOR_STORE_PATH", "modulename": "genai_docs_helper.config", "qualname": "VECTOR_STORE_PATH", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;./data/chroma_db_warehouse&#x27;"}, {"fullname": "genai_docs_helper.config.ENABLE_CACHE", "modulename": "genai_docs_helper.config", "qualname": "ENABLE_CACHE", "kind": "variable", "doc": "<p></p>\n", "default_value": "True"}, {"fullname": "genai_docs_helper.config.ENABLE_REDIS", "modulename": "genai_docs_helper.config", "qualname": "ENABLE_REDIS", "kind": "variable", "doc": "<p></p>\n", "default_value": "False"}, {"fullname": "genai_docs_helper.config.REDIS_URL", "modulename": "genai_docs_helper.config", "qualname": "REDIS_URL", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;redis://localhost:6379&#x27;"}, {"fullname": "genai_docs_helper.config.MAX_WORKERS", "modulename": "genai_docs_helper.config", "qualname": "MAX_WORKERS", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "genai_docs_helper.config.BATCH_SIZE", "modulename": "genai_docs_helper.config", "qualname": "BATCH_SIZE", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "genai_docs_helper.config.EARLY_STOPPING_THRESHOLD", "modulename": "genai_docs_helper.config", "qualname": "EARLY_STOPPING_THRESHOLD", "kind": "variable", "doc": "<p></p>\n", "default_value": "5"}, {"fullname": "genai_docs_helper.config.CONFIDENCE_THRESHOLD", "modulename": "genai_docs_helper.config", "qualname": "CONFIDENCE_THRESHOLD", "kind": "variable", "doc": "<p></p>\n", "default_value": "0.7"}, {"fullname": "genai_docs_helper.config.RETRY_COUNT", "modulename": "genai_docs_helper.config", "qualname": "RETRY_COUNT", "kind": "variable", "doc": "<p></p>\n", "default_value": "3"}, {"fullname": "genai_docs_helper.config.EMBEDDING", "modulename": "genai_docs_helper.config", "qualname": "EMBEDDING", "kind": "variable", "doc": "<p></p>\n", "default_value": "OllamaEmbeddings(model=&#x27;llama3.2&#x27;, validate_model_on_init=False, base_url=&#x27;http://localhost:11434&#x27;, client_kwargs={}, async_client_kwargs={}, sync_client_kwargs={}, mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, keep_alive=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None)"}, {"fullname": "genai_docs_helper.consts", "modulename": "genai_docs_helper.consts", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.consts.RETRIEVE", "modulename": "genai_docs_helper.consts", "qualname": "RETRIEVE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;retrieve&#x27;"}, {"fullname": "genai_docs_helper.consts.GRADE_DOCUMENTS", "modulename": "genai_docs_helper.consts", "qualname": "GRADE_DOCUMENTS", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;grade_documents&#x27;"}, {"fullname": "genai_docs_helper.consts.GENERATE", "modulename": "genai_docs_helper.consts", "qualname": "GENERATE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;generate&#x27;"}, {"fullname": "genai_docs_helper.consts.PARAPHRASE", "modulename": "genai_docs_helper.consts", "qualname": "PARAPHRASE", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;paraphrase&#x27;"}, {"fullname": "genai_docs_helper.consts.END", "modulename": "genai_docs_helper.consts", "qualname": "END", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;end&#x27;"}, {"fullname": "genai_docs_helper.consts.RESTART", "modulename": "genai_docs_helper.consts", "qualname": "RESTART", "kind": "variable", "doc": "<p></p>\n", "default_value": "&#x27;retrieve_with_reset&#x27;"}, {"fullname": "genai_docs_helper.graph", "modulename": "genai_docs_helper.graph", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.graph.logger", "modulename": "genai_docs_helper.graph", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.graph (INFO)&gt;"}, {"fullname": "genai_docs_helper.graph.decide_to_generate", "modulename": "genai_docs_helper.graph", "qualname": "decide_to_generate", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.graph.grade_generation_grounded_in_documents_and_question", "modulename": "genai_docs_helper.graph", "qualname": "grade_generation_grounded_in_documents_and_question", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.graph.retrieve_with_reset", "modulename": "genai_docs_helper.graph", "qualname": "retrieve_with_reset", "kind": "function", "doc": "<p>Reset retry count when starting a new retrieval</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.graph.workflow", "modulename": "genai_docs_helper.graph", "qualname": "workflow", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;langgraph.graph.state.StateGraph object&gt;"}, {"fullname": "genai_docs_helper.graph.graph", "modulename": "genai_docs_helper.graph", "qualname": "graph", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;langgraph.graph.state.CompiledStateGraph object&gt;"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.logger", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.loader_embed_to_vectorstore (INFO)&gt;"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.load_markdown_files", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "load_markdown_files", "kind": "function", "doc": "<p>Load markdown files from directory</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">directory</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./data/docs/&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.load_jupyter_notebooks", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "load_jupyter_notebooks", "kind": "function", "doc": "<p>Load jupyter notebooks from directory</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">directory</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./data/demand_forecast_notebooks/&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.process_documents", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "process_documents", "kind": "function", "doc": "<p>Split documents into chunks</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span>, </span><span class=\"param\"><span class=\"n\">chunk_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2000</span>, </span><span class=\"param\"><span class=\"n\">chunk_overlap</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">20</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.create_vector_store", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "create_vector_store", "kind": "function", "doc": "<p>Create and persist vector store</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span>, </span><span class=\"param\"><span class=\"n\">persist_directory</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./data/chroma_db&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.extract_relevant_context", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "extract_relevant_context", "kind": "function", "doc": "<p>Extract and format relevant context from a document.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">doc</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.loader_embed_to_vectorstore.format_citation", "modulename": "genai_docs_helper.loader_embed_to_vectorstore", "qualname": "format_citation", "kind": "function", "doc": "<p>Format source information into a citation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">source_info</span><span class=\"p\">:</span> <span class=\"n\">Dict</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.monitoring", "modulename": "genai_docs_helper.monitoring", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor", "kind": "class", "doc": "<p>Monitor and log performance metrics</p>\n"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.__init__", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">log_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./logs/performance&#39;</span></span>)</span>"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.log_dir", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.log_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.metrics", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.metrics", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, Any]"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.start_time", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.start_time", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[float]"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.start_request", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.start_request", "kind": "function", "doc": "<p>Start timing a request</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">request_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.log_stage", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.log_stage", "kind": "function", "doc": "<p>Log a stage completion</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">request_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">stage</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">duration</span><span class=\"p\">:</span> <span class=\"nb\">float</span>,</span><span class=\"param\">\t<span class=\"n\">metadata</span><span class=\"p\">:</span> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.end_request", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.end_request", "kind": "function", "doc": "<p>End timing and save metrics</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">request_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.monitoring.PerformanceMonitor.get_summary", "modulename": "genai_docs_helper.monitoring", "qualname": "PerformanceMonitor.get_summary", "kind": "function", "doc": "<p>Get performance summary</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">request_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes", "modulename": "genai_docs_helper.nodes", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.nodes.generate", "modulename": "genai_docs_helper.nodes", "qualname": "generate", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.grade_documents", "modulename": "genai_docs_helper.nodes", "qualname": "grade_documents", "kind": "function", "doc": "<p>Grade documents for relevance with intelligent batch processing and early stopping.</p>\n\n<p>This function serves as the main entry point for document relevance assessment,\nimplementing several optimizations for production use:</p>\n\n<ul>\n<li>Batch processing to reduce LLM API calls</li>\n<li>Early stopping when sufficient high-quality documents are found</li>\n<li>Comprehensive error handling and fallback mechanisms</li>\n<li>Quality assessment and confidence scoring</li>\n<li>Performance monitoring and metrics collection</li>\n</ul>\n\n<p>Args:\n    state: Current graph state containing documents and question</p>\n\n<p>Returns:\n    Updated state with filtered documents and grading metadata</p>\n\n<p>Performance Notes:\n    - Typical processing time: 1-3 seconds for 20 documents\n    - Early stopping can reduce processing time by 40-60%\n    - Batch processing reduces API costs by ~80%</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve", "modulename": "genai_docs_helper.nodes", "qualname": "retrieve", "kind": "function", "doc": "<p>Main retrieval function with intelligent path selection and caching.</p>\n\n<p>This function serves as the primary entry point for document retrieval,\nimplementing a two-tier strategy: fast path for quick responses and\ncomprehensive path for complex queries requiring better coverage.</p>\n\n<p>Features:\n    - Intelligent caching with Redis fallback to memory\n    - Adaptive retrieval strategy based on document availability\n    - Comprehensive error handling with graceful fallbacks\n    - Performance monitoring and metrics collection\n    - Deduplication and semantic reranking</p>\n\n<p>Args:\n    state: Current graph state containing question and context</p>\n\n<p>Returns:\n    Updated state dict with retrieved documents and metadata</p>\n\n<p>Raises:\n    Exception: Only for critical failures; most errors are handled gracefully</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.paraphrase", "modulename": "genai_docs_helper.nodes", "qualname": "paraphrase", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.generate", "modulename": "genai_docs_helper.nodes.generate", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.nodes.generate.logger", "modulename": "genai_docs_helper.nodes.generate", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.nodes.generate (INFO)&gt;"}, {"fullname": "genai_docs_helper.nodes.generate.cache", "modulename": "genai_docs_helper.nodes.generate", "qualname": "cache", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;genai_docs_helper.cache.query_cache.QueryCache object&gt;"}, {"fullname": "genai_docs_helper.nodes.generate.generate", "modulename": "genai_docs_helper.nodes.generate", "qualname": "generate", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.grade_documents", "modulename": "genai_docs_helper.nodes.grade_documents", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.nodes.grade_documents.logger", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.nodes.grade_documents (INFO)&gt;"}, {"fullname": "genai_docs_helper.nodes.grade_documents.GRADING_CONFIG", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "GRADING_CONFIG", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;batch_size&#x27;: 5, &#x27;min_relevant_docs&#x27;: 5, &#x27;confidence_threshold&#x27;: 0.7, &#x27;max_grading_errors_ratio&#x27;: 0.3, &#x27;early_stopping_enabled&#x27;: True}"}, {"fullname": "genai_docs_helper.nodes.grade_documents.grade_document_batch", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "grade_document_batch", "kind": "function", "doc": "<p>Grade multiple documents in batches for improved efficiency and throughput.</p>\n\n<p>This function processes documents in groups to reduce the number of LLM calls\nwhile maintaining accuracy. It includes robust fallback mechanisms for when\nbatch processing fails.</p>\n\n<p>Args:\n    documents: List of documents to grade for relevance\n    question: User's question for relevance assessment\n    batch_size: Number of documents to process simultaneously</p>\n\n<p>Returns:\n    List of tuples containing (document, is_relevant, confidence_score)</p>\n\n<p>Performance:\n    - Batch processing reduces LLM calls by ~5x\n    - Fallback ensures 100% document coverage even with partial failures\n    - Typical processing time: 2-5 seconds for 25 documents</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">batch_size</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.grade_documents.assess_grading_quality", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "assess_grading_quality", "kind": "function", "doc": "<p>Analyze the quality and distribution of grading results.</p>\n\n<p>Args:\n    graded_results: List of (document, is_relevant, confidence) tuples</p>\n\n<p>Returns:\n    Quality metrics including confidence distribution and relevance rates</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">graded_results</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Tuple</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">,</span> <span class=\"nb\">bool</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">]]</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.grade_documents.should_stop_early", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "should_stop_early", "kind": "function", "doc": "<p>Determine if early stopping criteria are met for efficiency.</p>\n\n<p>Early stopping helps reduce processing time when we have sufficient\nhigh-quality documents for answer generation.</p>\n\n<p>Args:\n    relevant_docs: Currently identified relevant documents\n    confidence_scores: Confidence scores for relevant documents</p>\n\n<p>Returns:\n    True if early stopping criteria are satisfied</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">relevant_docs</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Any</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">confidence_scores</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">bool</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.grade_documents.grade_documents", "modulename": "genai_docs_helper.nodes.grade_documents", "qualname": "grade_documents", "kind": "function", "doc": "<p>Grade documents for relevance with intelligent batch processing and early stopping.</p>\n\n<p>This function serves as the main entry point for document relevance assessment,\nimplementing several optimizations for production use:</p>\n\n<ul>\n<li>Batch processing to reduce LLM API calls</li>\n<li>Early stopping when sufficient high-quality documents are found</li>\n<li>Comprehensive error handling and fallback mechanisms</li>\n<li>Quality assessment and confidence scoring</li>\n<li>Performance monitoring and metrics collection</li>\n</ul>\n\n<p>Args:\n    state: Current graph state containing documents and question</p>\n\n<p>Returns:\n    Updated state with filtered documents and grading metadata</p>\n\n<p>Performance Notes:\n    - Typical processing time: 1-3 seconds for 20 documents\n    - Early stopping can reduce processing time by 40-60%\n    - Batch processing reduces API costs by ~80%</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.paraphrase", "modulename": "genai_docs_helper.nodes.paraphrase", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.nodes.paraphrase.logger", "modulename": "genai_docs_helper.nodes.paraphrase", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.nodes.paraphrase (INFO)&gt;"}, {"fullname": "genai_docs_helper.nodes.paraphrase.paraphrase", "modulename": "genai_docs_helper.nodes.paraphrase", "qualname": "paraphrase", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">typing</span><span class=\"o\">.</span><span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve", "modulename": "genai_docs_helper.nodes.retrieve", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.nodes.retrieve.logger", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "logger", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;Logger genai_docs_helper.genai_docs_helper.nodes.retrieve (INFO)&gt;"}, {"fullname": "genai_docs_helper.nodes.retrieve.cache", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "cache", "kind": "variable", "doc": "<p></p>\n", "default_value": "&lt;genai_docs_helper.cache.query_cache.QueryCache object&gt;"}, {"fullname": "genai_docs_helper.nodes.retrieve.RETRIEVAL_CONFIGS", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "RETRIEVAL_CONFIGS", "kind": "variable", "doc": "<p></p>\n", "default_value": "{&#x27;fast&#x27;: {&#x27;k&#x27;: 20}, &#x27;standard&#x27;: {&#x27;k&#x27;: 50}, &#x27;comprehensive&#x27;: {&#x27;k&#x27;: 100}}"}, {"fullname": "genai_docs_helper.nodes.retrieve.fast_retriever", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "fast_retriever", "kind": "variable", "doc": "<p></p>\n", "default_value": "VectorStoreRetriever(tags=[&#x27;Chroma&#x27;, &#x27;OllamaEmbeddings&#x27;], vectorstore=&lt;langchain_chroma.vectorstores.Chroma object&gt;, search_kwargs={&#x27;k&#x27;: 20})"}, {"fullname": "genai_docs_helper.nodes.retrieve.standard_retriever", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "standard_retriever", "kind": "variable", "doc": "<p></p>\n", "default_value": "VectorStoreRetriever(tags=[&#x27;Chroma&#x27;, &#x27;OllamaEmbeddings&#x27;], vectorstore=&lt;langchain_chroma.vectorstores.Chroma object&gt;, search_kwargs={&#x27;k&#x27;: 50})"}, {"fullname": "genai_docs_helper.nodes.retrieve.comprehensive_retriever", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "comprehensive_retriever", "kind": "variable", "doc": "<p></p>\n", "default_value": "VectorStoreRetriever(tags=[&#x27;Chroma&#x27;, &#x27;OllamaEmbeddings&#x27;], vectorstore=&lt;langchain_chroma.vectorstores.Chroma object&gt;, search_kwargs={&#x27;k&#x27;: 100})"}, {"fullname": "genai_docs_helper.nodes.retrieve.compute_semantic_similarity", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "compute_semantic_similarity", "kind": "function", "doc": "<p>Compute cosine similarity between query and document embeddings.</p>\n\n<p>Args:\n    query_embedding: Normalized embedding vector for the query\n    doc_embedding: Normalized embedding vector for the document</p>\n\n<p>Returns:\n    Similarity score between -1 and 1 (higher is more similar)</p>\n\n<p>Note:\n    Uses numpy's dot product which is optimized for vector operations.\n    Assumes embeddings are already normalized by the embedding model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">query_embedding</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span>, </span><span class=\"param\"><span class=\"n\">doc_embedding</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">float</span><span class=\"p\">]</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">float</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.fast_semantic_rerank", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "fast_semantic_rerank", "kind": "function", "doc": "<p>Efficiently rerank documents using embedding-based semantic similarity.</p>\n\n<p>This function provides a fast alternative to LLM-based reranking by using\npre-computed embeddings to score document relevance. It's particularly useful\nfor real-time applications where speed is critical.</p>\n\n<p>Args:\n    question: The user's query for relevance scoring\n    documents: List of retrieved documents to rerank\n    top_k: Maximum number of documents to return</p>\n\n<p>Returns:\n    Documents sorted by semantic relevance (most relevant first)</p>\n\n<p>Performance:\n    - Typical execution time: 100-500ms for 50 documents\n    - Memory usage: ~10MB for embeddings computation\n    - Accuracy: ~85-90% agreement with LLM reranking</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>,</span><span class=\"param\">\t<span class=\"n\">documents</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">langchain_core</span><span class=\"o\">.</span><span class=\"n\">documents</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Document</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">top_k</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">20</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">langchain_core</span><span class=\"o\">.</span><span class=\"n\">documents</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Document</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.parallel_retrieve", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "parallel_retrieve", "kind": "function", "doc": "<p>Retrieve documents in parallel across multiple query variations.</p>\n\n<p>This function executes multiple retrieval operations concurrently to improve\ncoverage while maintaining reasonable response times. It includes deduplication\nto avoid returning identical content multiple times.</p>\n\n<p>Args:\n    queries: List of query strings to search for\n    timeout_per_query: Maximum seconds to wait for each query</p>\n\n<p>Returns:\n    Deduplicated list of retrieved documents from all queries</p>\n\n<p>Performance:\n    - Parallel execution reduces total time by ~3-4x\n    - Automatic timeout prevents hanging on slow queries\n    - Memory-efficient deduplication using content hashing</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">queries</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>,</span><span class=\"param\">\t<span class=\"n\">timeout_per_query</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span></span><span class=\"return-annotation\">) -> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">langchain_core</span><span class=\"o\">.</span><span class=\"n\">documents</span><span class=\"o\">.</span><span class=\"n\">base</span><span class=\"o\">.</span><span class=\"n\">Document</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.generate_cache_key", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "generate_cache_key", "kind": "function", "doc": "<p>Generate a deterministic cache key for query results.</p>\n\n<p>Args:\n    question: The user's question\n    strategy: Retrieval strategy used (\"fast\", \"standard\", \"comprehensive\")</p>\n\n<p>Returns:\n    MD5 hash suitable for cache lookup</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">strategy</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;standard&#39;</span></span><span class=\"return-annotation\">) -> <span class=\"nb\">str</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.execute_fast_retrieval_path", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "execute_fast_retrieval_path", "kind": "function", "doc": "<p>Execute the optimized fast retrieval path for time-sensitive queries.</p>\n\n<p>This path prioritizes speed over comprehensiveness, using a smaller document\nset and embedding-based reranking to provide quick responses.</p>\n\n<p>Args:\n    question: Current processed question\n    original_question: User's original question for relevance scoring</p>\n\n<p>Returns:\n    Complete retrieval result dict or None if insufficient documents found</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">original_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.execute_comprehensive_retrieval_path", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "execute_comprehensive_retrieval_path", "kind": "function", "doc": "<p>Execute comprehensive retrieval with query expansion and parallel processing.</p>\n\n<p>This path prioritizes recall and answer quality over speed, using multiple\nquery variations and more sophisticated reranking.</p>\n\n<p>Args:\n    question: Current processed question\n    original_question: User's original question</p>\n\n<p>Returns:\n    Complete retrieval result with enhanced document coverage</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">question</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"n\">original_question</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.nodes.retrieve.retrieve", "modulename": "genai_docs_helper.nodes.retrieve", "qualname": "retrieve", "kind": "function", "doc": "<p>Main retrieval function with intelligent path selection and caching.</p>\n\n<p>This function serves as the primary entry point for document retrieval,\nimplementing a two-tier strategy: fast path for quick responses and\ncomprehensive path for complex queries requiring better coverage.</p>\n\n<p>Features:\n    - Intelligent caching with Redis fallback to memory\n    - Adaptive retrieval strategy based on document availability\n    - Comprehensive error handling with graceful fallbacks\n    - Performance monitoring and metrics collection\n    - Deduplication and semantic reranking</p>\n\n<p>Args:\n    state: Current graph state containing question and context</p>\n\n<p>Returns:\n    Updated state dict with retrieved documents and metadata</p>\n\n<p>Raises:\n    Exception: Only for critical failures; most errors are handled gracefully</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"p\">:</span> <span class=\"n\">genai_docs_helper</span><span class=\"o\">.</span><span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">GraphState</span></span><span class=\"return-annotation\">) -> <span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">Any</span><span class=\"p\">]</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.state", "modulename": "genai_docs_helper.state", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.state.GraphState", "modulename": "genai_docs_helper.state", "qualname": "GraphState", "kind": "class", "doc": "<p>Represents the complete state of the document retrieval and generation graph.</p>\n\n<p>This state object tracks the entire lifecycle of a query through the system,\nfrom initial question processing through document retrieval, grading, and\nfinal answer generation. It includes performance metrics and error tracking\nfor production monitoring.</p>\n\n<p>Attributes:\n    question (str): Current processed question (may be paraphrased)\n    original_question (str): The user's original unmodified question\n    generation (str): Final generated answer from the LLM\n    documents (List[str]): Current filtered and relevant documents\n    history (List[str]): Historical record of all generated responses\n    retry_count (int): Number of retry attempts for the current question</p>\n\n<pre><code># Enhanced retrieval and processing fields\nquery_variations (Optional[List[str]]): Alternative phrasings for better retrieval\nretrieved_documents_raw (Optional[List[Any]]): All documents before filtering\nreranked_documents (Optional[List[Any]]): Documents after semantic reranking\n\n# Performance and monitoring fields\ncache_key (Optional[str]): Unique identifier for caching results\nperformance_metrics (Optional[Dict[str, Any]]): Timing and efficiency data\nerror_log (Optional[List[str]]): Accumulated error messages for debugging\nconfidence_score (Optional[float]): System confidence in the answer (0.0-1.0)\ntimestamp (Optional[datetime]): Processing timestamp for audit trails\n</code></pre>\n", "bases": "typing.TypedDict"}, {"fullname": "genai_docs_helper.state.GraphState.question", "modulename": "genai_docs_helper.state", "qualname": "GraphState.question", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.state.GraphState.original_question", "modulename": "genai_docs_helper.state", "qualname": "GraphState.original_question", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.state.GraphState.generation", "modulename": "genai_docs_helper.state", "qualname": "GraphState.generation", "kind": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "genai_docs_helper.state.GraphState.documents", "modulename": "genai_docs_helper.state", "qualname": "GraphState.documents", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[str]"}, {"fullname": "genai_docs_helper.state.GraphState.history", "modulename": "genai_docs_helper.state", "qualname": "GraphState.history", "kind": "variable", "doc": "<p></p>\n", "annotation": ": List[str]"}, {"fullname": "genai_docs_helper.state.GraphState.retry_count", "modulename": "genai_docs_helper.state", "qualname": "GraphState.retry_count", "kind": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "genai_docs_helper.state.GraphState.query_variations", "modulename": "genai_docs_helper.state", "qualname": "GraphState.query_variations", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[str]]"}, {"fullname": "genai_docs_helper.state.GraphState.retrieved_documents_raw", "modulename": "genai_docs_helper.state", "qualname": "GraphState.retrieved_documents_raw", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[Any]]"}, {"fullname": "genai_docs_helper.state.GraphState.reranked_documents", "modulename": "genai_docs_helper.state", "qualname": "GraphState.reranked_documents", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[Any]]"}, {"fullname": "genai_docs_helper.state.GraphState.cache_key", "modulename": "genai_docs_helper.state", "qualname": "GraphState.cache_key", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[str]"}, {"fullname": "genai_docs_helper.state.GraphState.performance_metrics", "modulename": "genai_docs_helper.state", "qualname": "GraphState.performance_metrics", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[Dict[str, Any]]"}, {"fullname": "genai_docs_helper.state.GraphState.error_log", "modulename": "genai_docs_helper.state", "qualname": "GraphState.error_log", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[List[str]]"}, {"fullname": "genai_docs_helper.state.GraphState.confidence_score", "modulename": "genai_docs_helper.state", "qualname": "GraphState.confidence_score", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[float]"}, {"fullname": "genai_docs_helper.state.GraphState.timestamp", "modulename": "genai_docs_helper.state", "qualname": "GraphState.timestamp", "kind": "variable", "doc": "<p></p>\n", "annotation": ": Optional[datetime.datetime]"}, {"fullname": "genai_docs_helper.utils", "modulename": "genai_docs_helper.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "genai_docs_helper.utils.get_logger", "modulename": "genai_docs_helper.utils", "qualname": "get_logger", "kind": "function", "doc": "<p>Get a logger instance with consistent configuration.</p>\n\n<p>Args:\n    name: Logger name (typically __name__)</p>\n\n<p>Returns:\n    Configured logger instance</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">) -> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.utils.setup_logging", "modulename": "genai_docs_helper.utils", "qualname": "setup_logging", "kind": "function", "doc": "<p>Set up consistent logging configuration for the application.</p>\n\n<p>Args:\n    name: Logger name\n    log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n    log_dir: Directory to store log files\n    enable_file_logging: Whether to log to file\n    enable_console_logging: Whether to log to console</p>\n\n<p>Returns:\n    Configured logger instance</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">name</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;genai_docs_helper&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">log_level</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;INFO&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">log_dir</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;./logs&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">enable_file_logging</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">enable_console_logging</span><span class=\"p\">:</span> <span class=\"nb\">bool</span> <span class=\"o\">=</span> <span class=\"kc\">True</span></span><span class=\"return-annotation\">) -> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span>:</span></span>", "funcdef": "def"}, {"fullname": "genai_docs_helper.utils.log_performance_metrics", "modulename": "genai_docs_helper.utils", "qualname": "log_performance_metrics", "kind": "function", "doc": "<p>Log performance metrics in a consistent format.</p>\n\n<p>Args:\n    logger: Logger instance\n    metrics: Dictionary of performance metrics\n    request_id: Optional request identifier</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">logger</span><span class=\"p\">:</span> <span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">Logger</span>,</span><span class=\"param\">\t<span class=\"n\">metrics</span><span class=\"p\">:</span> <span class=\"nb\">dict</span>,</span><span class=\"param\">\t<span class=\"n\">request_id</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();