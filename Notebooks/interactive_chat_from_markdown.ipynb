{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, NotebookLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Custom prompt template\n",
    "custom_prompt_template = \"\"\"You are a helpful AI assistant specialized in demand forecasting and related topics.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide your answer following these guidelines:\n",
    "1. Use ONLY information from the provided context\n",
    "2. ALWAYS cite your sources using [Source X] format where X is the source number\n",
    "3. If multiple sources support a statement, cite all relevant sources: [Source 1,2]\n",
    "4. If the context doesn't contain enough information, clearly state that\n",
    "5. Structure your response in a clear, logical manner\n",
    "6. Keep the answer focused and relevant to demand forecasting\n",
    "\n",
    "Remember: Every significant statement should have a source citation.\n",
    "\n",
    "Answer: Let me help you with that.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "PROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "def load_markdown_files(directory: str = \"./data/docs/\") -> List:\n",
    "    \"\"\"Load markdown files from directory\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "def load_jupyter_notebooks(directory: str = \"./demand_forecast_notebooks/\") -> List:\n",
    "    \"\"\"Load jupyter notebooks from directory\"\"\"\n",
    "    documents = []\n",
    "    for notebook_path in glob.glob(f\"{directory}/**/*.ipynb\", recursive=True):\n",
    "        if \".ipynb_checkpoints\" not in notebook_path:\n",
    "            try:\n",
    "                loader = NotebookLoader(\n",
    "                    notebook_path,\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=50,\n",
    "                    remove_newline=True,\n",
    "                )\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notebook {notebook_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def process_documents(documents: List, chunk_size: int = 1000, chunk_overlap: int = 20):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def create_vector_store(documents: List, persist_directory: str = \"./data/chroma_db\"):\n",
    "    \"\"\"Create and persist vector store\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    # Remove the persist() call as it's now automatic\n",
    "    return vectorstore\n",
    "\n",
    "def setup_qa_chain(vectorstore):\n",
    "    \"\"\"Setup the QA chain with custom prompt\"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Create the QA chain with custom prompt\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        chain_type_kwargs={\"prompt\": PROMPT, \"verbose\": True},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "def extract_relevant_context(doc) -> Dict:\n",
    "    \"\"\"Extract and format relevant context from a document.\"\"\"\n",
    "    return {\n",
    "        \"content\": doc.page_content,\n",
    "        \"source\": doc.metadata.get(\"source\", \"Unnamed Source\"),\n",
    "        \"page\": doc.metadata.get(\"page\", None),\n",
    "        \"chunk\": doc.metadata.get(\"chunk\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "def format_citation(source_info: Dict) -> str:\n",
    "    \"\"\"Format source information into a citation.\"\"\"\n",
    "    citation = source_info[\"source\"]\n",
    "    if source_info[\"page\"]:\n",
    "        citation += f\", page {source_info['page']}\"\n",
    "    return citation\n",
    "\n",
    "\n",
    "def ask_question(qa_chain, question: str) -> str:\n",
    "    \"\"\"Ask a question and return a well-formatted answer with citations.\"\"\"\n",
    "    try:\n",
    "        # Get the answer and source documents\n",
    "        result = qa_chain({\"query\": question})\n",
    "\n",
    "        # Extract answer and sources\n",
    "        answer = result[\"result\"]\n",
    "        sources = result.get(\"source_documents\", [])\n",
    "\n",
    "        # Create source mapping\n",
    "        source_map = {}\n",
    "        for idx, doc in enumerate(sources, 1):\n",
    "            source_info = extract_relevant_context(doc)\n",
    "            source_map[idx] = source_info\n",
    "\n",
    "        # Format the response in Markdown\n",
    "        md = f\"### Question\\n{question}\\n\\n\"\n",
    "        md += f\"### Answer\\n{answer}\\n\\n\"\n",
    "\n",
    "        if source_map:\n",
    "            md += \"### References\\n\\n\"\n",
    "            for idx, source_info in source_map.items():\n",
    "                citation = format_citation(source_info)\n",
    "                md += f\"[Source {idx}] {citation}\\n\"\n",
    "                excerpt = source_info[\"content\"][:200].replace(\"\\n\", \" \").strip()\n",
    "                md += f\"> {excerpt}...\\n\\n\"\n",
    "\n",
    "        return md\n",
    "    except Exception as e:\n",
    "        return f\"**An error occurred:** {str(e)}\"\n",
    "\n",
    "\n",
    "def interactive_qa(qa_chain):\n",
    "    \"\"\"Interactive Q&A session with formatted Markdown output and citations.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Markdown, HTML\n",
    "\n",
    "        use_markdown = True\n",
    "    except ImportError:\n",
    "        use_markdown = False\n",
    "\n",
    "    session_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    header = f\"\"\"\n",
    "    # Demand Forecasting Q&A Session\n",
    "    Session started: {session_time}\n",
    "\n",
    "    Enter your questions below. Type 'exit' to end the session.\n",
    "    \"\"\"\n",
    "\n",
    "    if use_markdown:\n",
    "        display(Markdown(header))\n",
    "    else:\n",
    "        print(header)\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        if question.lower() == \"exit\":\n",
    "            footer = \"\\n### Session Ended\\nThank you for using the Q&A system!\"\n",
    "            if use_markdown:\n",
    "                display(Markdown(footer))\n",
    "            else:\n",
    "                print(footer)\n",
    "            break\n",
    "\n",
    "        if use_markdown:\n",
    "            display(Markdown(\"---\\n*Processing your question...*\"))\n",
    "        else:\n",
    "            print(\"\\nProcessing your question...\")\n",
    "\n",
    "        answer_md = ask_question(qa_chain, question)\n",
    "\n",
    "        if use_markdown:\n",
    "            display(Markdown(answer_md))\n",
    "            display(Markdown(\"---\"))\n",
    "        else:\n",
    "            print(answer_md)\n",
    "            print(\"---\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_docs = load_markdown_files(\"../data/docs/\")\n",
    "    notebook_docs = load_jupyter_notebooks(\"./demand_forecast_notebooks/\")\n",
    "\n",
    "    all_documents = markdown_docs + notebook_docs\n",
    "    print(\n",
    "        f\"Loaded {len(markdown_docs)} markdown files and {len(notebook_docs)} notebooks\"\n",
    "    )\n",
    "\n",
    "    print(\"Processing documents...\")\n",
    "    splits = process_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "\n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = create_vector_store(splits)\n",
    "    print(\"Vector store created and persisted\")\n",
    "\n",
    "    qa_chain = setup_qa_chain(vectorstore)\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qa_chain = main()\n",
    "    interactive_qa(qa_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049411c9",
   "metadata": {},
   "source": [
    "# Update it to use LCEL\n",
    "## remove markdown output\n",
    "## chat history added\n",
    "## warnings are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, NotebookLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Custom prompt template\n",
    "custom_prompt_template = \"\"\"You are a helpful AI assistant specialized in demand forecasting and related topics.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide your answer following these guidelines:\n",
    "1. Use ONLY information from the provided context\n",
    "2. ALWAYS cite your sources using [Source X] format where X is the source number\n",
    "3. If multiple sources support a statement, cite all relevant sources: [Source 1,2]\n",
    "4. If the context doesn't contain enough information, clearly state that\n",
    "5. Structure your response in a clear, logical manner\n",
    "6. Keep the answer focused and relevant to demand forecasting\n",
    "\n",
    "Remember: Every significant statement should have a source citation.\n",
    "\n",
    "Answer: Let me help you with that.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "PROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def load_markdown_files(directory: str = \"./data/docs/\") -> List:\n",
    "    \"\"\"Load markdown files from directory\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "def load_jupyter_notebooks(directory: str = \"./demand_forecast_notebooks/\") -> List:\n",
    "    \"\"\"Load jupyter notebooks from directory\"\"\"\n",
    "    documents = []\n",
    "    for notebook_path in glob.glob(f\"{directory}/**/*.ipynb\", recursive=True):\n",
    "        if \".ipynb_checkpoints\" not in notebook_path:\n",
    "            try:\n",
    "                loader = NotebookLoader(\n",
    "                    notebook_path,\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=50,\n",
    "                    remove_newline=True\n",
    "                )\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notebook {notebook_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def process_documents(documents: List, chunk_size: int = 1000, chunk_overlap: int = 20):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def create_vector_store(documents: List, persist_directory: str = \"../data/chroma_db\"):\n",
    "    \"\"\"Create and persist vector store\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def extract_relevant_context(doc) -> Dict:\n",
    "    \"\"\"Extract and format relevant context from a document.\"\"\"\n",
    "    return {\n",
    "        \"content\": doc.page_content,\n",
    "        \"source\": doc.metadata.get(\"source\", \"Unnamed Source\"),\n",
    "        \"page\": doc.metadata.get(\"page\", None),\n",
    "    }\n",
    "\n",
    "def format_citation(source_info: Dict) -> str:\n",
    "    \"\"\"Format source information into a citation.\"\"\"\n",
    "    citation = source_info[\"source\"]\n",
    "    if source_info[\"page\"]:\n",
    "        citation += f\", page {source_info['page']}\"\n",
    "    return citation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72fa6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalRetriever:\n",
    "    def __init__(self, vectorstore):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.memory = []  # List to store conversation history\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> Tuple[List, str]:\n",
    "        # Updated to use invoke instead of get_relevant_documents\n",
    "        retriever = self.vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 3,\n",
    "                \"lambda_mult\": 0.7\n",
    "            }\n",
    "        )\n",
    "        # Use invoke instead of get_relevant_documents\n",
    "        docs = retriever.invoke(query)\n",
    "        return docs\n",
    "\n",
    "    def add_message(self, question: str, answer: str):\n",
    "        \"\"\"Add a message pair to memory\"\"\"\n",
    "        self.memory.append(HumanMessage(content=question))\n",
    "        self.memory.append(AIMessage(content=answer))\n",
    "\n",
    "    def get_chat_history(self) -> str:\n",
    "        \"\"\"Format chat history into a string\"\"\"\n",
    "        if not self.memory:\n",
    "            return \"\"\n",
    "\n",
    "        return \"\\n\".join(\n",
    "            f\"Human: {msg.content}\" if isinstance(msg, HumanMessage)\n",
    "            else f\"Assistant: {msg.content}\"\n",
    "            for msg in self.memory\n",
    "        )\n",
    "\n",
    "def setup_qa_chain(vectorstore):\n",
    "    \"\"\"Setup the QA chain using LCEL with conversation memory\"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Initialize the conversational retriever\n",
    "    conv_retriever = ConversationalRetriever(vectorstore)\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Updated prompt template with chat history\n",
    "    conv_prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a helpful AI assistant specialized in demand forecasting and related topics.\n",
    "\n",
    "Previous conversation:\n",
    "{chat_history}\n",
    "\n",
    "Use the following context to answer the question. If you reference something from the previous conversation,\n",
    "make it explicit by saying \"As discussed earlier...\" or similar phrases.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Please provide your answer following these guidelines:\n",
    "1. Use ONLY information from the provided context and previous conversation\n",
    "2. ALWAYS cite your sources using [Source X] format where X is the source number\n",
    "3. If multiple sources support a statement, cite all relevant sources: [Source 1,2]\n",
    "4. If the context doesn't contain enough information, clearly state that\n",
    "5. Structure your response in a clear, logical manner\n",
    "6. Keep the answer focused and relevant to demand forecasting\n",
    "\n",
    "Answer: Let me help you with that.\n",
    "\"\"\",\n",
    "        input_variables=[\"chat_history\", \"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    def get_response(input_dict):\n",
    "        # Get relevant documents using invoke\n",
    "        docs = conv_retriever.get_relevant_documents(input_dict[\"question\"])\n",
    "\n",
    "        # Prepare the input for the chain\n",
    "        chain_input = {\n",
    "            \"context\": format_docs(docs),\n",
    "            \"question\": input_dict[\"question\"],\n",
    "            \"chat_history\": conv_retriever.get_chat_history()\n",
    "        }\n",
    "\n",
    "        # Generate response\n",
    "        response = conv_prompt.format(**chain_input)\n",
    "        response = llm.invoke(response)\n",
    "        response = StrOutputParser().invoke(response)\n",
    "\n",
    "        # Add to conversation history\n",
    "        conv_retriever.add_message(input_dict[\"question\"], response)\n",
    "\n",
    "        return {\"result\": response, \"source_documents\": docs}\n",
    "\n",
    "    final_chain = RunnablePassthrough() | get_response\n",
    "\n",
    "    return final_chain\n",
    "\n",
    "def ask_question(qa_chain, question: str) -> str:\n",
    "    \"\"\"Ask a question and return a formatted answer with citations.\"\"\"\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"question\": question})\n",
    "\n",
    "        answer = result[\"result\"]\n",
    "        sources = result.get(\"source_documents\", [])\n",
    "\n",
    "        # Create a dictionary to track unique sources\n",
    "        unique_sources = {}\n",
    "        for doc in sources:\n",
    "            source_info = extract_relevant_context(doc)\n",
    "            # Create a unique key based on source and content\n",
    "            key = (source_info[\"source\"], source_info[\"content\"][:100])\n",
    "            if key not in unique_sources:\n",
    "                unique_sources[key] = source_info\n",
    "\n",
    "        # Format the response as plain text\n",
    "        response = f\"Question: {question}\\n\\n\"\n",
    "        response += f\"Answer: {answer}\\n\\n\"\n",
    "\n",
    "        if unique_sources:\n",
    "            response += \"References:\\n\"\n",
    "            for idx, (_, source_info) in enumerate(unique_sources.items(), 1):\n",
    "                citation = format_citation(source_info)\n",
    "                response += f\"[Source {idx}] {citation}\\n\"\n",
    "                excerpt = source_info[\"content\"][:150].replace(\"\\n\", \" \").strip()\n",
    "                response += f\"Excerpt: {excerpt}...\\n\\n\"\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "    \n",
    "def interactive_qa(qa_chain):\n",
    "    \"\"\"Interactive Q&A session with chat memory and plain text output.\"\"\"\n",
    "    print(\"\\nDemand Forecasting Q&A Session\")\n",
    "    print(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"\\nEnter your questions below. Type 'exit' to end the session.\")\n",
    "    print(\"Type 'history' to see the conversation history.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "\n",
    "        # Handle special commands\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"\\nSession Ended. Thank you for using the Q&A system!\")\n",
    "            break\n",
    "\n",
    "        if question.lower() == \"history\":\n",
    "            print(\"\\nConversation History:\")\n",
    "            if not conversation_history:\n",
    "                print(\"No previous conversations.\")\n",
    "            else:\n",
    "                for i, (q, a) in enumerate(conversation_history, 1):\n",
    "                    print(f\"\\nDialog {i}:\")\n",
    "                    print(f\"Q: {q}\")\n",
    "                    print(f\"A: {a}\")\n",
    "            print(\"-\" * 50)\n",
    "            continue\n",
    "\n",
    "        print(\"\\nProcessing your question...\")\n",
    "\n",
    "        # Add conversation context to the question\n",
    "        if conversation_history:\n",
    "            context = \"\\n\".join([\n",
    "                f\"Q: {q}\\nA: {a}\"\n",
    "                for q, a in conversation_history[-3:]  # Last 3 conversations\n",
    "            ])\n",
    "            print(\"\\nUsing context from previous conversations...\")\n",
    "\n",
    "        # Get the answer\n",
    "        answer_text = ask_question(qa_chain, question)\n",
    "\n",
    "        # Extract just the answer part (without the references) for history\n",
    "        answer_only = answer_text.split(\"References:\")[0].strip()\n",
    "\n",
    "        # Store in conversation history\n",
    "        conversation_history.append((question, answer_only))\n",
    "\n",
    "        # Print the full response including references\n",
    "        print(\"\\n\" + answer_text)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # If the conversation is getting too long, keep only the last N interactions\n",
    "        if len(conversation_history) > 5:  # Keep last 5 conversations\n",
    "            conversation_history = conversation_history[-5:]\n",
    "\n",
    "def main():\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_docs = load_markdown_files(\"../data/docs/\")\n",
    "    notebook_docs = load_jupyter_notebooks(\"./demand_forecast_notebooks/\")\n",
    "\n",
    "    all_documents = markdown_docs + notebook_docs\n",
    "    print(f\"Loaded {len(markdown_docs)} markdown files and {len(notebook_docs)} notebooks\")\n",
    "\n",
    "    print(\"Processing documents...\")\n",
    "    splits = process_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "\n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = create_vector_store(splits)\n",
    "    print(\"Vector store created and persisted\")\n",
    "\n",
    "    # Create the chain with memory\n",
    "    qa_chain = setup_qa_chain(vectorstore)\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf30e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 5 markdown files and 13 notebooks\n",
      "Processing documents...\n",
      "Created 485 splits\n",
      "Creating vector store...\n",
      "Vector store created and persisted\n",
      "\n",
      "Demand Forecasting Q&A Session\n",
      "Session started: 2025-04-30 22:34:03\n",
      "\n",
      "Enter your questions below. Type 'exit' to end the session.\n",
      "Type 'history' to see the conversation history.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Question: is there a mentino to arima here\n",
      "\n",
      "Answer: Yes, there is a mention of ARIMA in the provided context. It states that a process $X_t$ is ARIMA(p,d,q) if and only if $\\nabla^d X_t$ is a stationary ARMA(p,q) [Source 1]. This highlights the relationship between ARIMA and ARMA models, where differencing the series at appropriate lags allows for the extension from ARMA to ARIMA to address issues such as trend and seasonality in time series forecasting.\n",
      "\n",
      "References:\n",
      "[Source 1] demand_forecast_notebooks/ts-2-linear-vision.ipynb\n",
      "Excerpt: 'markdown' cell: '<a id=\"section-two\"></a># Beyond ARMAAs we can see from the examples above, there are issues when applying the baseline $ARMA(p,q)$...\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "Using context from previous conversations...\n",
      "\n",
      "Question: how did we use that in these documenatinon\n",
      "\n",
      "Answer: Based on the provided context, there is no specific mention of how ARIMA was used in the official documentation. Therefore, without additional information from the documentation itself, it is not possible to determine how ARIMA was specifically utilized in that context. If you need further details on how ARIMA was applied in the documentation, I recommend referring directly to the source material for a more comprehensive understanding.\n",
      "\n",
      "References:\n",
      "[Source 1] demand_forecast_notebooks/ts-5-automatic-for-the-people.ipynb\n",
      "Excerpt: from the official documentation:'...\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Session Ended. Thank you for using the Q&A system!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    qa_chain = main()\n",
    "    interactive_qa(qa_chain)\n",
    "\n",
    "# the memory still not perfoming perfect. it needs more attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15628e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_genai_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
