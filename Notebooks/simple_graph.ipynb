{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load markdown files\n",
      "Load Jupyter notebooks\n",
      "Path: ../data/demand_forecast_notebooks/ts-5-automatic-for-the-people.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-4-sales-and-demand-forecasting.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-2-linear-vision.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-11-deep-learning-for-ts-transfer-learning.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-1a-smoothing-methods.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-1b-prophet.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-7-survival-analysis.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-9-hybrid-methods.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-8-hierarchical-time-series.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-10-validation-methods-for-time-series.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-6-deep-learning-for-ts-rnn-and-friends.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-0-the-basics.ipynb\n",
      "Path: ../data/demand_forecast_notebooks/ts-3-time-series-for-finance.ipynb\n",
      "Combine all documents\n",
      "Process documents into chunks\n",
      "Total documents after splitting: 156\n",
      "Create and persist vector store\n",
      "Persisting vector store to disk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persisting vector store to disk completed.\n",
      "Vector store created with 8 documents.\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import glob\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (DirectoryLoader,\n",
    "                                                  NotebookLoader,\n",
    "                                                  UnstructuredMarkdownLoader)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from genai_docs_helper.config import LLM_TYPE\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "EMBEDDING = (\n",
    "    OllamaEmbeddings(\n",
    "        model=\"llama3.2\",\n",
    "        # verbose=True\n",
    "    )\n",
    "    if LLM_TYPE == \"ollama\"\n",
    "    else OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "def load_markdown_files(directory: str = \"../data/docs/\") -> List:\n",
    "    \"\"\"Load markdown files from directory\"\"\"\n",
    "    loader = DirectoryLoader(directory, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "def load_jupyter_notebooks(directory: str = \"../data/demand_forecast_notebooks/\") -> List:\n",
    "    \"\"\"Load jupyter notebooks from directory\"\"\"\n",
    "    documents = []\n",
    "    for notebook_path in glob.glob(f\"{directory}/**/*.ipynb\", recursive=True):\n",
    "        print(f\"Path: {notebook_path}\")\n",
    "        if \".ipynb_checkpoints\" not in notebook_path:\n",
    "            try:\n",
    "                loader = NotebookLoader(notebook_path, include_outputs=True, max_output_length=50, remove_newline=True)\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notebook {notebook_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def process_documents(documents: List, chunk_size: int = 3000, chunk_overlap: int = 20):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    splited_documents = text_splitter.split_documents(documents)\n",
    "    print(f\"Total documents after splitting: {len(splited_documents)}\")\n",
    "    return splited_documents\n",
    "\n",
    "\n",
    "def create_vector_store(documents: List, persist_directory: str = \"../data/chroma_db\"):\n",
    "    \"\"\"Create and persist vector store\"\"\"\n",
    "    print(\"Persisting vector store to disk...\")\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(documents=documents, embedding=EMBEDDING, persist_directory=persist_directory)\n",
    "    print(\"Persisting vector store to disk completed.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def extract_relevant_context(doc) -> Dict:\n",
    "    \"\"\"Extract and format relevant context from a document.\"\"\"\n",
    "    return {\n",
    "        \"content\": doc.page_content,\n",
    "        \"source\": doc.metadata.get(\"source\", \"Unnamed Source\"),\n",
    "        \"page\": doc.metadata.get(\"page\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "def format_citation(source_info: Dict) -> str:\n",
    "    \"\"\"Format source information into a citation.\"\"\"\n",
    "    citation = source_info[\"source\"]\n",
    "    if source_info[\"page\"]:\n",
    "        citation += f\", page {source_info['page']}\"\n",
    "    return citation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Load markdown files\")\n",
    "    markdown_docs = load_markdown_files()\n",
    "\n",
    "    print(\"Load Jupyter notebooks\")\n",
    "    notebook_docs = load_jupyter_notebooks()\n",
    "\n",
    "    print(\"Combine all documents\")\n",
    "    all_docs = markdown_docs + notebook_docs\n",
    "\n",
    "    print(\"Process documents into chunks\")\n",
    "    processed_docs = process_documents(all_docs)\n",
    "\n",
    "    print(\"Create and persist vector store\")\n",
    "    vector_store = create_vector_store(processed_docs[:2])\n",
    "\n",
    "    print(f\"Vector store created with {len(vector_store)} documents.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-docs-helper-TPIRBE5T-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
