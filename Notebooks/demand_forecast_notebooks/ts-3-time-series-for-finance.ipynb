{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2179861,"sourceType":"datasetVersion","datasetId":423609}],"dockerImageVersionId":30152,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**If you think this notebook deserves an upvote, I'd love to have it. An upvote per view, its all I ask**\n(credit to [Dan Carlin](https://twitter.com/HardcoreHistory) for coining the phrase ;-) \n\n---------------------------------------\n\nThis is part of a series of notebooks about practical time series methods:\n\n* [Part 0: the basics](https://www.kaggle.com/konradb/ts-0-the-basics)\n* [Part 1a: smoothing methods](https://www.kaggle.com/konradb/ts-1a-smoothing-methods)\n* [Part 1b: Prophet](https://www.kaggle.com/konradb/ts-1b-prophet) \n* [Part 2: ARMA](https://www.kaggle.com/konradb/ts-2-arma-and-friends)\n* [Part 3: Time series for finance](https://www.kaggle.com/konradb/ts-3-time-series-for-finance) - **this notebook**\n* [Part 4: Sales and demand forecasting](https://www.kaggle.com/konradb/ts-4-sales-and-demand-forecasting)\n* [Part 5: Automatic for the people](https://www.kaggle.com/code/konradb/ts-5-automatic-for-the-people)\n* [Part 6: Deep learning for TS - sequences](https://www.kaggle.com/konradb/ts-6-deep-learning-for-ts-sequences)\n* [Part 7: Survival analysis](https://www.kaggle.com/code/konradb/ts-7-survival-analysis)\n* [Part 8: Hierarchical time series](https://www.kaggle.com/code/konradb/ts-8-hierarchical-time-series)\n* [Part 9: Hybrid methods](https://www.kaggle.com/code/konradb/ts-9-hybrid-methods/)\n* [Part 10: Validation methods for time series](https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series/)\n* [Part 11: Transfer learning](https://www.kaggle.com/code/konradb/ts-11-deep-learning-for-ts-transfer-learning)\n\n\nThe series is accompanied by video presentations on the YouTube channel of [Abhishek](https://www.kaggle.com/abhishek):\n\n* [Talk 0](https://www.youtube.com/watch?v=cKzXOOtOXYY) \n* [Talk 1](https://www.youtube.com/watch?v=kAI67Sz92-s) - combining the content from parts 1a and 1b\n* [Talk 2](https://www.youtube.com/watch?v=LjV5DE3KR-U) \n* [Talk 3](https://www.youtube.com/watch?v=74rDhJexmTg) - **based on this notebook**\n* [Talk 4](https://www.youtube.com/watch?v=RdH8zd07u2E)  \n* [Talk 5](https://www.youtube.com/watch?v=wBP8Pc4Wxzs)\n* [Talk 6](https://www.youtube.com/watch?v=81AEI0tj0Kk)\n* [Talk 7](https://www.youtube.com/watch?v=m-8I_hkmz9o)\n* [Talk 8](https://www.youtube.com/watch?v=7ZTarg4QYR4)\n* [Talk 9](https://www.youtube.com/watch?v=NYZzBvKcfp4)\n* [Talk 10](https://www.youtube.com/watch?v=47WeBiLV2Uo)\n* [Talk 11]()\n\n\n---------------------------------------\n\n\nThis notebook was inspired by the Q&A part of the YouTube session about ARIMA and friends - the repeated questions about stock performance forecasting suggested it was a topic that reasonated with a lot of people. When we were discussing ARIMA, applying the logic underlying those models was mentioned briefly; one thing leading to another, I decided to add an episode in the series and talk about modeling techniques useful specifically (but not exclusively!) in the financial context.\n\n* [Volatility clustering](#section-one)\n* [Mean and variance models](#section-two)\n* [Forecasting](#section-three)\n* [Value at Risk](#section-four)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import itertools\nimport pandas as pd\nimport numpy as np\nfrom random import gauss\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.stats.diagnostic import het_arch, acorr_ljungbox\n\nfrom scipy.stats import shapiro\nfrom scipy.stats import probplot\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n\nimport warnings\nwarnings.simplefilter(action='ignore', category= FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:20:45.350074Z","iopub.execute_input":"2022-03-05T15:20:45.350928Z","iopub.status.idle":"2022-03-05T15:20:45.405901Z","shell.execute_reply.started":"2022-03-05T15:20:45.350869Z","shell.execute_reply":"2022-03-05T15:20:45.404857Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# general settings\nclass CFG:\n    data_folder = '../input/tsdata-1/'\n    img_dim1 = 20\n    img_dim2 = 10\n        \n# adjust the parameters for displayed figures    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})    ","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:20:55.210189Z","iopub.execute_input":"2022-03-05T15:20:55.210883Z","iopub.status.idle":"2022-03-05T15:20:55.216089Z","shell.execute_reply.started":"2022-03-05T15:20:55.21082Z","shell.execute_reply":"2022-03-05T15:20:55.215225Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install arch","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:54:58.35751Z","iopub.execute_input":"2022-03-05T14:54:58.357808Z","iopub.status.idle":"2022-03-05T14:55:08.65596Z","shell.execute_reply.started":"2022-03-05T14:54:58.357758Z","shell.execute_reply":"2022-03-05T14:55:08.654803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Volatility clustering\n\nOne of the reason financial time series are challenging to forecast is volatility clustering - periods of high variation in the data cluster together, followed by periods of more stable behavior. You can see an example in the graph below:\n","metadata":{}},{"cell_type":"code","source":"stock_name = 'TATASTEEL'\ndf = pd.read_csv('../input/nifty50-stock-market-data/'+stock_name+'.csv')\ndf.set_index(\"Date\", drop=False, inplace=True)\n\ndf.VWAP.pct_change().plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:55:08.658395Z","iopub.execute_input":"2022-03-05T14:55:08.658656Z","iopub.status.idle":"2022-03-05T14:55:09.122435Z","shell.execute_reply.started":"2022-03-05T14:55:08.658627Z","shell.execute_reply":"2022-03-05T14:55:09.121764Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dfl = np.log1p(df.VWAP).diff().dropna()\nplot_acf(dfl, lags = 10, title='Autocorrelation - returns'); print()\nplot_acf(dfl**2, lags = 10, title='Autocorrelation - squared returns'); print()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:55:09.12445Z","iopub.execute_input":"2022-03-05T14:55:09.125156Z","iopub.status.idle":"2022-03-05T14:55:09.559917Z","shell.execute_reply.started":"2022-03-05T14:55:09.125118Z","shell.execute_reply":"2022-03-05T14:55:09.558962Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nplot_pacf(dfl, lags = 10, title='Partial autocorrelation - returns'); print()\nplot_pacf(dfl**2, lags = 10, title='Partial autocorrelation - squared returns'); print()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:55:09.561204Z","iopub.execute_input":"2022-03-05T14:55:09.561464Z","iopub.status.idle":"2022-03-05T14:55:10.093249Z","shell.execute_reply.started":"2022-03-05T14:55:09.56143Z","shell.execute_reply":"2022-03-05T14:55:10.092042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"How do we go about modeling this? \n\n<a id=\"section-two\"></a>\n# Mean and variance models\n\nIn the ARIMA episode https://www.kaggle.com/konradb/ts-2-arma-and-friends we have described how a linear model can be specified to predict (conditional) expected value. Similar logic can be applied to build a model for predicting (conditional) variance of the residuals from another model - **and not the value of the series itself**.\n\n","metadata":{}},{"cell_type":"markdown","source":"## ARCH\n\nA simplest model for time-varying variance is ARCH - the abbreviation stands for **a**uto**r**egressive **c**onditional **h**eteroskedasticity:\n\n\\begin{equation}\n\\sigma_t^2 = \\alpha_0 + \\sum_{i=1}^p \\alpha_i \\epsilon_{t-i}^2 \\\\\n\\end{equation}\n\nwhere $\\epsilon_t = \\sigma_t \\; z_t$. Unpacking:\n\n* AR(p) model: the current value (mean / expectation) of the series depends on $p$  previous timesteps values\n* ARCH(p): the variance of the current time step is dependent on $p$ lagged squared standard error terms. The standard error is the difference between the observed and predicted value from another model.\n* estimated with OLS \n* Lagrange multiplier test\n\nLet's see how we can go about this in practice:","metadata":{}},{"cell_type":"code","source":"from arch import arch_model\n\nam = arch_model(100 * dfl, p = 1, q = 0)\nres = am.fit(update_freq=5)\nprint(res.summary())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:19:46.589308Z","iopub.execute_input":"2022-03-05T15:19:46.590277Z","iopub.status.idle":"2022-03-05T15:19:46.612852Z","shell.execute_reply.started":"2022-03-05T15:19:46.590226Z","shell.execute_reply":"2022-03-05T15:19:46.611755Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res.plot(); print()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:55:11.652456Z","iopub.execute_input":"2022-03-05T14:55:11.652873Z","iopub.status.idle":"2022-03-05T14:56:48.83449Z","shell.execute_reply.started":"2022-03-05T14:55:11.652822Z","shell.execute_reply":"2022-03-05T14:56:48.833467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can run some diagnostics on the model:\n* Ljung-Box: \n\na test for autocorrelation (usable in tandem with ACF / PACF plots). H0 : The data is independently distributed, no autocorrelation, H1 : there is serial correlation.\n\n* Engle ARCH: Ljung-Box and a Lagrange Multiplier test to determine if our ARCH model has captured the conditional heteroskedasticity \n\nH0 : the residuals are homoscedastic H1: squared residuals exhibit heteroskedasticity","metadata":{}},{"cell_type":"code","source":"max_lags = 5\nresiduals = res.resid\nst_residuals = np.divide(res.resid, res.conditional_volatility)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:48.836147Z","iopub.execute_input":"2022-03-05T14:56:48.837003Z","iopub.status.idle":"2022-03-05T14:56:48.843411Z","shell.execute_reply.started":"2022-03-05T14:56:48.83695Z","shell.execute_reply":"2022-03-05T14:56:48.842403Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_acf(dfl ** 2); print()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:48.844546Z","iopub.execute_input":"2022-03-05T14:56:48.844855Z","iopub.status.idle":"2022-03-05T14:56:49.145112Z","shell.execute_reply.started":"2022-03-05T14:56:48.844797Z","shell.execute_reply":"2022-03-05T14:56:49.144093Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_acf(st_residuals ** 2); print()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.146763Z","iopub.execute_input":"2022-03-05T14:56:49.147424Z","iopub.status.idle":"2022-03-05T14:56:49.441854Z","shell.execute_reply.started":"2022-03-05T14:56:49.147387Z","shell.execute_reply":"2022-03-05T14:56:49.440752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ljung-Box\n\nlb_test = acorr_ljungbox(dfl ** 2 , lags = max_lags)\nfor lag in range(max_lags):\n    print('lag: ' + str(lag) + ' pval: ' + str(np.round( lb_test[1][lag], 4) ))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.443465Z","iopub.execute_input":"2022-03-05T14:56:49.443697Z","iopub.status.idle":"2022-03-05T14:56:49.460132Z","shell.execute_reply.started":"2022-03-05T14:56:49.44367Z","shell.execute_reply":"2022-03-05T14:56:49.458767Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lb_test = acorr_ljungbox(st_residuals ** 2 , lags = max_lags)\nfor lag in range(max_lags):\n    print('lag: ' + str(lag) + ' pval: ' + str(np.round( lb_test[1][lag], 4) ))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.462914Z","iopub.execute_input":"2022-03-05T14:56:49.463289Z","iopub.status.idle":"2022-03-05T14:56:49.479913Z","shell.execute_reply.started":"2022-03-05T14:56:49.463253Z","shell.execute_reply":"2022-03-05T14:56:49.478852Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Engle ARCH\netest = het_arch(dfl, maxlag=5)\nprint('pval Lagrange: ' + str(np.round(etest[1],4)))\nprint('pval F-test: ' + str(np.round(etest[1],4)))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.484094Z","iopub.execute_input":"2022-03-05T14:56:49.484348Z","iopub.status.idle":"2022-03-05T14:56:49.500949Z","shell.execute_reply.started":"2022-03-05T14:56:49.484318Z","shell.execute_reply":"2022-03-05T14:56:49.49996Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"etest = het_arch(st_residuals, maxlag=5)\nprint('pval Lagrange: ' + str(np.round(etest[1],4)))\nprint('pval F-test: ' + str(np.round(etest[1],4)))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.502536Z","iopub.execute_input":"2022-03-05T14:56:49.503114Z","iopub.status.idle":"2022-03-05T14:56:49.519965Z","shell.execute_reply.started":"2022-03-05T14:56:49.503064Z","shell.execute_reply":"2022-03-05T14:56:49.518906Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GARCH\n\nGARCH is an extension of ARCH - it allows for variance depending on its own lags and the lags of the squared residuals. The model formula is:\n\n\\begin{equation}\n\\sigma_t^2 = \\omega + \\sum_{i=1}^p \\alpha_i \\epsilon_{t-i}^2 + \\sum_{i=1}^q \\beta_i \\sigma_{t-i}^2\\\\\n\\end{equation}\n \nUnpacking:\n* p - number of lag error terms\n* q - number of lag variances\n* `arch` notation!\n\n\nIn theory you can specify different orders of $(p,q)$, but absent a good reason to think otherwise it's usually good to start with (1,1).\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"gm = arch_model(100 * dfl, p = 1, q = 1)\nres = gm.fit(update_freq=5)\nprint(res.summary())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.521986Z","iopub.execute_input":"2022-03-05T14:56:49.522558Z","iopub.status.idle":"2022-03-05T14:56:49.667527Z","shell.execute_reply.started":"2022-03-05T14:56:49.522505Z","shell.execute_reply":"2022-03-05T14:56:49.666509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ljung-Box\nresiduals = res.resid\nst_residuals = np.divide(res.resid, res.conditional_volatility)\nlb_test = acorr_ljungbox(dfl ** 2 , lags = max_lags)\nfor lag in range(max_lags):\n    print('lag: ' + str(lag) + ' pval: ' + str(np.round( lb_test[1][lag], 4) ))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.673213Z","iopub.execute_input":"2022-03-05T14:56:49.674134Z","iopub.status.idle":"2022-03-05T14:56:49.707732Z","shell.execute_reply.started":"2022-03-05T14:56:49.674069Z","shell.execute_reply":"2022-03-05T14:56:49.706698Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lb_test = acorr_ljungbox(st_residuals ** 2 , lags = max_lags)\nfor lag in range(max_lags):\n    print('lag: ' + str(lag) + ' pval: ' + str(np.round( lb_test[1][lag], 4) ))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.71351Z","iopub.execute_input":"2022-03-05T14:56:49.71459Z","iopub.status.idle":"2022-03-05T14:56:49.745477Z","shell.execute_reply.started":"2022-03-05T14:56:49.714528Z","shell.execute_reply":"2022-03-05T14:56:49.744203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Engle ARCH\netest = het_arch(dfl, maxlag=5)\nprint('pval Lagrange: ' + str(np.round(etest[1],4)))\nprint('pval F-test: ' + str(np.round(etest[1],4)))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.751382Z","iopub.execute_input":"2022-03-05T14:56:49.751906Z","iopub.status.idle":"2022-03-05T14:56:49.774569Z","shell.execute_reply.started":"2022-03-05T14:56:49.751853Z","shell.execute_reply":"2022-03-05T14:56:49.773131Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"etest = het_arch(st_residuals, maxlag=5)\nprint('pval Lagrange: ' + str(np.round(etest[1],4)))\nprint('pval F-test: ' + str(np.round(etest[1],4)))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.776491Z","iopub.execute_input":"2022-03-05T14:56:49.777258Z","iopub.status.idle":"2022-03-05T14:56:49.79213Z","shell.execute_reply.started":"2022-03-05T14:56:49.777186Z","shell.execute_reply":"2022-03-05T14:56:49.791105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extensions\n\n* numerous extensions are possible to the basic GARCH \n* GJR - use asymmetric shock:\n\\begin{equation}\n\\sigma_t^2 = \\omega + \\alpha_1 \\epsilon_{t-1}^2 + \\gamma \\mathbf{1}_{\\epsilon_{t-1} < 0 } + \\beta_1 \\sigma_{t-1}^2\\\\\n\\end{equation}\n\n* TARCH - absolute values:\n\\begin{equation}\n\\sigma_t^k  = \\omega + \\alpha_1 \\left| \\epsilon_{t-1} \\right|^k + \\gamma \\left| \\epsilon_{t-1} \\right|^k   \\mathbf{1}_{\\epsilon_{t-1} < 0 } + \\beta_1 \\sigma_{t-1}^k\\\\\n\\end{equation}\n","metadata":{}},{"cell_type":"markdown","source":"## ARMA-GARCH\n\n* we assumed constant **mean** so far - frequently not true \n* combine ARIMA and GARCH: conditional mean $\\rightarrow$ conditional variance on residuals \n* joint estimation > sequential - estimate consistency","metadata":{}},{"cell_type":"code","source":"from arch.univariate import ARX\n\nar = ARX(100 * dfl, lags=[1, 3])\nprint(ar.fit().summary())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.794645Z","iopub.execute_input":"2022-03-05T14:56:49.795376Z","iopub.status.idle":"2022-03-05T14:56:49.827399Z","shell.execute_reply.started":"2022-03-05T14:56:49.79532Z","shell.execute_reply":"2022-03-05T14:56:49.826464Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from arch.univariate import ARCH, GARCH\n\nar.volatility = GARCH(p=1, q = 1)\nres = ar.fit(update_freq=0, disp=\"off\")\nprint(res.summary())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:49.829451Z","iopub.execute_input":"2022-03-05T14:56:49.830149Z","iopub.status.idle":"2022-03-05T14:56:50.003675Z","shell.execute_reply.started":"2022-03-05T14:56:49.830096Z","shell.execute_reply":"2022-03-05T14:56:50.002666Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = res.plot()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:56:50.005809Z","iopub.execute_input":"2022-03-05T14:56:50.006489Z","iopub.status.idle":"2022-03-05T14:58:21.712655Z","shell.execute_reply.started":"2022-03-05T14:56:50.006435Z","shell.execute_reply":"2022-03-05T14:58:21.711756Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Forecasting\n\n* multi-step forecasts: recursion $\\rightarrow$ analytical form $\\sim$ linear in squared residual\n* simulation: parametric distribution\n* bootstrap: akin to simulation, but empirical residuals\n","metadata":{}},{"cell_type":"code","source":"\ndfl = np.log1p(df.VWAP).diff().dropna()\nprint(dfl.index.min(), dfl.index.max())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:58:21.713877Z","iopub.execute_input":"2022-03-05T14:58:21.714101Z","iopub.status.idle":"2022-03-05T14:58:21.723667Z","shell.execute_reply.started":"2022-03-05T14:58:21.714074Z","shell.execute_reply":"2022-03-05T14:58:21.722671Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split into training and validation\nxtr, xval = dfl.loc[:'2021-01-01'], dfl.loc['2021-01-01':]","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:58:21.724931Z","iopub.execute_input":"2022-03-05T14:58:21.725172Z","iopub.status.idle":"2022-03-05T14:58:21.739891Z","shell.execute_reply.started":"2022-03-05T14:58:21.725142Z","shell.execute_reply":"2022-03-05T14:58:21.738847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model \n\nar = ARX(100 * xtr, lags=[1])\nar.volatility = GARCH(p=1, q = 1)\nres = ar.fit(update_freq=0, disp=\"off\")\nprint(res.summary())\n","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:58:21.741069Z","iopub.execute_input":"2022-03-05T14:58:21.741651Z","iopub.status.idle":"2022-03-05T14:58:21.916763Z","shell.execute_reply.started":"2022-03-05T14:58:21.741611Z","shell.execute_reply":"2022-03-05T14:58:21.91576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# build the forecast\nxfor = res.forecast(horizon= xval.shape[0])\n\n# ugly workaround, I know\nmean_forecast = xfor.mean.tail(1).values.reshape(len(xval),1)\n\nforecast = pd.DataFrame()\nforecast['Date'] = xval.index\nforecast['VWAP'] = xval.values\nforecast['fc_mean'] = xfor.mean.tail(1).values.reshape(len(xval),1)\nforecast['fc_std'] = np.sqrt(xfor.variance.tail(1).values.reshape(len(xval),1))","metadata":{"execution":{"iopub.status.busy":"2022-03-05T14:58:21.918459Z","iopub.execute_input":"2022-03-05T14:58:21.919506Z","iopub.status.idle":"2022-03-05T14:58:21.950927Z","shell.execute_reply.started":"2022-03-05T14:58:21.919446Z","shell.execute_reply":"2022-03-05T14:58:21.949832Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecast","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:02:15.304817Z","iopub.execute_input":"2022-03-05T15:02:15.30522Z","iopub.status.idle":"2022-03-05T15:02:15.330029Z","shell.execute_reply.started":"2022-03-05T15:02:15.305182Z","shell.execute_reply":"2022-03-05T15:02:15.329326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Value at Risk\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(url=\"http://upload.wikimedia.org/wikipedia/commons/6/64/VaR_diagram.JPG\")","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:23:58.778156Z","iopub.execute_input":"2022-03-05T15:23:58.778517Z","iopub.status.idle":"2022-03-05T15:23:58.785404Z","shell.execute_reply.started":"2022-03-05T15:23:58.778484Z","shell.execute_reply":"2022-03-05T15:23:58.784768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* VaR is a measure of the risk of loss for investments\n* imperfect, but very popular\n* estimates how much a set of investments might lose (with a given probability), given normal market conditions, in a set time period such as a day\n* GARCH: VaR $\\sim$ conditional mean, the conditional volatility and the quantile of the standardized residuals\n\n\\begin{equation}\nVaR_{t+1|t} = -\\mu_{t+1|t} - \\sigma_{t+1|t} q_q\n\\end{equation}\n","metadata":{}},{"cell_type":"code","source":"import arch.data.sp500\n\ndata = arch.data.sp500.load()\nmarket = data[\"Adj Close\"]\nreturns = 100 * market.pct_change().dropna()","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:19:13.991813Z","iopub.execute_input":"2022-03-05T15:19:13.992082Z","iopub.status.idle":"2022-03-05T15:19:15.480423Z","shell.execute_reply.started":"2022-03-05T15:19:13.992054Z","shell.execute_reply":"2022-03-05T15:19:15.479547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"am = arch_model(returns, vol=\"Garch\", p=1, o=0, q=1, dist=\"skewt\")\nres = am.fit(disp=\"off\", last_obs=\"2017-12-31\")","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:20:09.334605Z","iopub.execute_input":"2022-03-05T15:20:09.335207Z","iopub.status.idle":"2022-03-05T15:20:09.481065Z","shell.execute_reply.started":"2022-03-05T15:20:09.33515Z","shell.execute_reply":"2022-03-05T15:20:09.480204Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"forecasts = res.forecast(start=\"2018-1-1\", reindex=False)\ncond_mean = forecasts.mean[\"2018\":]\ncond_var = forecasts.variance[\"2018\":]\nq = am.distribution.ppf([0.01, 0.05], res.params[-2:])\nprint(q)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:20:22.450397Z","iopub.execute_input":"2022-03-05T15:20:22.450728Z","iopub.status.idle":"2022-03-05T15:20:22.472345Z","shell.execute_reply.started":"2022-03-05T15:20:22.450678Z","shell.execute_reply":"2022-03-05T15:20:22.471658Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"value_at_risk = -cond_mean.values - np.sqrt(cond_var).values * q[None, :]\nvalue_at_risk = pd.DataFrame(value_at_risk, columns=[\"1%\", \"5%\"], index=cond_var.index)\nax = value_at_risk.plot(legend=False)\nxl = ax.set_xlim(value_at_risk.index[0], value_at_risk.index[-1])\nrets_2018 = returns[\"2018\":].copy()\nrets_2018.name = \"S&P 500 Return\"\nc = []\nfor idx in value_at_risk.index:\n    if rets_2018[idx] > -value_at_risk.loc[idx, \"5%\"]:\n        c.append(\"#000000\")\n    elif rets_2018[idx] < -value_at_risk.loc[idx, \"1%\"]:\n        c.append(\"#BB0000\")\n    else:\n        c.append(\"#BB00BB\")\nc = np.array(c, dtype=\"object\")\nlabels = {\"#BB0000\": \"1% Exceedence\", \"#BB00BB\": \"5% Exceedence\", \"#000000\": \"No Exceedence\",}\nmarkers = {\"#BB0000\": \"x\", \"#BB00BB\": \"s\", \"#000000\": \"o\"}\nfor color in np.unique(c):\n    sel = c == color\n    ax.scatter(\n        rets_2018.index[sel],\n        -rets_2018.loc[sel],\n        marker=markers[color],\n        c=c[sel],\n        label=labels[color],\n    )\nax.set_title(\"Parametric VaR\")\nleg = ax.legend(frameon=False, ncol=3)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T15:21:19.723292Z","iopub.execute_input":"2022-03-05T15:21:19.723898Z","iopub.status.idle":"2022-03-05T15:21:20.198496Z","shell.execute_reply.started":"2022-03-05T15:21:19.723846Z","shell.execute_reply":"2022-03-05T15:21:20.197467Z"},"trusted":true},"outputs":[],"execution_count":null}]}