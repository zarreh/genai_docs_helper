{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61ee431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, NotebookLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI  # Changed this import\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "def load_markdown_files(directory: str=\"../data/docs/\") -> List:\n",
    "    \"\"\"Load markdown files from directory\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "def load_jupyter_notebooks(directory: str=\"./demand_forecast_notebooks/\") -> List:\n",
    "    \"\"\"Load jupyter notebooks from directory\"\"\"\n",
    "    documents = []\n",
    "    # Find all .ipynb files in the directory and subdirectories\n",
    "    for notebook_path in glob.glob(f\"{directory}/**/*.ipynb\", recursive=True):\n",
    "        if \".ipynb_checkpoints\" not in notebook_path:  # Skip checkpoint files\n",
    "            try:\n",
    "                loader = NotebookLoader(\n",
    "                    notebook_path,\n",
    "                    include_outputs=True,  # Include cell outputs\n",
    "                    max_output_length=50,   # Limit output length\n",
    "                    remove_newline=True     # Clean up newlines\n",
    "                )\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notebook {notebook_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def process_documents(documents: List, chunk_size: int = 1000, chunk_overlap: int = 20):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def create_vector_store(documents: List, persist_directory: str = \"../data/chroma_db\"):\n",
    "    \"\"\"Create and persist vector store\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def setup_qa_chain(vectorstore):\n",
    "    \"\"\"Setup the QA chain\"\"\"\n",
    "    # Changed the LLM initialization to use ChatOpenAI\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",  # Cheapest available model\n",
    "        temperature=0,               # Deterministic, minimizes hallucination\n",
    "    )\n",
    "\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    print(\"Loading documents...\")\n",
    "    # Load both markdown files and notebooks\n",
    "    markdown_docs = load_markdown_files(\"../data/docs/\")\n",
    "    notebook_docs = load_jupyter_notebooks(\"./demand_forecast_notebooks/\")\n",
    "\n",
    "    # Combine all documents\n",
    "    all_documents = markdown_docs + notebook_docs\n",
    "    print(f\"Loaded {len(markdown_docs)} markdown files and {len(notebook_docs)} notebooks\")\n",
    "\n",
    "    # Process documents\n",
    "    print(\"Processing documents...\")\n",
    "    splits = process_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "\n",
    "    # Create vector store\n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = create_vector_store(splits)\n",
    "    print(\"Vector store created and persisted\")\n",
    "\n",
    "    # Setup QA chain\n",
    "    qa_chain = setup_qa_chain(vectorstore)\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "# Interactive QA function\n",
    "def ask_question(qa_chain, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question about the loaded documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = qa_chain.run(question)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def interactive_qa(qa_chain):\n",
    "    \"\"\"\n",
    "    Interactive Q&A session with the loaded documents.\n",
    "    \"\"\"\n",
    "    print(\"\\nEnter your questions (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        question = input(\"\\nQuestion: \").strip()\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = ask_question(qa_chain, question)\n",
    "        print(\"\\nAnswer:\", answer)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize the system\n",
    "#     qa_chain = main()\n",
    "#     # Start interactive session\n",
    "#     interactive_qa(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1baba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 5 markdown files and 13 notebooks\n",
      "Processing documents...\n",
      "Created 485 splits\n",
      "Creating vector store...\n",
      "Vector store created and persisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28210/2465801957.py:67: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "qa_chain = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28210/3761915091.py:109: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: \n",
      "There is no mention of a specific model being tried in the given context, so it is not possible to determine which model had the best accuracy overall. The context only discusses the use of information criteria to select a model with the minimal number of parameters necessary to capture the details of the data. \n"
     ]
    }
   ],
   "source": [
    "question = \"which model they tried turned the best accuracy overall?\"\n",
    "answer = ask_question(qa_chain, question)\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eda994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The project is focused on time series analysis using modern techniques to analyze and forecast time series data. It aims to develop a robust time series analysis toolkit, implement various forecasting methods, detect anomalies and change points in time series data, and provide comprehensive documentation and tutorials. The toolkit includes traditional statistical methods, machine learning models, and deep learning techniques to handle different aspects of time series data.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the project all about?\"\n",
    "answer = ask_question(qa_chain, question)\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d07bb4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 5 markdown files and 13 notebooks\n",
      "Processing documents...\n",
      "Created 485 splits\n",
      "Creating vector store...\n",
      "Vector store created and persisted\n",
      "\n",
      "Welcome to the Demand Forecasting Q&A System!\n",
      "Enter your questions (type 'exit' to quit)\n",
      "------------------------------------------------\n",
      "\n",
      "Processing your question...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful AI assistant specialized in demand forecasting and related topics. \n",
      "Use the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context: Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Question: what is the project all about?\n",
      "\n",
      "Please provide your answer following these guidelines:\n",
      "1. Only use information from the provided context\n",
      "2. If the context doesn't contain enough information to fully answer the question, clearly state that\n",
      "3. If you need to make any assumptions, explicitly state them\n",
      "4. If relevant, cite specific parts of the context\n",
      "5. Keep the answer focused and relevant to demand forecasting\n",
      "\n",
      "Answer: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer\n",
       "\n",
       "The project is focused on machine learning and time series analysis. It aims to develop a robust time series analysis toolkit, implement various forecasting methods, detect anomalies and change points in time series data, and provide comprehensive documentation and tutorials. The toolkit includes traditional statistical methods, machine learning models, and deep learning techniques, such as exponential smoothing for forecasting time series data.\n",
       "\n",
       "---\n",
       "\n",
       "### Sources Used\n",
       "**1. ../data/docs/0_project_overview.md**\n",
       "\n",
       "> Machine Learning Project Documentation  Project Overview  Introduction  Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage mod...\n",
       "\n",
       "**2. ../data/docs/0_project_overview.md**\n",
       "\n",
       "> Machine Learning Project Documentation  Project Overview  Introduction  Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage mod...\n",
       "\n",
       "**3. ../data/docs/0_project_overview.md**\n",
       "\n",
       "> Machine Learning Project Documentation  Project Overview  Introduction  Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage mod...\n",
       "\n",
       "**4. ../data/docs/0_project_overview.md**\n",
       "\n",
       "> Machine Learning Project Documentation  Project Overview  Introduction  Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage mod...\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------\n",
      "\n",
      "Thank you for using the Q&A system!\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader, NotebookLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Define a custom prompt template\n",
    "custom_prompt_template = \"\"\"You are a helpful AI assistant specialized in demand forecasting and related topics. \n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide your answer following these guidelines:\n",
    "1. Only use information from the provided context\n",
    "2. If the context doesn't contain enough information to fully answer the question, clearly state that\n",
    "3. If you need to make any assumptions, explicitly state them\n",
    "4. If relevant, cite specific parts of the context\n",
    "5. Keep the answer focused and relevant to demand forecasting\n",
    "\n",
    "Answer: Let me help you with that.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "PROMPT = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def load_markdown_files(directory: str=\"../data/docs/\") -> List:\n",
    "    \"\"\"Load markdown files from directory\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        directory,\n",
    "        glob=\"**/*.md\",\n",
    "        loader_cls=UnstructuredMarkdownLoader\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "def load_jupyter_notebooks(directory: str=\"./demand_forecast_notebooks/\") -> List:\n",
    "    \"\"\"Load jupyter notebooks from directory\"\"\"\n",
    "    documents = []\n",
    "    for notebook_path in glob.glob(f\"{directory}/**/*.ipynb\", recursive=True):\n",
    "        if \".ipynb_checkpoints\" not in notebook_path:\n",
    "            try:\n",
    "                loader = NotebookLoader(\n",
    "                    notebook_path,\n",
    "                    include_outputs=True,\n",
    "                    max_output_length=50,\n",
    "                    remove_newline=True\n",
    "                )\n",
    "                documents.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading notebook {notebook_path}: {e}\")\n",
    "    return documents\n",
    "\n",
    "def process_documents(documents: List, chunk_size: int = 1000, chunk_overlap: int = 20):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def create_vector_store(documents: List, persist_directory: str = \"../data/chroma_db\"):\n",
    "    \"\"\"Create and persist vector store\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore\n",
    "\n",
    "def setup_qa_chain(vectorstore):\n",
    "    \"\"\"Setup the QA chain with custom prompt\"\"\"\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    # Create the QA chain with custom prompt\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(\n",
    "            search_kwargs={\n",
    "                \"k\": 4  # Number of relevant chunks to retrieve\n",
    "            }\n",
    "        ),\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": PROMPT,\n",
    "            \"verbose\": True  # Set to True to see the internal workings\n",
    "        },\n",
    "        return_source_documents=True  # This will return the source documents used for the answer\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "def main():\n",
    "    print(\"Loading documents...\")\n",
    "    markdown_docs = load_markdown_files(\"../data/docs/\")\n",
    "    notebook_docs = load_jupyter_notebooks(\"./demand_forecast_notebooks/\")\n",
    "\n",
    "    all_documents = markdown_docs + notebook_docs\n",
    "    print(f\"Loaded {len(markdown_docs)} markdown files and {len(notebook_docs)} notebooks\")\n",
    "\n",
    "    print(\"Processing documents...\")\n",
    "    splits = process_documents(all_documents)\n",
    "    print(f\"Created {len(splits)} splits\")\n",
    "\n",
    "    print(\"Creating vector store...\")\n",
    "    vectorstore = create_vector_store(splits)\n",
    "    print(\"Vector store created and persisted\")\n",
    "\n",
    "    qa_chain = setup_qa_chain(vectorstore)\n",
    "    return qa_chain\n",
    "\n",
    "# def ask_question(qa_chain, question: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Ask a question about the loaded documents with improved response formatting.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Get both the answer and source documents\n",
    "#         result = qa_chain({\"query\": question})\n",
    "        \n",
    "#         # Format the response\n",
    "#         response = \"\\nAnswer: \" + result['result']\n",
    "        \n",
    "#         # Add source information if available\n",
    "#         if 'source_documents' in result:\n",
    "#             response += \"\\n\\nSources used:\"\n",
    "#             for i, doc in enumerate(result['source_documents'], 1):\n",
    "#                 # Extract source information from metadata if available\n",
    "#                 source = doc.metadata.get('source', f'Document {i}')\n",
    "#                 response += f\"\\n{i}. {source}\"\n",
    "        \n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "def ask_question(qa_chain, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question about the loaded documents and return a Markdown-formatted answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get both the answer and source documents\n",
    "        result = qa_chain({\"query\": question})\n",
    "\n",
    "        # Format the response as Markdown\n",
    "        md = f\"## Answer\\n\\n{result['result']}\\n\"\n",
    "        if 'source_documents' in result and result['source_documents']:\n",
    "            md += \"\\n---\\n\\n### Sources Used\\n\"\n",
    "            for i, doc in enumerate(result['source_documents'], 1):\n",
    "                source = doc.metadata.get('source', f'Document {i}')\n",
    "                snippet = doc.page_content[:200].replace('\\n', ' ')\n",
    "                md += f\"**{i}. {source}**\\n\\n> {snippet}...\\n\\n\"\n",
    "        return md\n",
    "    except Exception as e:\n",
    "        return f\"**An error occurred:** {str(e)}\"\n",
    "\n",
    "# def interactive_qa(qa_chain):\n",
    "#     \"\"\"\n",
    "#     Interactive Q&A session with improved formatting and source tracking.\n",
    "#     \"\"\"\n",
    "#     print(\"\\nWelcome to the Demand Forecasting Q&A System!\")\n",
    "#     print(\"Enter your questions (type 'exit' to quit)\")\n",
    "#     print(\"------------------------------------------------\")\n",
    "    \n",
    "#     while True:\n",
    "#         question = input(\"\\nYour question: \").strip()\n",
    "#         if question.lower() == 'exit':\n",
    "#             print(\"\\nThank you for using the Q&A system!\")\n",
    "#             break\n",
    "        \n",
    "#         print(\"\\nProcessing your question...\")\n",
    "#         answer = ask_question(qa_chain, question)\n",
    "#         print(answer)\n",
    "#         print(\"\\n------------------------------------------------\")\n",
    "\n",
    "def interactive_qa(qa_chain):\n",
    "    \"\"\"\n",
    "    Interactive Q&A session with Markdown output.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "        use_markdown = True\n",
    "    except ImportError:\n",
    "        use_markdown = False\n",
    "\n",
    "    print(\"\\nWelcome to the Demand Forecasting Q&A System!\")\n",
    "    print(\"Enter your questions (type 'exit' to quit)\")\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"\\nThank you for using the Q&A system!\")\n",
    "            break\n",
    "\n",
    "        print(\"\\nProcessing your question...\")\n",
    "        answer_md = ask_question(qa_chain, question)\n",
    "        if use_markdown:\n",
    "            display(Markdown(answer_md))\n",
    "        else:\n",
    "            print(answer_md)\n",
    "        print(\"\\n------------------------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qa_chain = main()\n",
    "    interactive_qa(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a8b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Loaded 5 markdown files and 13 notebooks\n",
      "Processing documents...\n",
      "Created 485 splits\n",
      "Creating vector store...\n",
      "Vector store created and persisted\n"
     ]
    }
   ],
   "source": [
    "qa_chain = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcff99af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful AI assistant specialized in demand forecasting and related topics. \n",
      "Use the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context: Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Machine Learning Project Documentation\n",
      "\n",
      "Project Overview\n",
      "\n",
      "Introduction\n",
      "\n",
      "Welcome to the documentation for our machine learning project focused on time series analysis. This project aims to leverage modern techniques to analyze and forecast time series data, providing valuable insights and predictions.\n",
      "\n",
      "Objectives\n",
      "\n",
      "Develop a robust time series analysis toolkit.\n",
      "\n",
      "Implement various forecasting methods.\n",
      "\n",
      "Detect anomalies and change points in time series data.\n",
      "\n",
      "Provide comprehensive documentation and tutorials.\n",
      "\n",
      "Project Details\n",
      "\n",
      "Time Series Analysis Toolkit\n",
      "\n",
      "Overview\n",
      "\n",
      "Our time series analysis toolkit includes a variety of methods and models designed to handle different aspects of time series data. These include traditional statistical methods, machine learning models, and deep learning techniques.\n",
      "\n",
      "Methods and Models\n",
      "\n",
      "Exponential Smoothing: A classic method for forecasting time series data, including single, double, and triple exponential smoothing.\n",
      "\n",
      "Question: what is the project all about?\n",
      "\n",
      "Please provide your answer following these guidelines:\n",
      "1. Only use information from the provided context\n",
      "2. If the context doesn't contain enough information to fully answer the question, clearly state that\n",
      "3. If you need to make any assumptions, explicitly state them\n",
      "4. If relevant, cite specific parts of the context\n",
      "5. Keep the answer focused and relevant to demand forecasting\n",
      "\n",
      "Answer: Let me help you with that.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: \n",
      "Answer: The project is focused on machine learning and time series analysis. It aims to develop a robust time series analysis toolkit, implement various forecasting methods, detect anomalies and change points in time series data, and provide comprehensive documentation and tutorials. The toolkit includes traditional statistical methods, machine learning models, and deep learning techniques, such as exponential smoothing for forecasting time series data.\n",
      "\n",
      "Sources used:\n",
      "1. ../data/docs/0_project_overview.md\n",
      "2. ../data/docs/0_project_overview.md\n",
      "3. ../data/docs/0_project_overview.md\n",
      "4. ../data/docs/0_project_overview.md\n"
     ]
    }
   ],
   "source": [
    "question = \"what is the project all about?\"\n",
    "answer = ask_question(qa_chain, question)\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb4c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_genai_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
