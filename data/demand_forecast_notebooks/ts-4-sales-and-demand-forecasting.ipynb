{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"},{"sourceId":3352946,"sourceType":"datasetVersion","datasetId":2023101}],"dockerImageVersionId":30170,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**If you think this notebook deserves an upvote, I'd love to have it. An upvote per view, its all I ask** (credit to [Dan Carlin](https://twitter.com/HardcoreHistory) for coining the phrase ;-) \n\n---------------------------------------\n\nThis is part of a series of notebooks about practical time series methods:\n\n* [Part 0: the basics](https://www.kaggle.com/konradb/ts-0-the-basics)\n* [Part 1a: smoothing methods](https://www.kaggle.com/konradb/ts-1a-smoothing-methods)\n* [Part 1b: Prophet](https://www.kaggle.com/konradb/ts-1b-prophet) \n* [Part 2: ARMA](https://www.kaggle.com/konradb/ts-2-arma-and-friends)\n* [Part 3: Time series for finance](https://www.kaggle.com/konradb/ts-3-time-series-for-finance) \n* [Part 4: Sales and demand forecasting](https://www.kaggle.com/konradb/ts-4-sales-and-demand-forecasting) - **this notebook**\n* [Part 5: Automatic for the people](https://www.kaggle.com/code/konradb/ts-5-automatic-for-the-people) \n* [Part 6: Deep learning for TS - sequences](https://www.kaggle.com/konradb/ts-6-deep-learning-for-ts-sequences) \n* [Part 7: Survival analysis](https://www.kaggle.com/konradb/ts-7-survival-analysis) \n* [Part 8: Hierarchical time series](https://www.kaggle.com/code/konradb/ts-8-hierarchical-time-series)\n* [Part 9: Hybrid methods](https://www.kaggle.com/code/konradb/ts-9-hybrid-methods/)\n* [Part 10: Validation methods for time series](https://www.kaggle.com/code/konradb/ts-10-validation-methods-for-time-series/)\n* [Part 11: Transfer learning](https://www.kaggle.com/code/konradb/ts-11-deep-learning-for-ts-transfer-learning)\n\n\nThe series is accompanied by video presentations on the YouTube channel of [Abhishek](https://www.kaggle.com/abhishek):\n\n* [Talk 0](https://www.youtube.com/watch?v=cKzXOOtOXYY) \n* [Talk 1](https://www.youtube.com/watch?v=kAI67Sz92-s) - combining the content from parts 1a and 1b\n* [Talk 2](https://www.youtube.com/watch?v=LjV5DE3KR-U) \n* [Talk 3](https://www.youtube.com/watch?v=74rDhJexmTg)\n* [Talk 4](https://www.youtube.com/watch?v=RdH8zd07u2E)  - **based on this notebook**\n* [Talk 5](https://www.youtube.com/watch?v=wBP8Pc4Wxzs)\n* [Talk 6](https://www.youtube.com/watch?v=81AEI0tj0Kk)\n* [Talk 7](https://www.youtube.com/watch?v=m-8I_hkmz9o)\n* [Talk 8](https://www.youtube.com/watch?v=7ZTarg4QYR4)\n* [Talk 9](https://www.youtube.com/watch?v=NYZzBvKcfp4)\n* [Talk 10](https://www.youtube.com/watch?v=47WeBiLV2Uo)\n* [Talk 11]()\n\n\n---------------------------------------\n\n\nThis notebook is sort of a companion piece to the one about time series for finance: https://www.kaggle.com/konradb/ts-3-time-series-for-finance - similarly to volatility clustering, intermittent time series is a problem usually (but not exclusively) encountered in business applications. When predicting sales or demand in retail, we have three main issues to deal with: data is (frequently) integer-valued, intermittent and not that large. We will discuss those different aspects in the following sections:\n\n* [Croston model](#section-one)\n* [ML approach](#section-two)\n* [New launches](#section-three)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.030438,"end_time":"2022-03-06T18:06:24.272153","exception":false,"start_time":"2022-03-06T18:06:24.241715","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import itertools\nimport pandas as pd\nimport numpy as np\nfrom random import gauss, shuffle \n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.stats.diagnostic import het_arch, acorr_ljungbox\n\nfrom sklearn.metrics import mean_absolute_percentage_error as mape, mean_squared_error as mse\nfrom scipy.stats import shapiro, probplot\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.cluster import KMeans\n\nimport os\nimport lightgbm as lgb\n\nfrom  datetime import datetime, timedelta\nimport gc, copy\n\nfrom gensim.models import Word2Vec \n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n\nimport warnings\nwarnings.simplefilter(action='ignore', category= FutureWarning)","metadata":{"papermill":{"duration":1.280622,"end_time":"2022-03-06T18:06:25.592438","exception":false,"start_time":"2022-03-06T18:06:24.311816","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-25T21:43:51.794056Z","iopub.execute_input":"2022-03-25T21:43:51.794482Z","iopub.status.idle":"2022-03-25T21:43:51.805125Z","shell.execute_reply.started":"2022-03-25T21:43:51.794437Z","shell.execute_reply":"2022-03-25T21:43:51.803945Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# general settings\nclass CFG:\n    data_folder = '../input/tsdata-1/'\n    img_dim1 = 20\n    img_dim2 = 10\n        \n# adjust the parameters for displayed figures    \nplt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})    ","metadata":{"papermill":{"duration":0.032606,"end_time":"2022-03-06T18:06:25.651664","exception":false,"start_time":"2022-03-06T18:06:25.619058","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-25T21:15:04.500835Z","iopub.execute_input":"2022-03-25T21:15:04.501231Z","iopub.status.idle":"2022-03-25T21:15:04.509087Z","shell.execute_reply.started":"2022-03-25T21:15:04.501183Z","shell.execute_reply":"2022-03-25T21:15:04.507884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def my_rmse(x,y):\n    return(np.round( np.sqrt(mse(x,y)) ,4))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:15:04.514387Z","iopub.execute_input":"2022-03-25T21:15:04.5151Z","iopub.status.idle":"2022-03-25T21:15:04.524332Z","shell.execute_reply.started":"2022-03-25T21:15:04.515056Z","shell.execute_reply":"2022-03-25T21:15:04.523434Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will be using the data from the M5 competition: https://www.kaggle.com/c/m5-forecasting-accuracy. The contest used a hierarchical sales data from Walmart to forecast daily sales for the next 28 days. From the comp description page: \"*The data covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events*\". \n\n\nWe start by formatting the data to a more usable format.","metadata":{}},{"cell_type":"code","source":"xdat = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nxdat = xdat.loc[xdat.state_id == 'CA']\nxdat.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\nxdat = reduce_mem_usage(xdat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# melt the data into long format\nxdat = pd.melt(xdat, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sales').dropna()\n\n# get proper timestamps\nxcal = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xdat = pd.merge(xdat, xcal, on='d', how='left')\ndel xcal","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"xdat.drop(['d', 'wm_yr_wk', 'weekday', 'month', 'year',  'event_name_1', 'event_type_1', \n           'event_name_2', 'event_type_2', 'snap_TX', 'snap_WI', 'state_id'], \n                  axis = 1, inplace = True)\nxdat.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"That looks more manageable - let's get started.\n\n\n<a id=\"section-one\"></a>\n# Croston model\n\n## Basic Croston\n\nA first simple approach to predicting intermittent demand series is the Croston model, which takes a three step approach:\n- evaluate the average demand level when there is a demand occurrence\n- evaluate the average time between two demand occurrences\n- forecast the demand as the demand level (when there is an occurrence) multiplied by the probability of having an occurrence.\n\nWe go about this by adjusting exponential smoothing to our problem. If we denote the actual demand level by $X_t$ and our level estimate as $a_t$, then if $X_t > 0$ we get:\n\n\\begin{equation}\na_{t+1} = \\alpha X_t + (1- \\alpha) a_t \n\\end{equation}\n\nand $a_{t+1} = a_t$ otherwise; $\\alpha$ has the same role as in the basic exponential smoothing (see https://www.kaggle.com/konradb/ts-1a-smoothing-methods for a refresher). \n\nA second important component of a model for an intermittent time series is periodicity: we capture time between two demand occurrences $p$ and time elapsed time elapsed since the previous demand occurrence $q$. As before, for $X_t > 0$ we can specify\n\n\\begin{equation}\np_{t+1} = \\alpha q + (1-\\alpha) p_t\n\\end{equation}\n\nand $p_{t+1} = p_t$ otherwise. The forecast for a given period is the given by\n\n\\begin{equation}\nf_{t+1} = \\frac{a_t}{p_t}\n\\end{equation}\n\n\nHow does it perform in practice? Let's begin by subsetting our dataset to a single product x store combination:","metadata":{}},{"cell_type":"code","source":"df = xdat.loc[(xdat.item_id == 'HOBBIES_1_288') & (xdat.store_id == 'CA_1') ][['date', 'sales']].copy()\ndf = df.loc[(df.date >= '2012-01-01') & (df.date <= '2015-06-30')]\ndf.set_index('date').sales.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nDislacimer: the code for both the basic Croston model and the TSB variant (discussed below) is taken from the excellent post by Nicolas Vandeput: https://medium.com/towards-data-science/croston-forecast-model-for-intermittent-demand-360287a17f5f","metadata":{}},{"cell_type":"code","source":"def Croston(ts,extra_periods=1,alpha=0.4):\n    \n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), periodicity(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    q = 1 #periods since last demand observation\n    \n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1 + first_occurence\n    f[0] = a[0]/p[0]\n    # Create all the t+1 forecasts\n    for t in range(0,cols):        \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = alpha*q + (1-alpha)*p[t]\n            f[t+1] = a[t+1]/p[t+1]\n            q = 1           \n        else:\n            a[t+1] = a[t]\n            p[t+1] = p[t]\n            f[t+1] = f[t]\n            q += 1\n       \n    # Future Forecast \n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate a forecast with base Croston method:","metadata":{}},{"cell_type":"code","source":"pred_croston =  Croston(df.sales, extra_periods = 10)\npred_croston","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_croston = pred_croston.Forecast.head(len(df))\n\nprint('RMSE: ' + str(my_rmse(df.sales,pred_croston)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# xvalid = pd.DataFrame(xvalid.values, columns = ['actual'])\ndf['Croston'] = pred_croston.values\ndf.set_index('date').plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If there is a period with no demand, the forecast from the Croston model is not updated - which is counterintuitive:\n- extended period with no demand should lead to a reduced forecast going forward\n- surge in demand after a long period of inactivity should lead to an increase\n\nIn 2011 Teunter, Synteos and Babai proposed an extension to the basic Croston model: https://www.sciencedirect.com/science/article/abs/pii/S0377221711004437. The main change they proposed was allowing the model to decrease the periodicity estimate, even in the absence of demand. \n\nThe level of demand is estimated in the same manner as before\n\n\\begin{equation}\na_{t+1} = \\alpha X_t + (1- \\alpha) a_t \n\\end{equation}\n\nWe change the definition of periodicity: $p$ denotes the probability of having a demand occurrence and it will be updated each period:\n- decrease if there is no demand occurrence, in an exponential manner \n- increase otherwise\n\n\\begin{equation}\np_{t+1} = \\beta + (1- \\beta) p_t\n\\end{equation}\n\nif $X_t > 0$ and $(1- \\beta) p_t$ otherwise. The forecast for a given period is the given by\n\n\\begin{equation}\nf_{t+1} = a_{t+1}p_{t+1}\n\\end{equation}\n\nCaveat emptor:\n- the forecast f is defined as the periodicity p multiplied by the level a (**and not divided by it, as in the original model**)\n- the forecast for t+1 is defined based on the level and periodicity estimates of t+1 (**and not t**)\n\n","metadata":{}},{"cell_type":"code","source":"def Croston_TSB(ts,extra_periods=1,alpha=0.4,beta=0.1):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), probability(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1/(1 + first_occurence)\n    f[0] = p[0]*a[0]\n                 \n    # Create all the t+1 forecasts\n    for t in range(0,cols): \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = beta*(1) + (1-beta)*p[t]  \n        else:\n            a[t+1] = a[t]\n            p[t+1] = (1-beta)*p[t]       \n        f[t+1] = p[t+1]*a[t+1]\n        \n    # Future Forecast\n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_croston_tsb =  Croston_TSB(df.sales, extra_periods = 10, alpha = 0.1, beta = 0.1)\n\npred_croston_tsb = pred_croston_tsb.Forecast.head(len(df))\nprint('RMSE: ' + str(my_rmse(df.sales,pred_croston_tsb)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Croston_TSB'] = pred_croston_tsb.values\ndf.set_index('date').plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Moving from vanilla to TSB Croston:\n- improves the error (somewhat)\n- solves the issues with forecast adjustment in zero sales periods\n\nOverall, it gives us a way of handling an intermittent time series, but it does not have a natural extension to a multivariate case - so we need to try something else. \n\n<a id=\"section-two\"></a>\n# ML approach\n\nIf you are dealing with tabular data (which is a category time series belong to), tree-based methods are something of a Swiss-Army chainsaw: they work out of the box, with minimal requirements on data. In this section we will formulate a time series problem as a regression one, so that we can use LightGBM to predict the sales in multiple stores (we continue working with the data from the M5 competition). Quick reminder of what our data looks like:","metadata":{}},{"cell_type":"code","source":"xdat.head(4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = xdat.loc[xdat.item_id == 'HOBBIES_1_288' ][['date', 'sales']].copy()\ndf.set_index('date').sales.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = xdat.loc[xdat.item_id == 'FOODS_3_823' ][['date', 'sales']].copy()\ndf.set_index('date').sales.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = xdat.loc[xdat.item_id == 'HOUSEHOLD_2_513' ][['date', 'sales']].copy()\ndf.set_index('date').sales.plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We could run multiple separate models like Croston, but it's infeasible for two reasons:\n- there are a LOT of store x product combinations\n- shared factors between stores\n\nInstead, we will generate some features to capture the temporal dynamics and treat the problem of predicting all the series jointly as a regression.","metadata":{}},{"cell_type":"code","source":"xdat.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Start with lag features: \n- simple moving averages over a given period\n- shifted ","metadata":{}},{"cell_type":"code","source":"def create_features(dt, lags = [28], wins = [7,28]):\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).fillna(-1)\n\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()).fillna(-1)\n        \n    return dt\n\nxdat = create_features(xdat)\n\nxdat.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Time-based features:","metadata":{}},{"cell_type":"code","source":"xdat['date'] = pd.to_datetime(xdat[\"date\"])\n\ndate_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n\n    }\n    \nfor date_feat_name, date_feat_func in date_features.items():\n    if date_feat_name in xdat.columns:\n        xdat[date_feat_name] = xdat[date_feat_name].astype(\"int16\")\n    else:\n        xdat[date_feat_name] = getattr(xdat[\"date\"].dt, date_feat_func).astype(\"int16\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encode categorical features\ncat_feats = ['item_id', 'dept_id','store_id', 'cat_id']\nfor cc in cat_feats:\n    le = LabelEncoder()\n    xdat[cc] = le.fit_transform(xdat[cc])\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split into train and test\ncutoff = xdat.date.max() - pd.to_timedelta(28, unit = 'D')\nxtrain = xdat.loc[xdat.date < cutoff].copy()\nxvalid = xdat.loc[xdat.date >= cutoff].copy()\n\ndel xdat\n\nxtrain.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ytrain = xtrain['sales'].copy()\nyvalid = xvalid['sales'].copy()\n\nxtrain.drop(['id', 'wday', 'sales',  'snap_CA', 'year', 'date'], axis = 1, inplace = True)\nxvalid.drop(['id', 'wday', 'sales',  'snap_CA', 'year', 'date'], axis = 1, inplace = True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are finally ready to go down the usual lgbm path:","metadata":{}},{"cell_type":"code","source":"dtrain = lgb.Dataset(xtrain , label = ytrain,  free_raw_data=False)\ndvalid = lgb.Dataset(xvalid, label = yvalid,   free_raw_data=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Can we exploit the nature of the data?","metadata":{}},{"cell_type":"code","source":"ytrain.plot.density()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Giant spike at zero $\\implies$ we need a criterion that allows for a probability mass ","metadata":{}},{"cell_type":"code","source":"\nparams = {\n        \"objective\" : \"poisson\",\n    \n        \"metric\" : \"rmse\",\n        \"learning_rate\" : 0.075,\n         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n        'verbosity': 1,\n       'num_iterations' : 1000,        \n        'num_leaves': 128,\n        \"min_data_in_leaf\": 50,\n}\n\n\nm_lgb = lgb.train(params, dtrain, valid_sets = [dtrain, dvalid], \n                  early_stopping_rounds = 150,\n                  verbose_eval=100)  ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"More general step: tweedie https://en.wikipedia.org/wiki/Tweedie_distribution","metadata":{}},{"cell_type":"code","source":"params = {\n        \"objective\" : \"tweedie\",\n    \n        \"metric\" : \"rmse\",\n        \"learning_rate\" : 0.075,\n         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n        'verbosity': 1,\n       'num_iterations' : 1000,        \n        'num_leaves': 128,\n        \"min_data_in_leaf\": 50,\n}\n\n\nm_lgb = lgb.train(params, dtrain, valid_sets = [dtrain, dvalid], \n                  early_stopping_rounds = 150,\n                  verbose_eval=100) \n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lgb.plot_importance(m_lgb,height = 0.5)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Takeaway:\n- time series as regression\n- (lagged) rolling statistics\n- distribution of target\n- careful about ranges $\\rightarrow$ extrapolation\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# New launches\n\nFinally, we will have a quick look at the problem of predicting the sales patterns for new products which - by their very nature - do not have a lot of history to use for a time series model. We will use a subset of the Visuelle dataset: https://github.com/HumaticsLAB/GTM-Transformer. We have training / validation split prepared by the dataset authors:\n","metadata":{}},{"cell_type":"code","source":"xtrain = pd.read_csv('../input/partial-visuelle/train.csv')\nxtrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:15.876834Z","iopub.execute_input":"2022-03-25T21:24:15.877229Z","iopub.status.idle":"2022-03-25T21:24:15.943214Z","shell.execute_reply.started":"2022-03-25T21:24:15.877185Z","shell.execute_reply":"2022-03-25T21:24:15.942065Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xtest = pd.read_csv('../input/partial-visuelle/test.csv')\nxtest.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:18.355328Z","iopub.execute_input":"2022-03-25T21:24:18.355672Z","iopub.status.idle":"2022-03-25T21:24:18.391686Z","shell.execute_reply.started":"2022-03-25T21:24:18.355638Z","shell.execute_reply":"2022-03-25T21:24:18.39042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xtrain.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:20.755166Z","iopub.execute_input":"2022-03-25T21:24:20.755505Z","iopub.status.idle":"2022-03-25T21:24:20.762818Z","shell.execute_reply.started":"2022-03-25T21:24:20.755473Z","shell.execute_reply":"2022-03-25T21:24:20.761824Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MultiOutput\n\nInitial idea: predict the entire sequence (sales in the first 12 days) based on the other information.","metadata":{}},{"cell_type":"code","source":"# split into targets and variables we will use for clustering\ntarget_cols = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\ncat_cols = ['category', 'day', 'week', 'month', 'color', 'fabric', 'extra' ]\n\ny0, y1 = xtrain[target_cols].copy(), xtest[target_cols].copy()\nx0, x1 = xtrain[cat_cols].copy(), xtest[cat_cols].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:24.476141Z","iopub.execute_input":"2022-03-25T21:24:24.476513Z","iopub.status.idle":"2022-03-25T21:24:24.489296Z","shell.execute_reply.started":"2022-03-25T21:24:24.476474Z","shell.execute_reply":"2022-03-25T21:24:24.488509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transforms categoricals\nxdat = pd.concat([x0,x1], axis = 0)\nxdat = pd.get_dummies(xdat)\nx0 = xdat.iloc[:x0.shape[0]]\nx1 = xdat.iloc[x0.shape[0]:]\ndel xdat","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:26.840433Z","iopub.execute_input":"2022-03-25T21:24:26.840861Z","iopub.status.idle":"2022-03-25T21:24:26.867247Z","shell.execute_reply.started":"2022-03-25T21:24:26.840819Z","shell.execute_reply":"2022-03-25T21:24:26.866264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model\nbase_model = Ridge()\nmo_base = MultiOutputRegressor(base_model, n_jobs=-1)\n\nmo_base.fit(x0, y0)\n\nmo_prediction = mo_base.predict(x1) ","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:28.835935Z","iopub.execute_input":"2022-03-25T21:24:28.836545Z","iopub.status.idle":"2022-03-25T21:24:30.97627Z","shell.execute_reply.started":"2022-03-25T21:24:28.836507Z","shell.execute_reply":"2022-03-25T21:24:30.975283Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('(sort of) MSE: ' + str(np.round( np.sqrt(np.average((mo_prediction - y1)**2)) ,4  )))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:31.196144Z","iopub.execute_input":"2022-03-25T21:24:31.196776Z","iopub.status.idle":"2022-03-25T21:24:31.204673Z","shell.execute_reply.started":"2022-03-25T21:24:31.196726Z","shell.execute_reply":"2022-03-25T21:24:31.203976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's have a look at a few examples:","metadata":{}},{"cell_type":"code","source":"ii = 0 \nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:33.395758Z","iopub.execute_input":"2022-03-25T21:24:33.396642Z","iopub.status.idle":"2022-03-25T21:24:33.682023Z","shell.execute_reply.started":"2022-03-25T21:24:33.396581Z","shell.execute_reply":"2022-03-25T21:24:33.680969Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ii = 12\nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:37.07604Z","iopub.execute_input":"2022-03-25T21:24:37.077046Z","iopub.status.idle":"2022-03-25T21:24:37.368228Z","shell.execute_reply.started":"2022-03-25T21:24:37.076995Z","shell.execute_reply":"2022-03-25T21:24:37.367178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ii = 80\nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What else can we try?\n\n\n## Embeddings","metadata":{}},{"cell_type":"markdown","source":"\n\nAlternative approach:\n- create embeddings for the categorical part with cat2vec https://openreview.net/pdf?id=HyNxRZ9xg\n- cluster the products for which we know the sales\n- calculate average target per cluster\n- map the new ones to the nearest cluster","metadata":{}},{"cell_type":"code","source":"def correct_time(df):\n    df['release_date'] = pd.to_datetime(df['release_date'])\n    df['week'] = df.release_date.dt.isocalendar().week\n    df['month'] = df.release_date.dt.month\n    df['day'] = df.release_date.dt.day\n    \ncorrect_time(xtrain)\ncorrect_time(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:24:42.196366Z","iopub.execute_input":"2022-03-25T21:24:42.196791Z","iopub.status.idle":"2022-03-25T21:24:42.226396Z","shell.execute_reply.started":"2022-03-25T21:24:42.196749Z","shell.execute_reply":"2022-03-25T21:24:42.224886Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split into targets and variables we will use for clustering\ntarget_cols = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\ncat_cols = ['category', 'day', 'week', 'month', 'color', 'fabric', 'extra' ]\n\ny0, y1 = xtrain[target_cols].copy(), xtest[target_cols].copy()\nx0, x1 = xtrain[cat_cols].copy(), xtest[cat_cols].copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:25:03.318145Z","iopub.execute_input":"2022-03-25T21:25:03.318516Z","iopub.status.idle":"2022-03-25T21:25:03.333957Z","shell.execute_reply.started":"2022-03-25T21:25:03.318481Z","shell.execute_reply":"2022-03-25T21:25:03.332883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Implementation from https://www.kaggle.com/code/classtag/cat2vec-powerful-feature-for-categorical\n\n\n### Cat2vec","metadata":{}},{"cell_type":"code","source":"def apply_w2v(sentences, model, num_features):\n    def _average_word_vectors(words, model, vocabulary, num_features):\n        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n        n_words = 0.\n        for word in words:\n            if word in vocabulary: \n                n_words = n_words + 1.\n                feature_vector = np.add(feature_vector, model.wv[word])\n        if n_words:\n            feature_vector = np.divide(feature_vector, n_words)\n        return feature_vector\n    \n    vocab = set(model.wv.index_to_key)\n    feats = [_average_word_vectors(s, model, vocab, num_features) for s in sentences]\n    return np.array(feats)\n\ndef gen_cat2vec_sentences(data):\n    X_w2v = copy.deepcopy(data)\n    names = list(X_w2v.columns.values)\n    for c in names:\n        X_w2v[c] = X_w2v[c].fillna('unknow').astype('category')\n        X_w2v[c].cat.categories = [\"%s %s\" % (c,g) for g in X_w2v[c].cat.categories]\n    X_w2v = X_w2v.values.tolist()\n    return X_w2v\n\ndef fit_cat2vec_model(data):\n    X_w2v = gen_cat2vec_sentences(data)\n    for i in X_w2v:\n        shuffle(i)\n    model = Word2Vec(X_w2v, vector_size= 10, window= 3)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:41:53.156254Z","iopub.execute_input":"2022-03-25T21:41:53.156621Z","iopub.status.idle":"2022-03-25T21:41:53.169803Z","shell.execute_reply.started":"2022-03-25T21:41:53.156587Z","shell.execute_reply":"2022-03-25T21:41:53.16864Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit a cat2vec\nc2v_model = fit_cat2vec_model(pd.concat([x0,x1], axis = 0))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:29:13.995833Z","iopub.execute_input":"2022-03-25T21:29:13.996969Z","iopub.status.idle":"2022-03-25T21:29:14.250193Z","shell.execute_reply.started":"2022-03-25T21:29:13.996916Z","shell.execute_reply":"2022-03-25T21:29:14.248793Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate the embeddings\nx0_c2v = apply_w2v(gen_cat2vec_sentences(x0), c2v_model, 10)\nx1_c2v = apply_w2v(gen_cat2vec_sentences(x1), c2v_model, 10)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:42:19.763444Z","iopub.execute_input":"2022-03-25T21:42:19.763825Z","iopub.status.idle":"2022-03-25T21:42:19.984487Z","shell.execute_reply.started":"2022-03-25T21:42:19.763784Z","shell.execute_reply":"2022-03-25T21:42:19.983481Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clustering","metadata":{}},{"cell_type":"code","source":"# fit kmeans\nclustering = KMeans(n_clusters = 10)\nclustering.fit(x0_c2v)\n\ny0_clusters = clustering.predict(x0_c2v)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T22:00:40.557495Z","iopub.execute_input":"2022-03-25T22:00:40.557931Z","iopub.status.idle":"2022-03-25T22:00:42.074842Z","shell.execute_reply.started":"2022-03-25T22:00:40.557888Z","shell.execute_reply":"2022-03-25T22:00:42.074088Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# assign the clusters for the test set\ny1_clusters = clustering.predict(x1_c2v)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T21:53:02.537101Z","iopub.execute_input":"2022-03-25T21:53:02.537504Z","iopub.status.idle":"2022-03-25T21:53:02.748234Z","shell.execute_reply.started":"2022-03-25T21:53:02.537467Z","shell.execute_reply":"2022-03-25T21:53:02.74722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Mapping","metadata":{}},{"cell_type":"code","source":"\ncl_prediction = np.zeros(y1.shape)\n\nfor ii in range(10):\n    cl_prediction[y1_clusters == ii,:] = y0.loc[y0_clusters == ii].mean()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-25T22:02:15.063902Z","iopub.execute_input":"2022-03-25T22:02:15.064369Z","iopub.status.idle":"2022-03-25T22:02:15.082666Z","shell.execute_reply.started":"2022-03-25T22:02:15.064322Z","shell.execute_reply":"2022-03-25T22:02:15.081359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('(sort of) MSE: ' + str(np.round( np.sqrt(np.average((cl_prediction - y1)**2)) ,4  )))","metadata":{"execution":{"iopub.status.busy":"2022-03-25T22:02:34.716959Z","iopub.execute_input":"2022-03-25T22:02:34.717344Z","iopub.status.idle":"2022-03-25T22:02:34.730309Z","shell.execute_reply.started":"2022-03-25T22:02:34.717304Z","shell.execute_reply":"2022-03-25T22:02:34.728952Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ii = 0 \nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted - MO\")\nplt.plot(cl_prediction[ii,:], label = \"predicted - CL\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T22:03:15.758092Z","iopub.execute_input":"2022-03-25T22:03:15.758896Z","iopub.status.idle":"2022-03-25T22:03:16.13717Z","shell.execute_reply.started":"2022-03-25T22:03:15.758844Z","shell.execute_reply":"2022-03-25T22:03:16.135814Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ii = 12 \nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted - MO\")\nplt.plot(cl_prediction[ii,:], label = \"predicted - CL\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T22:03:38.04749Z","iopub.execute_input":"2022-03-25T22:03:38.047901Z","iopub.status.idle":"2022-03-25T22:03:38.397215Z","shell.execute_reply.started":"2022-03-25T22:03:38.047858Z","shell.execute_reply":"2022-03-25T22:03:38.396092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ii = 80\nplt.plot(y0.values[ii,:], label = \"real sales\")\nplt.plot(mo_prediction[ii,:], label = \"predicted - MO\")\nplt.plot(cl_prediction[ii,:], label = \"predicted - CL\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This concludes our adventure in demand prediction territory:\n- Croston model gets you through univariate cases\n- LGBM is the way to go for handling multiple series at once\n- hybrid approach to predicting new products\n\n\nStay tuned! ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T15:27:16.008299Z","iopub.execute_input":"2022-03-24T15:27:16.008849Z","iopub.status.idle":"2022-03-24T15:27:16.051701Z","shell.execute_reply.started":"2022-03-24T15:27:16.008816Z","shell.execute_reply":"2022-03-24T15:27:16.050691Z"}}}]}